{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\")) #make the folder \"automl\" part of this\n",
    "\n",
    "RESULTS_PATH = 'results.csv'\n",
    "OPTUNA_DATABASE = 'study_results.db'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.result_logger import ResultLogger\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "from automl.utils.optuna_utils import load_study_from_database\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATE_NUMBER = 20 #the number of neighbor points to sum to plot the needed graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meta(type):\n",
    "    \n",
    "    def __prepare__(cls, name, **kwargs):\n",
    "        print(\"Meta __prepare__\")\n",
    "        return {\"attribute_name\": \"value\"}\n",
    "        \n",
    "    def __new__(cls, name, bases, attrs):\n",
    "        print(\"Meta __new__\")\n",
    "        return super().__new__(cls, name, bases, attrs)\n",
    "    \n",
    "    def __init__(cls, name, bases, attrs):\n",
    "        print(\"Meta __init__\")\n",
    "        super().__init__(name, bases, attrs)\n",
    "        \n",
    "class MyClass(metaclass=Meta):\n",
    "    \n",
    "    print(\"Class body\")\n",
    "    \n",
    "    attribute_name_2 = \"value_2\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"MyClass __init__\")\n",
    "    \n",
    "print(MyClass.attribute_name)\n",
    "print(MyClass.attribute_name_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment_path = 'data\\\\experiments\\\\HyperparameterOptimizationPipeline_3'\n",
    "experiment_path = 'archived_data\\\\first'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of HyperparameterOptimizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_optimization_results : ResultLogger = ResultLogger(input={\n",
    "                                        \"logger_directory\" : experiment_path,\n",
    "                                        \"filename\" : RESULTS_PATH\n",
    "                                      })\n",
    "\n",
    "hyperparameter_optimization_results.proccess_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_study = load_study_from_database(database_path=hyperparameter_optimization_results.lg.logDir + '\\\\' + OPTUNA_DATABASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vis.plot_param_importances(optuna_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = vis.plot_parallel_coordinate(optuna_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vis.plot_intermediate_values(optuna_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vis.plot_optimization_history(optuna_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_plot = [\"hidden_size\", \"hidden_layers\"]\n",
    "\n",
    "fig = vis.plot_contour(optuna_study, params=parameters_to_plot)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_plot = [\"epsilon_start\", \"epsilon_decay\", \"epsilon_end\"]\n",
    "\n",
    "fig = vis.plot_contour(optuna_study, params=parameters_to_plot)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global evaluation of configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_of_configurations : dict[str, ResultLogger] = {}\n",
    "\n",
    "for configuration_name in os.listdir(experiment_path):\n",
    "    configuration_path = os.path.join(experiment_path, configuration_name)\n",
    "    \n",
    "    if os.path.isdir(configuration_path):  # Ensure it's a file, not a subdirectory\n",
    "        \n",
    "        results_of_configurations[configuration_name] = ResultLogger(input={\n",
    "                                        \"logger_directory\" : configuration_path,\n",
    "                                        \"filename\" : RESULTS_PATH\n",
    "                                      })\n",
    "        results_of_configurations[configuration_name].proccess_input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Configurations:  {results_of_configurations.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global view of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_optimization_results.plot_bar_graph(x_axis='experiment', y_axis='result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruned Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_optuna_trials = [trial for trial in optuna_study.trials if trial.state == optuna.trial.TrialState.PRUNED]\n",
    "\n",
    "pruned_optuna_trials_per_steps : dict[int, list[optuna.trial.FrozenTrial]] = {} #the pruned trials by the number of completed steps\n",
    "\n",
    "for pruned_optuna_trial in pruned_optuna_trials:\n",
    "    \n",
    "    n_completed_steps = len(pruned_optuna_trial.intermediate_values)\n",
    "    \n",
    "    try:\n",
    "        list_of_pruned = pruned_optuna_trials_per_steps[n_completed_steps]\n",
    "    \n",
    "    except:\n",
    "        list_of_pruned = []\n",
    "        pruned_optuna_trials_per_steps[n_completed_steps] = list_of_pruned    \n",
    "        \n",
    "    list_of_pruned.append(pruned_optuna_trial)\n",
    "    \n",
    "    \n",
    "\n",
    "pruned_trials = [f'configuration_{trial.number + 1}' for trial in optuna_study.trials if trial.state == optuna.trial.TrialState.PRUNED]\n",
    "\n",
    "print(f\"Pruned trials: {pruned_trials}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_completed_steps in pruned_optuna_trials_per_steps.keys():\n",
    "    \n",
    "    pruned_optuna_trials = pruned_optuna_trials_per_steps[n_completed_steps] \n",
    "    \n",
    "    pruned_trials = [f'configuration_{trial.number + 1}' for trial in pruned_optuna_trials ]\n",
    "\n",
    "    for configuration_name in pruned_trials:\n",
    "        \n",
    "        results_logger = results_of_configurations[configuration_name]\n",
    "    \n",
    "        #results_logger.plot_graph(x_axis='episode', y_axis=[('total_reward', name)], to_show=False)\n",
    "        results_logger.plot_confidence_interval(x_axis='episode', y_column='total_reward',show_std=False, to_show=False, y_values_label=configuration_name, aggregate_number=AGGREGATE_NUMBER)\n",
    "        #results_logger.plot_linear_regression(x_axis='episode', y_axis='total_reward', to_show=False, y_label=configuration_name + '_linear')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed Trials Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_optuna_trials = [trial for trial in optuna_study.trials if trial.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "completed_optuna_trials.sort(key=lambda trial: trial.value) # sort given the trial value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst Completed Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORST = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "worst_optuna_trials = completed_optuna_trials[0:N_WORST]\n",
    "\n",
    "worst_configurations_to_study = [f\"configuration_{trial.number + 1}\" for trial in worst_optuna_trials]\n",
    "\n",
    "print(f\"Worst configurations to study: {worst_configurations_to_study}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for configuration_name in worst_configurations_to_study:\n",
    "    \n",
    "    results_logger = results_of_configurations[configuration_name]\n",
    "\n",
    "    #results_logger.plot_graph(x_axis='episode', y_axis=[('total_reward', name)], to_show=False)\n",
    "    results_logger.plot_confidence_interval(x_axis='episode', y_column='total_reward',show_std=False, to_show=False, y_values_label=configuration_name, aggregate_number=AGGREGATE_NUMBER)\n",
    "    #results_logger.plot_linear_regression(x_axis='episode', y_axis='total_reward', to_show=False, y_label=configuration_name + '_linear')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BEST = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optuna_trials = completed_optuna_trials[(len(completed_optuna_trials) - 1 - N_BEST):]\n",
    "\n",
    "best_configurations_to_study = [f\"configuration_{trial.number + 1}\" for trial in best_optuna_trials]\n",
    "\n",
    "print(f\"Best configurations to study: {best_configurations_to_study}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for configuration_name in best_configurations_to_study:\n",
    "    \n",
    "    results_logger = results_of_configurations[configuration_name]\n",
    "\n",
    "    #results_logger.plot_graph(x_axis='episode', y_axis=[('total_reward', name)], to_show=False)\n",
    "    results_logger.plot_confidence_interval(x_axis='episode', y_column='total_reward',show_std=False, to_show=False, y_values_label=configuration_name, aggregate_number=AGGREGATE_NUMBER)\n",
    "    #results_logger.plot_linear_regression(x_axis='episode', y_axis='total_reward', to_show=False, y_label=configuration_name + '_linear')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smaller study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations_to_study = ['configuration_45']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for configuration_name in configurations_to_study:\n",
    "    \n",
    "    results_logger = results_of_configurations[configuration_name]\n",
    "\n",
    "    #results_logger.plot_graph(x_axis='episode', y_axis=[('total_reward', name)], to_show=False)\n",
    "    results_logger.plot_confidence_interval(x_axis='episode', y_column='total_reward',show_std=True, to_show=False, y_values_label=configuration_name, aggregate_number=AGGREGATE_NUMBER)\n",
    "    results_logger.plot_linear_regression(x_axis='episode', y_axis='total_reward', to_show=False, y_label=configuration_name + '_linear')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_to_study : dict[str, ResultLogger]= {}\n",
    "\n",
    "for configuration_name in configurations_to_study:\n",
    "    \n",
    "    results_logger = results_of_configurations[configuration_name]\n",
    "    \n",
    "    for agent_name in [\"agent_1\", \"agent_2\"]:\n",
    "      \n",
    "        agent_results_logger = ResultLogger(input={\n",
    "                                            \"logger_directory\" : f\"{results_logger.lg.logDir}\\\\{agent_name}\",\n",
    "                                            \"filename\" : RESULTS_PATH\n",
    "                                          })\n",
    "\n",
    "        agents_to_study[f\"{configuration_name}_{agent_name}\"] = agent_results_logger\n",
    "        \n",
    "        agent_results_logger.proccess_input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent_name, agent_results_logger in agents_to_study.items():\n",
    "    \n",
    "    #results_logger.plot_graph(x_axis='episode', y_axis=[('total_reward', name)], to_show=False)\n",
    "    agent_results_logger.plot_confidence_interval(x_axis='episode', y_column='total_reward',show_std=False, to_show=False, y_values_label=agent_name, aggregate_number=AGGREGATE_NUMBER)\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
