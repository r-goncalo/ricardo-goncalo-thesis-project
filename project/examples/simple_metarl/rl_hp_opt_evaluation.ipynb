{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\")) #make the folder \"automl\" part of this\n",
    "\n",
    "RESULTS_PATH = 'results.csv'\n",
    "OPTUNA_DATABASE = 'study_results.db'\n",
    "BASE_CONFIGURATION_NAME = 'configuration'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.result_logger import ResultLogger\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "from automl.utils.optuna_utils import load_study_from_database\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATE_NUMBER = 20 #the number of neighbor points to sum to plot the needed graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    experiments_base_path = \"C:\\\\Users\\\\rgoncalo\\\\Universidade\\\\ricardo-goncalo-thesis-project\\\\project\\\\examples\\\\simple_metarl\\\\archived_data\\\\sb3_montaincar_semi_trained_reduced_or_not\"\n",
    "    experiment_specific_path = \"sb3_montaincar_semi_trained_200\"\n",
    "    \n",
    "    experiment_path = f\"{experiments_base_path}\\\\{experiment_specific_path}\\\\Experiment\"\n",
    "    #experiment_path = 'data\\\\experiments\\\\HyperparameterOptimizationPipeline_0'\n",
    "    #experiment_path = 'archived_data\\\\first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    #hp_model_name = 'sb3_CartPole_dqn_gaussian_0_0.05_0.3'\n",
    "    #hp_model_name = 'sb3_CartPole_dqn_gaussian_0_0.2_0.8'\n",
    "    #hp_model_name = 'sb3_CartPole_dqn'\n",
    "    #hp_model_name = 'sb3_CartPole_dqn_perturbed_0_10'\n",
    "    hp_model_name = 'sb3_CartPole_dqn_forgotten_100_1e-17'\n",
    "    \n",
    "    \n",
    "    hp_experiment_name=f'sb3_zoo_dqn_cartpole_hp_opt'\n",
    "    \n",
    "    \n",
    "    experiment_path = f'C:\\\\rgoncalo\\\\experiments\\\\{hp_experiment_name}\\\\experiments\\\\{hp_model_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\n",
    "    base_experiment_path = \"C:\\\\Experiments\\\\rl-zoo-CartPole-dqn\\\\HPOpt\\\\first\\\\experiments\"\n",
    "\n",
    "    hp_model_name = 'sb3_CartPole_dqn_gaussian_0_0.05_0.3'\n",
    "    #hp_model_name = 'sb3_CartPole_dqn_gaussian_0_0.2_0.8'\n",
    "    #hp_model_name = 'sb3_CartPole_dqn'\n",
    "    #hp_model_name = 'sb3_CartPole_dqn_perturbed_0_10'\n",
    "    #hp_model_name = 'sb3_CartPole_dqn_forgotten_100_1e-17'\n",
    "    \n",
    "    experiment_path = f'{base_experiment_path}\\\\{hp_model_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(experiment_path):\n",
    "    raise Exception(\"DOES NOT EXIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of HyperparameterOptimizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.meta_rl.hp_optimization_pipeline import HyperparameterOptimizationPipeline\n",
    "\n",
    "hp_results_columns = HyperparameterOptimizationPipeline.results_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_optimization_results : ResultLogger = ResultLogger(input={\n",
    "                                        \"base_directory\" : experiment_path,\n",
    "                                        \"artifact_relative_directory\" : '',\n",
    "                                        \"results_filename\" : RESULTS_PATH,\n",
    "                                        \"results_columns\" : hp_results_columns,\n",
    "                                        \"create_new_directory\" : False\n",
    "                                      })\n",
    "\n",
    "hyperparameter_optimization_results.proccess_input_if_not_proccesd()\n",
    "\n",
    "print(f\"Hyperparameter_optimization_results in path: {hyperparameter_optimization_results.get_artifact_directory()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_study = load_study_from_database(database_path=hyperparameter_optimization_results.get_artifact_directory() + '\\\\' + OPTUNA_DATABASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"optuna_study done with with best value {optuna_study.best_value} in trial {optuna_study.best_trial.number} with best parameters:\\n{optuna_study.best_params}\")\n",
    "\n",
    "except:\n",
    "    print(\"No best trial yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Trials Info =====\")\n",
    "for t in optuna_study.trials:\n",
    "    print(f\"Trial {t.number}:\")\n",
    "    print(f\"  State: {t.state}\")\n",
    "    print(f\"  Value: {t.value}\")\n",
    "    print(f\"  Params: {t.params}\")\n",
    "    print(f\"  User attrs: {t.user_attrs}\")\n",
    "    print(f\"  System attrs: {t.system_attrs}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.importance import get_param_importances\n",
    "\n",
    "importances = get_param_importances(optuna_study)\n",
    "\n",
    "# Print nicely\n",
    "for param, importance in importances.items():\n",
    "    print(f\"{param}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vis.plot_param_importances(optuna_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = vis.plot_parallel_coordinate(optuna_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vis.plot_intermediate_values(optuna_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vis.plot_optimization_history(optuna_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "# Assume `optuna_study` is already loaded\n",
    "df = optuna_study.trials_dataframe()  # columns: value, params_*, state, etc.\n",
    "\n",
    "# Only consider completed trials\n",
    "df = df[df['state'] == 'COMPLETE']\n",
    "\n",
    "# Get all hyperparameter columns\n",
    "param_cols = [c for c in df.columns if c.startswith(\"params_\")]\n",
    "\n",
    "# Plot each hyperparameter vs objective with Gaussian Process regression\n",
    "for param in param_cols:\n",
    "    plt.figure(figsize=(7, 5))\n",
    "\n",
    "    # Extract values and objective\n",
    "    x = df[param].values\n",
    "    y = df['value'].values\n",
    "\n",
    "    # Scatter plot: parameter vs objective\n",
    "    plt.scatter(x, y, alpha=0.6, label=\"Trials\")\n",
    "\n",
    "    # Only fit GP if the param is numeric\n",
    "    if np.issubdtype(x.dtype, np.number):\n",
    "        # Reshape for sklearn (expects 2D arrays)\n",
    "        X = x.reshape(-1, 1)\n",
    "        Y = y.reshape(-1, 1)\n",
    "\n",
    "        # Kernel: RBF (smooth function) + WhiteKernel (noise)\n",
    "        kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0)\n",
    "\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, normalize_y=True, random_state=0)\n",
    "        gp.fit(X, Y)\n",
    "\n",
    "        # Predict on smooth range\n",
    "        x_range = np.linspace(min(x), max(x), 200).reshape(-1, 1)\n",
    "        y_mean, y_std = gp.predict(x_range, return_std=True)\n",
    "\n",
    "        # Plot GP mean\n",
    "        plt.plot(x_range, y_mean, \"r-\", lw=2, label=\"GP mean\")\n",
    "\n",
    "        # Plot uncertainty band (±1 std)\n",
    "        plt.fill_between(\n",
    "            x_range.ravel(),\n",
    "            (y_mean - y_std).ravel(),\n",
    "            (y_mean + y_std).ravel(),\n",
    "            color=\"r\",\n",
    "            alpha=0.2,\n",
    "            label=\"GP ±1σ\"\n",
    "        )\n",
    "\n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel(\"Objective value\")\n",
    "    plt.title(f\"Effect of {param} on Objective (GP fit)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters_to_plot = [\"hidden_size\", \"hidden_layers\"]\n",
    "\n",
    "#fig = vis.plot_contour(optuna_study, params=parameters_to_plot)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters_to_plot = [\"epsilon_start\", \"epsilon_decay\", \"epsilon_end\"]\n",
    "\n",
    "#fig = vis.plot_contour(optuna_study, params=parameters_to_plot)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global evaluation of configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_of_configuration(configuration_name : str, results_logger : ResultLogger):\n",
    "\n",
    "    #results_logger.plot_graph(x_axis='episode', y_axis=[('total_reward', name)], to_show=False)\n",
    "\n",
    "    results_logger.plot_confidence_interval(x_axis='episode', y_column='episode_reward',show_std=True, to_show=False, y_values_label=\"mov_avg_std\", aggregate_number=AGGREGATE_NUMBER)\n",
    "\n",
    "    #results_logger.plot_linear_regression(x_axis='episode', y_axis='episode_reward', to_show=False)\n",
    "\n",
    "    #results_logger.plot_piecewise_linear_regression(x_axis='episode', y_axis='episode_reward', to_show=False, n_segments=10)\n",
    "\n",
    "    results_logger.plot_polynomial_regression(x_axis='episode', y_axis='episode_reward', to_show=False, degrees=4)\n",
    "\n",
    "\n",
    "    results_logger.plot_current_graph(title=configuration_name, y_label=\"episode_reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_of_configurations : dict[str, ResultLogger] = {}\n",
    "\n",
    "configurations_results_relative_path = \"RLTrainerComponent\"\n",
    "\n",
    "for configuration_name in os.listdir(experiment_path):\n",
    "    \n",
    "    if configuration_name.startswith(BASE_CONFIGURATION_NAME):\n",
    "\n",
    "        configuration_path = os.path.join(experiment_path, configuration_name)\n",
    "\n",
    "        if os.path.isdir(configuration_path):  # Ensure it's a file, not a subdirectory\n",
    "\n",
    "            try:\n",
    "                results_logger_of_config = ResultLogger(input={\n",
    "                                            \"results_filename\" : RESULTS_PATH,\n",
    "                                            \"base_directory\" : f\"{configuration_path}\\\\{configurations_results_relative_path}\",\n",
    "                                            \"artifact_relative_directory\" : '',\n",
    "                                            \"create_new_directory\" : False\n",
    "\n",
    "                                          })\n",
    "            \n",
    "                results_logger_of_config.proccess_input()\n",
    "\n",
    "                results_of_configurations[configuration_name] = results_logger_of_config\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Did not manage to store configuration {configuration_name} due to error {e}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"WARNING: Configuration path with name {configuration_name} is not a directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Configurations:  {results_of_configurations.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global view of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_optimization_results.plot_bar_graph(x_axis='experiment', y_axis='result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruned Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_optuna_trials = [trial for trial in optuna_study.trials if trial.state == optuna.trial.TrialState.PRUNED]\n",
    "\n",
    "pruned_optuna_trials_per_steps : dict[int, list[optuna.trial.FrozenTrial]] = {} #the pruned trials by the number of completed steps\n",
    "\n",
    "for pruned_optuna_trial in pruned_optuna_trials:\n",
    "    \n",
    "    n_completed_steps = len(pruned_optuna_trial.intermediate_values)\n",
    "    \n",
    "    try:\n",
    "        list_of_pruned = pruned_optuna_trials_per_steps[n_completed_steps]\n",
    "    \n",
    "    except:\n",
    "        list_of_pruned = []\n",
    "        pruned_optuna_trials_per_steps[n_completed_steps] = list_of_pruned    \n",
    "        \n",
    "    list_of_pruned.append(pruned_optuna_trial)\n",
    "\n",
    "\n",
    "for list_of_pruned in pruned_optuna_trials_per_steps.values():\n",
    "    list_of_pruned.sort(key=lambda trial: trial.value) \n",
    "    \n",
    "    \n",
    "\n",
    "pruned_trials = [f'configuration_{trial.number + 1}' for trial in optuna_study.trials if trial.state == optuna.trial.TrialState.PRUNED]\n",
    "\n",
    "print(f\"Pruned trials: {pruned_trials}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst Pruned Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORST_PRUNED = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_completed_steps in pruned_optuna_trials_per_steps.keys():\n",
    "    \n",
    "    pruned_optuna_trials : list[optuna.Trial] = pruned_optuna_trials_per_steps[n_completed_steps][0:WORST_PRUNED] \n",
    "    \n",
    "    pruned_trials = [f'configuration_{trial.number}' for trial in pruned_optuna_trials ]\n",
    "\n",
    "    for configuration_name in pruned_trials:\n",
    "        \n",
    "        results_logger = results_of_configurations[configuration_name]\n",
    "    \n",
    "        study_of_configuration(configuration_name, results_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Pruned Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PRUNED = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_completed_steps in pruned_optuna_trials_per_steps.keys():\n",
    "    \n",
    "    #correct this\n",
    "    pruned_optuna_trials : list[optuna.Trial] = pruned_optuna_trials_per_steps[n_completed_steps][-BEST_PRUNED:] \n",
    "    \n",
    "    pruned_trials = [f'configuration_{trial.number}' for trial in pruned_optuna_trials ]\n",
    "\n",
    "    for configuration_name in pruned_trials:\n",
    "        \n",
    "        results_logger = results_of_configurations[configuration_name]\n",
    "    \n",
    "        study_of_configuration(configuration_name, results_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed Trials Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_optuna_trials = [trial for trial in optuna_study.trials if trial.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "completed_optuna_trials.sort(key=lambda trial: trial.value) # sort given the trial value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst Completed Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORST = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "worst_optuna_trials = completed_optuna_trials[0:N_WORST]\n",
    "\n",
    "worst_configurations_to_study = [f\"configuration_{trial.number}\" for trial in worst_optuna_trials]\n",
    "\n",
    "print(f\"Worst configurations to study: {worst_configurations_to_study}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for configuration_name in worst_configurations_to_study:\n",
    "    \n",
    "    results_logger = results_of_configurations[configuration_name]\n",
    "\n",
    "    study_of_configuration(configuration_name, results_logger)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Completed Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_completed_optuna_trials = [trial for trial in optuna_study.trials if trial.state == optuna.trial.TrialState.RUNNING or trial.state == optuna.trial.TrialState.WAITING]\n",
    "\n",
    "non_completed_optuna_trials.sort(key=lambda trial: trial.value) # sort given the trial value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All non completed trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_completed_trials = [f\"configuration_{trial.number}\" for trial in non_completed_optuna_trials]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for configuration_name in non_completed_trials:\n",
    "    \n",
    "    results_logger = results_of_configurations[configuration_name]\n",
    "\n",
    "    study_of_configuration(configuration_name, results_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BEST = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optuna_trials = completed_optuna_trials[(len(completed_optuna_trials) - N_BEST):]\n",
    "\n",
    "best_configurations_to_study = [f\"configuration_{trial.number}\" for trial in best_optuna_trials]\n",
    "\n",
    "print(f\"Best configurations to study: {best_configurations_to_study}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for configuration_name in best_configurations_to_study:\n",
    "    \n",
    "    results_logger = results_of_configurations[configuration_name]\n",
    "\n",
    "    study_of_configuration(configuration_name, results_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smaller study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations_to_study = ['configuration_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "\n",
    "    for configuration_name in configurations_to_study:\n",
    "    \n",
    "        results_logger = results_of_configurations[configuration_name]\n",
    "\n",
    "        study_of_configuration(configuration_name, results_logger)\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\\nAvailable keys are {results_of_configurations.keys()}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_in_study = []\n",
    "# agents_in_study = [\"agent_1\", \"agent_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_to_study : dict[str, ResultLogger]= {}\n",
    "\n",
    "for configuration_name in configurations_to_study:\n",
    "    \n",
    "    results_logger = results_of_configurations[configuration_name]\n",
    "    \n",
    "    for agent_name in agents_in_study:\n",
    "      \n",
    "        agent_results_logger = ResultLogger(input={\n",
    "                                            \"logger_directory\" : f\"{results_logger.lg.logDir}\\\\{agent_name}\",\n",
    "                                            \"filename\" : RESULTS_PATH,\n",
    "                                            \"create_new_directory\" : False\n",
    "                                          })\n",
    "\n",
    "        agents_to_study[f\"{configuration_name}_{agent_name}\"] = agent_results_logger\n",
    "        \n",
    "        agent_results_logger.proccess_input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent_name, agent_results_logger in agents_to_study.items():\n",
    "    \n",
    "    study_of_configuration(agent_name, agent_results_logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
