{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\")) #make the folder \"automl\" part of this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from automl.meta_rl.hyperparameter_suggestion import HyperparameterSuggestion\n",
    "\n",
    "\n",
    "hyperparameters_to_change : list[HyperparameterSuggestion] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_to_change = [ *hyperparameters_to_change, \n",
    "                                \n",
    "                             HyperparameterSuggestion(\n",
    "                                name='num_episodes', \n",
    "                                hyperparameter_localizations= [\n",
    "                                    ('RLPipelineComponent', [\"num_episodes\"])\n",
    "                                ],\n",
    "                                #value_suggestion = ('int', {'low':200, 'high':800, 'step':100}) \n",
    "                                value_suggestion = ('int', {'low':2000, 'high':2000}) \n",
    "                            ),\n",
    "                            ]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_to_change = [ *hyperparameters_to_change, \n",
    "                                \n",
    "                            HyperparameterSuggestion(\n",
    "                                name='discount_factor', \n",
    "                                hyperparameter_localizations= [\n",
    "                                    ('RLPipelineComponent', [\"agents_input\", \"discount_factor\"])\n",
    "                                ],\n",
    "                                value_suggestion = ('float', {'low':0.5, 'high':0.99}) \n",
    "                            )\n",
    "                            ]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hyperparameters_to_change = [ *hyperparameters_to_change, \n",
    "\n",
    "                             HyperparameterSuggestion(\n",
    "                                name='epsilon_start', \n",
    "                                hyperparameter_localizations= [\n",
    "                                    ('RLPipelineComponent', [\"agents_input\", \"exploration_strategy_input\", \"epsilon_start\"])\n",
    "                                ],\n",
    "                                value_suggestion = ('float', {'low':0.95, 'high':0.999}) \n",
    "                            ),\n",
    "                             HyperparameterSuggestion(\n",
    "                                name='epsilon_end', \n",
    "                                hyperparameter_localizations= [\n",
    "                                    ('RLPipelineComponent', [\"agents_input\", \"exploration_strategy_input\", \"epsilon_end\"])\n",
    "                                ],\n",
    "                                value_suggestion = ('float', {'low':0.05, 'high':0.3}) \n",
    "                            ),\n",
    "                             HyperparameterSuggestion(\n",
    "                                name='epsilon_decay', \n",
    "                                hyperparameter_localizations= [\n",
    "                                    ('RLPipelineComponent', [\"agents_input\", \"exploration_strategy_input\", \"epsilon_decay\"])\n",
    "                                ],\n",
    "                                value_suggestion = ('float', {'low':0.95, 'high':0.9999}) \n",
    "                             )\n",
    "                             \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_to_change = [ *hyperparameters_to_change, \n",
    "                             \n",
    "                             HyperparameterSuggestion(\n",
    "                                name='hidden_layers', \n",
    "                                hyperparameter_localizations= [\n",
    "                                    ('RLPipelineComponent', [\"agents_input\", \"model_input\", \"hidden_layers\"])\n",
    "                                ],\n",
    "                                value_suggestion = ('int', {'low':2, 'high':5}) \n",
    "                            ),\n",
    "                             HyperparameterSuggestion(\n",
    "                                name='hidden_size', \n",
    "                                hyperparameter_localizations= [\n",
    "                                    ('RLPipelineComponent', [\"agents_input\", \"model_input\", \"hidden_size\"])\n",
    "                                ],\n",
    "                                value_suggestion = ('cat', {'choices' : [16, 32, 64, 128]}) \n",
    "                            )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_to_change = [ *hyperparameters_to_change, \n",
    "                             \n",
    "                             HyperparameterSuggestion(\n",
    "                                name='target_update_rate', \n",
    "                                hyperparameter_localizations= [\n",
    "                                    ('RLPipelineComponent', [\"agents_input\", \"learner_input\", \"target_update_rate\"])\n",
    "                                ],\n",
    "                                value_suggestion = ('float', {'low':0.01, 'high':0.15}) \n",
    "                            ),\n",
    "                             HyperparameterSuggestion(\n",
    "                                name='learning_rate', \n",
    "                                hyperparameter_localizations= [\n",
    "                                    ('RLPipelineComponent', [\"agents_input\", \"learner_input\", \"optimizer_input\", \"learning_rate\"])\n",
    "                                ],\n",
    "                                value_suggestion = ('float', {'low':0.000001, 'high':0.1}) \n",
    "                            )            \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_to_change = [ *hyperparameters_to_change, \n",
    "                             \n",
    "                             HyperparameterSuggestion(\n",
    "                                name='memory_capacity', \n",
    "                                hyperparameter_localizations= [\n",
    "                                    ('RLPipelineComponent', [\"agents_input\", \"memory_input\", \"capacity\"])\n",
    "                                ],\n",
    "                                value_suggestion = ('int', {'low':100, 'high':1000}) \n",
    "                            )\n",
    "                           \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.base_configurations.base_configurations import load_configuration_dict\n",
    "\n",
    "hp_optimization_input = {\n",
    "    \"configuration_dict\" : load_configuration_dict('basic_rl'),\n",
    "    \"hyperparameters_range_list\" : hyperparameters_to_change,\n",
    "    \"n_trials\" : 20\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.meta_rl.hp_optimization_pipeline import HyperparameterOptimizationPipeline\n",
    "\n",
    "hp_optimization_pipeline = HyperparameterOptimizationPipeline(hp_optimization_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 15:19:56,241] A new study created in memory with name: no-name-3f286d31-cdae-4f10-9beb-ec6a71dd3cda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_log_directory for object HyperparameterOptimizationPipeline\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17, with name:\n",
      "WARNING: Log directory already existed, was this inteded behaviour?\n",
      "Type of object with name: <class 'automl.meta_rl.hp_optimization_pipeline.HyperparameterOptimizationPipeline'> and name passed: \n",
      "HyperparameterOptimizationPipeline: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\n",
      "HyperparameterOptimizationPipeline: Hyperparameter names: ['num_episodes', 'discount_factor', 'epsilon_start', 'epsilon_end', 'epsilon_decay', 'hidden_layers', 'hidden_size', 'target_update_rate', 'learning_rate', 'memory_capacity']\n",
      "HyperparameterOptimizationPipeline: Creating component to test\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17, with name:configuration_1\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_1\n",
      "HyperparameterOptimizationPipeline: Created component with name configuration_1\n",
      "HyperparameterOptimizationPipeline: Starting new training with hyperparameter cofiguration\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: num_episodes: 2\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: discount_factor: 0.7980695129026047\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_start: 0.9973120858666872\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_end: 0.16492461015851623\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_decay: 0.992921387695795\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_layers: 4\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_size: 128\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: target_update_rate: 0.02359297662911433\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: learning_rate: 0.09748547736063735\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: memory_capacity: 243\n",
      "Type of object with name: <class 'automl.rl.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_1\n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: The model will trained and evaluated on: cuda\n",
      "RLPipelineComponent: No agents defined, creating them...\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized agents\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x00000203DC7AADF0> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_1, with name:agent_1\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_1\\agent_1\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x00000203DC7AAE20> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_1, with name:agent_2\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_1\\agent_2\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key state_memory_size passed to component RLTrainerComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLTrainerComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_1\n",
      "RLTrainerComponent: Agent paddle_0 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Agent paddle_1 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Starting to run episodes of training\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_1\\agent_1\n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing agent with more than one state memory size (2)\n",
      "agent_1: State length is 480\n",
      "agent_1: Initializing policy model...\n",
      "agent_1: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_1\\agent_2\n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing agent with more than one state memory size (2)\n",
      "agent_2: State length is 480\n",
      "agent_2: Initializing policy model...\n",
      "agent_2: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "AgentTrainer: Setting up episode 1\n",
      "AgentTrainer: Setting up episode 1\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "AgentTrainer: Ended episode: 1 with duration: 34, total reward: 3.666666666666668\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_1\\agent_1\n",
      "AgentTrainer: Ended episode: 1 with duration: 34, total reward: -6.333333333333332\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_1\\agent_2\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_1\n",
      "AgentTrainer: Setting up episode 2\n",
      "AgentTrainer: Setting up episode 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "[I 2025-03-01 15:19:58,913] Trial 0 finished with value: 3.5555555555555634 and parameters: {'num_episodes': 2, 'discount_factor': 0.7980695129026047, 'epsilon_start': 0.9973120858666872, 'epsilon_end': 0.16492461015851623, 'epsilon_decay': 0.992921387695795, 'hidden_layers': 4, 'hidden_size': 128, 'target_update_rate': 0.02359297662911433, 'learning_rate': 0.09748547736063735, 'memory_capacity': 243}. Best is trial 0 with value: 3.5555555555555634.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentTrainer: Ended episode: 2 with duration: 30, total reward: 3.222222222222223\n",
      "AgentTrainer: Ended episode: 2 with duration: 30, total reward: -6.777777777777777\n",
      "AgentTrainer: Values of exploraion strategy: {'n_random': 46, 'n_greedy': 18}\n",
      "AgentTrainer: Values of exploraion strategy: {'n_random': 49, 'n_greedy': 15}\n",
      "Last results: {'episode': 2, 'episode_steps': 60, 'avg_reward': -0.059259259259259386, 'total_reward': -3.5555555555555634}\n",
      "Result: -3.5555555555555634\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\n",
      "HyperparameterOptimizationPipeline: Creating component to test\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17, with name:configuration_2\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_2\n",
      "HyperparameterOptimizationPipeline: Created component with name configuration_2\n",
      "HyperparameterOptimizationPipeline: Starting new training with hyperparameter cofiguration\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: num_episodes: 2\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: discount_factor: 0.6203756495153998\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_start: 0.965964217713886\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_end: 0.20076342651761098\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_decay: 0.9518900986535046\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_layers: 5\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_size: 16\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: target_update_rate: 0.08812843358623534\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: learning_rate: 0.05541392906175839\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: memory_capacity: 692\n",
      "Type of object with name: <class 'automl.rl.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_2\n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: The model will trained and evaluated on: cuda\n",
      "RLPipelineComponent: No agents defined, creating them...\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized agents\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x00000203DE4A54C0> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_2, with name:agent_1\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_2\\agent_1\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x00000203DE4A5DF0> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_2, with name:agent_2\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_2\\agent_2\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key state_memory_size passed to component RLTrainerComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLTrainerComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_2\n",
      "RLTrainerComponent: Agent paddle_0 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Agent paddle_1 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Starting to run episodes of training\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_2\\agent_1\n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing agent with more than one state memory size (2)\n",
      "agent_1: State length is 480\n",
      "agent_1: Initializing policy model...\n",
      "agent_1: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_2\\agent_2\n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing agent with more than one state memory size (2)\n",
      "agent_2: State length is 480\n",
      "agent_2: Initializing policy model...\n",
      "agent_2: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "AgentTrainer: Setting up episode 1\n",
      "AgentTrainer: Setting up episode 1\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "AgentTrainer: Ended episode: 1 with duration: 30, total reward: 3.222222222222223\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_2\\agent_1\n",
      "AgentTrainer: Ended episode: 1 with duration: 30, total reward: -6.777777777777777\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_2\\agent_2\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_2\n",
      "AgentTrainer: Setting up episode 2\n",
      "AgentTrainer: Setting up episode 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentTrainer: In episode 1, optimizing at step 40 that is the total step 100\n",
      "AgentTrainer: Optimizing agent...\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "WARNING: input with key 3 passed to component AdamOptimizer but not in its input signature, will be ignored\n",
      "AgentTrainer: Optimization took 1.0981075763702393 seconds\n",
      "AgentTrainer: In episode 1, optimizing at step 40 that is the total step 100\n",
      "AgentTrainer: Optimizing agent...\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "WARNING: input with key 3 passed to component AdamOptimizer but not in its input signature, will be ignored\n",
      "AgentTrainer: Optimization took 0.44045352935791016 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 15:20:02,017] Trial 1 finished with value: -0.4444444444444233 and parameters: {'num_episodes': 2, 'discount_factor': 0.6203756495153998, 'epsilon_start': 0.965964217713886, 'epsilon_end': 0.20076342651761098, 'epsilon_decay': 0.9518900986535046, 'hidden_layers': 5, 'hidden_size': 16, 'target_update_rate': 0.08812843358623534, 'learning_rate': 0.05541392906175839, 'memory_capacity': 692}. Best is trial 1 with value: -0.4444444444444233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentTrainer: Ended episode: 2 with duration: 48, total reward: 5.222222222222219\n",
      "AgentTrainer: Ended episode: 2 with duration: 48, total reward: -4.777777777777781\n",
      "AgentTrainer: Values of exploraion strategy: {'n_random': 51, 'n_greedy': 27}\n",
      "AgentTrainer: Values of exploraion strategy: {'n_random': 47, 'n_greedy': 31}\n",
      "Last results: {'episode': 2, 'episode_steps': 96, 'avg_reward': 0.00462962962962941, 'total_reward': 0.4444444444444233}\n",
      "Result: 0.4444444444444233\n",
      "HyperparameterOptimizationPipeline: Creating component to test\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17, with name:configuration_3\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_3\n",
      "HyperparameterOptimizationPipeline: Created component with name configuration_3\n",
      "HyperparameterOptimizationPipeline: Starting new training with hyperparameter cofiguration\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: num_episodes: 2\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: discount_factor: 0.8436895164392251\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_start: 0.9681793509812575\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_end: 0.05437452068379965\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_decay: 0.9788588205330404\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_layers: 3\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_size: 32\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: target_update_rate: 0.07045443629810856\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: learning_rate: 0.0447959388966971\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: memory_capacity: 667\n",
      "Type of object with name: <class 'automl.rl.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_3\n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: The model will trained and evaluated on: cuda\n",
      "RLPipelineComponent: No agents defined, creating them...\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized agents\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x000002047FFA84C0> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_3, with name:agent_1\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_3\\agent_1\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x000002047FFA8DF0> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_3, with name:agent_2\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_3\\agent_2\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key state_memory_size passed to component RLTrainerComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLTrainerComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_3\n",
      "RLTrainerComponent: Agent paddle_0 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Agent paddle_1 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Starting to run episodes of training\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_3\\agent_1\n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing agent with more than one state memory size (2)\n",
      "agent_1: State length is 480\n",
      "agent_1: Initializing policy model...\n",
      "agent_1: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_3\\agent_2\n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing agent with more than one state memory size (2)\n",
      "agent_2: State length is 480\n",
      "agent_2: Initializing policy model...\n",
      "agent_2: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "AgentTrainer: Setting up episode 1\n",
      "AgentTrainer: Setting up episode 1\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "AgentTrainer: Ended episode: 1 with duration: 48, total reward: 5.222222222222219\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_3\\agent_1\n",
      "AgentTrainer: Ended episode: 1 with duration: 48, total reward: -4.777777777777781\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_3\\agent_2\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_3\n",
      "AgentTrainer: Setting up episode 2\n",
      "AgentTrainer: Setting up episode 2\n",
      "AgentTrainer: In episode 1, optimizing at step 4 that is the total step 100\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 0.0 seconds\n",
      "AgentTrainer: In episode 1, optimizing at step 4 that is the total step 100\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 0.0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "[I 2025-03-01 15:20:03,774] Trial 2 finished with value: -0.4444444444444233 and parameters: {'num_episodes': 2, 'discount_factor': 0.8436895164392251, 'epsilon_start': 0.9681793509812575, 'epsilon_end': 0.05437452068379965, 'epsilon_decay': 0.9788588205330404, 'hidden_layers': 3, 'hidden_size': 32, 'target_update_rate': 0.07045443629810856, 'learning_rate': 0.0447959388966971, 'memory_capacity': 667}. Best is trial 1 with value: -0.4444444444444233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentTrainer: Ended episode: 2 with duration: 48, total reward: 5.222222222222219\n",
      "AgentTrainer: Ended episode: 2 with duration: 48, total reward: -4.777777777777781\n",
      "AgentTrainer: Values of exploraion strategy: {'n_random': 66, 'n_greedy': 30}\n",
      "AgentTrainer: Values of exploraion strategy: {'n_random': 69, 'n_greedy': 27}\n",
      "Last results: {'episode': 2, 'episode_steps': 96, 'avg_reward': 0.00462962962962941, 'total_reward': 0.4444444444444233}\n",
      "Result: 0.4444444444444233\n",
      "HyperparameterOptimizationPipeline: Creating component to test\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17, with name:configuration_4\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_4\n",
      "HyperparameterOptimizationPipeline: Created component with name configuration_4\n",
      "HyperparameterOptimizationPipeline: Starting new training with hyperparameter cofiguration\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: num_episodes: 2\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: discount_factor: 0.9254582545000436\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_start: 0.9686687544533807\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_end: 0.12407676288514279\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_decay: 0.9654241767242389\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_layers: 3\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_size: 16\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: target_update_rate: 0.10208756645610337\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: learning_rate: 0.08028001927566804\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: memory_capacity: 831\n",
      "Type of object with name: <class 'automl.rl.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_4\n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: The model will trained and evaluated on: cuda\n",
      "RLPipelineComponent: No agents defined, creating them...\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized agents\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x00000203DE4A5A00> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_4, with name:agent_1\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_4\\agent_1\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x00000203DE4A51C0> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_4, with name:agent_2\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_4\\agent_2\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key state_memory_size passed to component RLTrainerComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLTrainerComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_4\n",
      "RLTrainerComponent: Agent paddle_0 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Agent paddle_1 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Starting to run episodes of training\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_4\\agent_1\n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing agent with more than one state memory size (2)\n",
      "agent_1: State length is 480\n",
      "agent_1: Initializing policy model...\n",
      "agent_1: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_4\\agent_2\n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing agent with more than one state memory size (2)\n",
      "agent_2: State length is 480\n",
      "agent_2: Initializing policy model...\n",
      "agent_2: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "AgentTrainer: Setting up episode 1\n",
      "AgentTrainer: Setting up episode 1\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "AgentTrainer: Ended episode: 1 with duration: 35, total reward: 3.777777777777779\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_4\\agent_1\n",
      "AgentTrainer: Ended episode: 1 with duration: 35, total reward: -6.222222222222221\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_4\\agent_2\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_4\n",
      "AgentTrainer: Setting up episode 2\n",
      "AgentTrainer: Setting up episode 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentTrainer: In episode 1, optimizing at step 30 that is the total step 100\n",
      "AgentTrainer: Optimizing agent...\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "WARNING: input with key 3 passed to component AdamOptimizer but not in its input signature, will be ignored\n",
      "AgentTrainer: Optimization took 0.30547451972961426 seconds\n",
      "AgentTrainer: In episode 1, optimizing at step 30 that is the total step 100\n",
      "AgentTrainer: Optimizing agent...\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "WARNING: input with key 3 passed to component AdamOptimizer but not in its input signature, will be ignored\n",
      "AgentTrainer: Optimization took 0.2840149402618408 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 15:20:06,942] Trial 3 finished with value: -16.44444444444437 and parameters: {'num_episodes': 2, 'discount_factor': 0.9254582545000436, 'epsilon_start': 0.9686687544533807, 'epsilon_end': 0.12407676288514279, 'epsilon_decay': 0.9654241767242389, 'hidden_layers': 3, 'hidden_size': 16, 'target_update_rate': 0.10208756645610337, 'learning_rate': 0.08028001927566804, 'memory_capacity': 831}. Best is trial 3 with value: -16.44444444444437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentTrainer: Ended episode: 2 with duration: 120, total reward: 13.222222222222191\n",
      "AgentTrainer: Ended episode: 2 with duration: 120, total reward: 3.2222222222221912\n",
      "AgentTrainer: Values of exploraion strategy: {'n_random': 96, 'n_greedy': 59}\n",
      "AgentTrainer: Values of exploraion strategy: {'n_random': 98, 'n_greedy': 57}\n",
      "Last results: {'episode': 2, 'episode_steps': 240, 'avg_reward': 0.0685185185185182, 'total_reward': 16.44444444444437}\n",
      "Result: 16.44444444444437\n",
      "HyperparameterOptimizationPipeline: Creating component to test\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17, with name:configuration_5\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_5\n",
      "HyperparameterOptimizationPipeline: Created component with name configuration_5\n",
      "HyperparameterOptimizationPipeline: Starting new training with hyperparameter cofiguration\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: num_episodes: 2\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: discount_factor: 0.8026535336644378\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_start: 0.9756024439974998\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_end: 0.23334119393858754\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_decay: 0.9919326657041537\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_layers: 5\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_size: 64\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: target_update_rate: 0.07876826155245541\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: learning_rate: 0.0759798025422459\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: memory_capacity: 594\n",
      "Type of object with name: <class 'automl.rl.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_5\n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: The model will trained and evaluated on: cuda\n",
      "RLPipelineComponent: No agents defined, creating them...\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized agents\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x000002047FF9A4F0> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_5, with name:agent_1\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_5\\agent_1\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x000002047FF9AE20> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_5, with name:agent_2\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_5\\agent_2\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key state_memory_size passed to component RLTrainerComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLTrainerComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_5\n",
      "RLTrainerComponent: Agent paddle_0 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Agent paddle_1 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Starting to run episodes of training\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_5\\agent_1\n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing agent with more than one state memory size (2)\n",
      "agent_1: State length is 480\n",
      "agent_1: Initializing policy model...\n",
      "agent_1: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_5\\agent_2\n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing agent with more than one state memory size (2)\n",
      "agent_2: State length is 480\n",
      "agent_2: Initializing policy model...\n",
      "agent_2: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "AgentTrainer: Setting up episode 1\n",
      "AgentTrainer: Setting up episode 1\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "AgentTrainer: Ended episode: 1 with duration: 76, total reward: 8.33333333333332\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_5\\agent_1\n",
      "AgentTrainer: Ended episode: 1 with duration: 76, total reward: -1.6666666666666803\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_5\\agent_2\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_5\n",
      "AgentTrainer: Setting up episode 2\n",
      "AgentTrainer: Setting up episode 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "[I 2025-03-01 15:20:08,692] Trial 4 finished with value: 3.5555555555555634 and parameters: {'num_episodes': 2, 'discount_factor': 0.8026535336644378, 'epsilon_start': 0.9756024439974998, 'epsilon_end': 0.23334119393858754, 'epsilon_decay': 0.9919326657041537, 'hidden_layers': 5, 'hidden_size': 64, 'target_update_rate': 0.07876826155245541, 'learning_rate': 0.0759798025422459, 'memory_capacity': 594}. Best is trial 3 with value: -16.44444444444437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentTrainer: Ended episode: 2 with duration: 30, total reward: 3.222222222222223\n",
      "AgentTrainer: Ended episode: 2 with duration: 30, total reward: -6.777777777777777\n",
      "AgentTrainer: Values of exploraion strategy: {'n_random': 85, 'n_greedy': 21}\n",
      "AgentTrainer: Values of exploraion strategy: {'n_random': 88, 'n_greedy': 18}\n",
      "Last results: {'episode': 2, 'episode_steps': 60, 'avg_reward': -0.059259259259259386, 'total_reward': -3.5555555555555634}\n",
      "Result: -3.5555555555555634\n",
      "HyperparameterOptimizationPipeline: Creating component to test\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17, with name:configuration_6\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_6\n",
      "HyperparameterOptimizationPipeline: Created component with name configuration_6\n",
      "HyperparameterOptimizationPipeline: Starting new training with hyperparameter cofiguration\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: num_episodes: 2\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: discount_factor: 0.9506285495815807\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_start: 0.9533128955452946\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_end: 0.13722470346293814\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_decay: 0.9612905941787039\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_layers: 5\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_size: 16\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: target_update_rate: 0.14059334819892394\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: learning_rate: 0.06407080077096763\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: memory_capacity: 960\n",
      "Type of object with name: <class 'automl.rl.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_6\n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: The model will trained and evaluated on: cuda\n",
      "RLPipelineComponent: No agents defined, creating them...\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized agents\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x0000020466C81460> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_6, with name:agent_1\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_6\\agent_1\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x0000020466C7DB20> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_6, with name:agent_2\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_6\\agent_2\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key state_memory_size passed to component RLTrainerComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLTrainerComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_6\n",
      "RLTrainerComponent: Agent paddle_0 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Agent paddle_1 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Starting to run episodes of training\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_6\\agent_1\n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing agent with more than one state memory size (2)\n",
      "agent_1: State length is 480\n",
      "agent_1: Initializing policy model...\n",
      "agent_1: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_6\\agent_2\n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing agent with more than one state memory size (2)\n",
      "agent_2: State length is 480\n",
      "agent_2: Initializing policy model...\n",
      "agent_2: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "AgentTrainer: Setting up episode 1\n",
      "AgentTrainer: Setting up episode 1\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "AgentTrainer: Ended episode: 1 with duration: 60, total reward: 6.555555555555547\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_6\\agent_1\n",
      "AgentTrainer: Ended episode: 1 with duration: 60, total reward: -3.4444444444444526\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_6\\agent_2\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "ResultLogger: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_6\n",
      "AgentTrainer: Setting up episode 2\n",
      "AgentTrainer: Setting up episode 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:71: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df), ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentTrainer: In episode 1, optimizing at step 80 that is the total step 200\n",
      "AgentTrainer: Optimizing agent...\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "WARNING: input with key 3 passed to component AdamOptimizer but not in its input signature, will be ignored\n",
      "AgentTrainer: Optimization took 0.3186352252960205 seconds\n",
      "AgentTrainer: In episode 1, optimizing at step 80 that is the total step 200\n",
      "AgentTrainer: Optimizing agent...\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "WARNING: input with key 3 passed to component AdamOptimizer but not in its input signature, will be ignored\n",
      "AgentTrainer: Optimization took 2.5735039710998535 seconds\n",
      "AgentTrainer: In episode 1, optimizing at step 180 that is the total step 300\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 3.5632855892181396 seconds\n",
      "AgentTrainer: In episode 1, optimizing at step 180 that is the total step 300\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 3.5350663661956787 seconds\n",
      "AgentTrainer: In episode 1, optimizing at step 280 that is the total step 400\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 3.571986436843872 seconds\n",
      "AgentTrainer: In episode 1, optimizing at step 280 that is the total step 400\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 3.522630214691162 seconds\n",
      "AgentTrainer: In episode 1, optimizing at step 380 that is the total step 500\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 3.574381113052368 seconds\n",
      "AgentTrainer: In episode 1, optimizing at step 380 that is the total step 500\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 3.6288044452667236 seconds\n",
      "AgentTrainer: In episode 1, optimizing at step 480 that is the total step 600\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 3.5455756187438965 seconds\n",
      "AgentTrainer: In episode 1, optimizing at step 480 that is the total step 600\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 3.576897621154785 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 15:20:52,862] Trial 5 finished with value: -111.00000000000215 and parameters: {'num_episodes': 2, 'discount_factor': 0.9506285495815807, 'epsilon_start': 0.9533128955452946, 'epsilon_end': 0.13722470346293814, 'epsilon_decay': 0.9612905941787039, 'hidden_layers': 5, 'hidden_size': 16, 'target_update_rate': 0.14059334819892394, 'learning_rate': 0.06407080077096763, 'memory_capacity': 960}. Best is trial 5 with value: -111.00000000000215.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLTrainerComponent: In episode 1, reached step 1000 that is beyond the current limit, 1000\n",
      "AgentTrainer: Ended episode: 2 with duration: 500, total reward: 55.44444444444501\n",
      "AgentTrainer: Ended episode: 2 with duration: 500, total reward: 55.555555555556126\n",
      "AgentTrainer: Values of exploraion strategy: {'n_random': 283, 'n_greedy': 277}\n",
      "AgentTrainer: Values of exploraion strategy: {'n_random': 274, 'n_greedy': 286}\n",
      "Last results: {'episode': 2, 'episode_steps': 1000, 'avg_reward': 0.11100000000000215, 'total_reward': 111.00000000000215}\n",
      "Result: 111.00000000000215\n",
      "HyperparameterOptimizationPipeline: Creating component to test\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17, with name:configuration_7\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_7\n",
      "HyperparameterOptimizationPipeline: Created component with name configuration_7\n",
      "HyperparameterOptimizationPipeline: Starting new training with hyperparameter cofiguration\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: num_episodes: 2\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: discount_factor: 0.9501324956742558\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_start: 0.9949271191672096\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_end: 0.2991417948751483\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: epsilon_decay: 0.9649965258781966\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_layers: 5\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: hidden_size: 32\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: target_update_rate: 0.08648524663727791\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: learning_rate: 0.09167816710133085\n",
      "Passing input to component: RLPipelineComponent\n",
      "HyperparameterOptimizationPipeline: memory_capacity: 109\n",
      "Type of object with name: <class 'automl.rl.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_7\n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: The model will trained and evaluated on: cuda\n",
      "RLPipelineComponent: No agents defined, creating them...\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized agents\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x00000203DE432DF0> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_7, with name:agent_1\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_7\\agent_1\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x000002047FFB8760> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_7, with name:agent_2\n",
      "Log directory did not exist, creating it at: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_7\\agent_2\n",
      "WARNING: input with key 1 passed to component AgentSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key state_memory_size passed to component RLTrainerComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLTrainerComponent: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_7\n",
      "RLTrainerComponent: Agent paddle_0 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Agent paddle_1 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Starting to run episodes of training\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_7\\agent_1\n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing agent with more than one state memory size (2)\n",
      "agent_1: State length is 480\n",
      "agent_1: Initializing policy model...\n",
      "agent_1: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "WARNING: input with key 2 passed to component EpsilonGreedyStrategy but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\HyperparameterOptimizationPipeline_17\\configuration_7\\agent_2\n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing agent with more than one state memory size (2)\n",
      "agent_2: State length is 480\n",
      "agent_2: Initializing policy model...\n",
      "agent_2: Creating policy model...\n",
      "WARNING: input with key 2 passed to component FullyConnectedModelSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component DeepQLearnerSchema but not in its input signature, will be ignored\n",
      "WARNING: input with key 2 passed to component MemoryComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "AgentTrainer: Setting up episode 1\n",
      "AgentTrainer: Setting up episode 1\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "Created fully connected model with input size = 806400, and output size = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-01 15:20:54,143] Trial 6 failed with parameters: {'num_episodes': 2, 'discount_factor': 0.9501324956742558, 'epsilon_start': 0.9949271191672096, 'epsilon_end': 0.2991417948751483, 'epsilon_decay': 0.9649965258781966, 'hidden_layers': 5, 'hidden_size': 32, 'target_update_rate': 0.08648524663727791, 'learning_rate': 0.09167816710133085, 'memory_capacity': 109} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\meta_rl\\hp_optimization_pipeline.py\", line 176, in <lambda>\n",
      "    study.optimize( lambda trial : self.objective(trial),\n",
      "  File \"c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\meta_rl\\hp_optimization_pipeline.py\", line 144, in objective\n",
      "    component_to_test.train()\n",
      "  File \"c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py\", line 379, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\rl_pipeline.py\", line 191, in train\n",
      "    self.rl_trainer.run_episodes(n_episodes=n_episodes)\n",
      "  File \"c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py\", line 379, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\trainers\\rl_trainer_component.py\", line 110, in run_episodes\n",
      "    self.__run_episode(self.values[\"episodes_done\"])\n",
      "  File \"c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\trainers\\rl_trainer_component.py\", line 153, in __run_episode\n",
      "    self.agents_in_training[other_agent_name].observe_new_state(self.env)\n",
      "  File \"c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\trainers\\agent_trainer_component.py\", line 134, in observe_new_state\n",
      "    self.agent.observe_new_state(env.observe(self.name))\n",
      "  File \"c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\environment\\environment_components.py\", line 110, in observe\n",
      "    return PettingZooEnvironmentLoader.state_translator(self.env.observe(*args), self.device)\n",
      "  File \"c:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\order_enforcing.py\", line 101, in observe\n",
      "    return super().observe(agent)\n",
      "  File \"c:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py\", line 41, in observe\n",
      "    return self.env.observe(agent)\n",
      "  File \"c:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py\", line 41, in observe\n",
      "    return self.env.observe(agent)\n",
      "  File \"c:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\pettingzoo\\butterfly\\cooperative_pong\\cooperative_pong.py\", line 417, in observe\n",
      "    obs = self.env.observe()\n",
      "  File \"c:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\pettingzoo\\butterfly\\cooperative_pong\\cooperative_pong.py\", line 287, in observe\n",
      "    observation = np.array(pygame.surfarray.pixels3d(self.screen))\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-01 15:20:54,188] Trial 6 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mhp_optimization_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:379\u001b[0m, in \u001b[0;36mrequires_input_proccess.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_was_proccessed:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproccess_input()\n\u001b[1;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\meta_rl\\hp_optimization_pipeline.py:176\u001b[0m, in \u001b[0;36mHyperparameterOptimizationPipeline.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;129m@requires_input_proccess\u001b[39m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    175\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(sampler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler)\n\u001b[1;32m--> 176\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_trial\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlg\u001b[38;5;241m.\u001b[39mwriteLine(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\meta_rl\\hp_optimization_pipeline.py:176\u001b[0m, in \u001b[0;36mHyperparameterOptimizationPipeline.run.<locals>.<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;129m@requires_input_proccess\u001b[39m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    175\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(sampler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler)\n\u001b[1;32m--> 176\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize( \u001b[38;5;28;01mlambda\u001b[39;00m trial : \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[0;32m    177\u001b[0m                    n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials,\n\u001b[0;32m    178\u001b[0m                    callbacks\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_trial])\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlg\u001b[38;5;241m.\u001b[39mwriteLine(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\meta_rl\\hp_optimization_pipeline.py:144\u001b[0m, in \u001b[0;36mHyperparameterOptimizationPipeline.objective\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlg\u001b[38;5;241m.\u001b[39mwriteLine(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting new training with hyperparameter cofiguration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_configuration(trial, component_to_test)\n\u001b[1;32m--> 144\u001b[0m \u001b[43mcomponent_to_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtried_configurations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    148\u001b[0m results \u001b[38;5;241m=\u001b[39m component_to_test\u001b[38;5;241m.\u001b[39mget_last_Results()\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:379\u001b[0m, in \u001b[0;36mrequires_input_proccess.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_was_proccessed:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproccess_input()\n\u001b[1;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\rl_pipeline.py:191\u001b[0m, in \u001b[0;36mRLPipelineComponent.train\u001b[1;34m(self, n_episodes)\u001b[0m\n\u001b[0;32m    188\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect() \u001b[38;5;66;03m#this forces the garbage collector to collect any abandoned objects\u001b[39;00m\n\u001b[0;32m    189\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache() \u001b[38;5;66;03m#this clears cache of cuda\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrl_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:379\u001b[0m, in \u001b[0;36mrequires_input_proccess.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_was_proccessed:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproccess_input()\n\u001b[1;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\trainers\\rl_trainer_component.py:110\u001b[0m, in \u001b[0;36mRLTrainerComponent.run_episodes\u001b[1;34m(self, n_episodes)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m#each episode is an instance of playing the game\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episodes_to_do):\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__run_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisodes_done\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_in_training \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents_in_training\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    113\u001b[0m         agent_in_training\u001b[38;5;241m.\u001b[39mend_episode() \n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\trainers\\rl_trainer_component.py:153\u001b[0m, in \u001b[0;36mRLTrainerComponent.__run_episode\u001b[1;34m(self, i_episode)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other_agent_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents_in_training\u001b[38;5;241m.\u001b[39mkeys(): \u001b[38;5;66;03m#make the other agents observe the transiction without remembering it\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m other_agent_name \u001b[38;5;241m!=\u001b[39m agent_name:\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents_in_training\u001b[49m\u001b[43m[\u001b[49m\u001b[43mother_agent_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve_new_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\trainers\\agent_trainer_component.py:134\u001b[0m, in \u001b[0;36mAgentTrainer.observe_new_state\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve_new_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, env : EnvironmentComponent):\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Makes the agent observe a new state, remembering it in case it needs that information in future computations'''\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mobserve_new_state(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\environment\\environment_components.py:110\u001b[0m, in \u001b[0;36mPettingZooEnvironmentLoader.observe\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PettingZooEnvironmentLoader\u001b[38;5;241m.\u001b[39mstate_translator(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\order_enforcing.py:101\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.observe\u001b[1;34m(self, agent)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    100\u001b[0m     EnvLogger\u001b[38;5;241m.\u001b[39merror_observe_before_reset()\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:41\u001b[0m, in \u001b[0;36mBaseWrapper.observe\u001b[1;34m(self, agent)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent: AgentID) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ObsType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:41\u001b[0m, in \u001b[0;36mBaseWrapper.observe\u001b[1;34m(self, agent)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent: AgentID) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ObsType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\pettingzoo\\butterfly\\cooperative_pong\\cooperative_pong.py:417\u001b[0m, in \u001b[0;36mraw_env.observe\u001b[1;34m(self, agent)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent):\n\u001b[1;32m--> 417\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs\n",
      "File \u001b[1;32mc:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\pettingzoo\\butterfly\\cooperative_pong\\cooperative_pong.py:287\u001b[0m, in \u001b[0;36mCooperativePong.observe\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 287\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurfarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixels3d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m     observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrot90(\n\u001b[0;32m    289\u001b[0m         observation, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    290\u001b[0m     )  \u001b[38;5;66;03m# now the obs is laid out as H, W as rows and cols\u001b[39;00m\n\u001b[0;32m    291\u001b[0m     observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfliplr(observation)  \u001b[38;5;66;03m# laid out in the correct order\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hp_optimization_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_optimization_pipeline.save_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\experiments\\HyperparameterOptimizationPipeline_12\n"
     ]
    }
   ],
   "source": [
    "experiment_path = hp_optimization_pipeline.lg.logDir\n",
    "\n",
    "print(experiment_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
