{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook substitutes some classes in an experience by \"debug\" versions of them, which write to file almost every intermidiate step, as to help detect any incoherence in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_STORE_EXPERIMENTS = \"data\\\\rl_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"dqn_multi_agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation before loading experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change logging system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.logger_component import LoggerSchema \n",
    "\n",
    "LoggerSchema.get_schema_parameter_signature(\"write_to_file_when_text_lines_over\").change_default_value(-1)\n",
    "LoggerSchema.get_schema_parameter_signature(\"necessary_logger_level\").change_default_value(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.component_with_results import ResultLogger\n",
    "\n",
    "\n",
    "ResultLogger.get_schema_parameter_signature(\"save_results_on_log\").change_default_value(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The base Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.rl.whole_configurations import rl_multi_agent_pipeline as base_rl_configuration\n",
    "\n",
    "\n",
    "rl_pipeline_config = base_rl_configuration.config_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Configuration Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_pipeline_input = rl_pipeline_config[\"input\"]\n",
    "\n",
    "rl_trainer_tuple = rl_pipeline_input[\"rl_trainer\"]\n",
    "rl_trainer_input = rl_trainer_tuple[1]\n",
    "\n",
    "agents_input = rl_pipeline_input[\"agents_input\"]\n",
    "\n",
    "policy_tuple = agents_input[\"policy\"]\n",
    "policy_input = policy_tuple[1]\n",
    "\n",
    "agents_trainers_input = rl_trainer_input[\"agents_trainers_input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_tuple = agents_trainers_input[\"learner\"]\n",
    "learner_input = learner_tuple[1]\n",
    "\n",
    "optimizer_tuple = learner_input[\"optimizer\"]\n",
    "optimizer_input = optimizer_tuple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_tuple = agents_trainers_input[\"memory\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = rl_pipeline_config[\"input\"][\"environment\"]\n",
    "environment_input = environment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_strategy_tuple = agents_trainers_input[\"exploration_strategy\"]\n",
    "exploration_strategy_input = exploration_strategy_tuple[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes to the base configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to help alter experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_value_in_dict(dict_with_value : dict, key, new_value):\n",
    "    print(f\"Old value for key '{key}': {dict_with_value.get(key, None)}, new value: {new_value}\")\n",
    "    dict_with_value[key] = new_value\n",
    "\n",
    "def remove_value_in_dict(dict_with_value : dict, key, new_value):\n",
    "    print(f\"Old value for key '{key}': {dict_with_value.get(key, None)}, to be removed...\")\n",
    "    dict_with_value.pop(key, None)\n",
    "\n",
    "\n",
    "\n",
    "def substitute_tuple_value_in_dict(dict_with_tuple : dict, key, tuple_index, new_value):\n",
    "\n",
    "    tuple_value : tuple = dict_with_tuple[key]\n",
    "\n",
    "    print(f\"Old value for tuple pos {tuple_index}: {tuple_value[tuple_index]}, new value: {new_value}\")\n",
    "    new_tuple_value = tuple( new_value if tuple_index == i else tuple_value[i] for i in range(len(tuple_value)) )\n",
    "\n",
    "    dict_with_tuple[key] = new_tuple_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other value changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rl_trainer_input[\"limit_total_steps\"] = 1000\n",
    "\n",
    "#rl_trainer_input.pop(\"limit_total_steps\", None)\n",
    "\n",
    "#rl_trainer_input[\"num_episodes\"] = 4000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agents_trainers_input[\"learning_start_step_delay\"] = 5000\n",
    "#agents_trainers_input[\"learning_start_ep_delay\"] = 150\n",
    "\n",
    "#substitute_value_in_dict(agents_trainers_input, \"learning_start_ep_delay\", 2897)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agents_trainers_input[\"optimization_interval\"] = 450\n",
    "\n",
    "#substitute_value_in_dict(agents_trainers_input, \"times_to_learn\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_input[\"clip_grad_norm\"] = 0.1\n",
    "\n",
    "#substitute_value_in_dict(optimizer_input, \"clip_grad_value\", 0.2956984463839789)\n",
    "\n",
    "#substitute_value_in_dict(optimizer_input, \"learning_rate\", 0.006807860813523758)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_value_in_dict(agents_trainers_input, \"discount_factor\", 0.8790365307757482)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_value_in_dict(learner_input, \"target_update_rate\", 0.5511208693081078)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_value_in_dict(exploration_strategy_input, \"epsilon_end\", 0.009535369612528788)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen RL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from automl.rl.rl_pipeline import RLPipelineComponent\n",
    "from automl.utils.json_utils.json_component_utils import gen_component_from\n",
    "\n",
    "rl_pipeline : RLPipelineComponent = gen_component_from(rl_pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment path: data\\rl_training\\dqn_multi_agent_28\n"
     ]
    }
   ],
   "source": [
    "rl_pipeline.pass_input({\n",
    "    \"base_directory\" : PATH_TO_STORE_EXPERIMENTS,\n",
    "                        \"artifact_relative_directory\" : experiment_name,\n",
    "                        \"create_new_directory\" : True,\n",
    "                        \"do_full_setup_of_seed\" : True}\n",
    "                        )\n",
    "\n",
    "experiment_path = rl_pipeline.get_artifact_directory()\n",
    "\n",
    "print(f\"Experiment path: {experiment_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global logger is trying to be activated in directory: data\\rl_training\\dqn_multi_agent_28\n",
      "[2025-11-28 15:13:09] Global logger activation as ended, activated in data\\rl_training\\dqn_multi_agent_28\\_global_logger\n"
     ]
    }
   ],
   "source": [
    "from automl.loggers.global_logger import activate_global_logger\n",
    "\n",
    "activate_global_logger(rl_pipeline.get_artifact_directory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-28 15:13:09] <automl.rl.rl_pipeline.RLPipelineComponent object at 0x000002A4A3048050>: Seed is 3572337724\n",
      "[2025-11-28 15:13:09] <automl.rl.rl_pipeline.RLPipelineComponent object at 0x000002A4A3048050>: Activating full setup of seed\n",
      "[2025-11-28 15:13:09] Python seed 3572337724\n",
      "[2025-11-28 15:13:09] Python seed 3572337724\n",
      "[2025-11-28 15:13:09] Torch seed 976400350\n",
      "[2025-11-28 15:13:09] Numpy seed 2159222677\n",
      "[2025-11-28 15:13:10] Component RLPipelineComponent of type <class 'automl.rl.rl_pipeline.RLPipelineComponent'> is dealing with the exception: 'ImageReverterToSingleChannel' object has no attribute 'device'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ImageReverterToSingleChannel' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrl_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:608\u001b[39m, in \u001b[36mrequires_input_proccess.<locals>.process_input_if_not_processed_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_input_if_not_processed_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m : Component, *args, **kwargs):\n\u001b[32m    607\u001b[39m     \u001b[38;5;28mself\u001b[39m.proccess_input_if_not_proccesd()\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\basic_components\\exec_component.py:117\u001b[39m, in \u001b[36mExecComponent.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    116\u001b[39m     \u001b[38;5;28mself\u001b[39m.values[\u001b[33m\"\u001b[39m\u001b[33mrunning_state\u001b[39m\u001b[33m\"\u001b[39m] = State.ERROR\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__on_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\basic_components\\exec_component.py:80\u001b[39m, in \u001b[36mExecComponent.__on_exception\u001b[39m\u001b[34m(self, exception)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''To be called when a non treated exception happens'''\u001b[39;00m\n\u001b[32m     78\u001b[39m globalWriteLine(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mComponent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is dealing with the exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_deal_with_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexception\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\rl_pipeline.py:241\u001b[39m, in \u001b[36mRLPipelineComponent._deal_with_exception\u001b[39m\u001b[34m(self, exception)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28msuper\u001b[39m()._deal_with_exception(exception)\n\u001b[32m    239\u001b[39m common_exception_handling(\u001b[38;5;28mself\u001b[39m, exception, \u001b[33m'\u001b[39m\u001b[33merror_report.txt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\basic_components\\exec_component.py:111\u001b[39m, in \u001b[36mExecComponent.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m._pre_algorithm()\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28mself\u001b[39m._pos_algorithm()\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_output()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:608\u001b[39m, in \u001b[36mrequires_input_proccess.<locals>.process_input_if_not_processed_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_input_if_not_processed_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m : Component, *args, **kwargs):\n\u001b[32m    607\u001b[39m     \u001b[38;5;28mself\u001b[39m.proccess_input_if_not_proccesd()\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\rl_pipeline.py:266\u001b[39m, in \u001b[36mRLPipelineComponent._algorithm\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;129m@requires_input_proccess\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_algorithm\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Executes the training part of the algorithm with a specified number of episodes < than the number specified\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[33;03m    It then evaluates and returns the results\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#trains the agents in the reinforcement learning pipeline\u001b[39;00m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.component_evaluator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    271\u001b[39m         \u001b[38;5;28mself\u001b[39m.lg.writeLine(\u001b[33m\"\u001b[39m\u001b[33mEvaluating the trained agents...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:608\u001b[39m, in \u001b[36mrequires_input_proccess.<locals>.process_input_if_not_processed_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_input_if_not_processed_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m : Component, *args, **kwargs):\n\u001b[32m    607\u001b[39m     \u001b[38;5;28mself\u001b[39m.proccess_input_if_not_proccesd()\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\rl_pipeline.py:251\u001b[39m, in \u001b[36mRLPipelineComponent.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m gc.collect() \u001b[38;5;66;03m#this forces the garbage collector to collect any abandoned objects\u001b[39;00m\n\u001b[32m    249\u001b[39m torch.cuda.empty_cache() \u001b[38;5;66;03m#this clears cache of cuda\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrl_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_in_between:\n\u001b[32m    254\u001b[39m     \u001b[38;5;28mself\u001b[39m.save_state()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:608\u001b[39m, in \u001b[36mrequires_input_proccess.<locals>.process_input_if_not_processed_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_input_if_not_processed_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m : Component, *args, **kwargs):\n\u001b[32m    607\u001b[39m     \u001b[38;5;28mself\u001b[39m.proccess_input_if_not_proccesd()\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\trainers\\rl_trainer_component.py:260\u001b[39m, in \u001b[36mRLTrainerComponent.run_episodes\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m.lg.writeLine(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting to run episodes with initial values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.values\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)   \n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \u001b[38;5;66;03m# loop of episodes and check end conditions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepisodes_done\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_if_to_end_training_session():\n\u001b[32m    263\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\trainers\\rl_trainer\\parallel_rl_trainer.py:102\u001b[39m, in \u001b[36mRLTrainerComponentParallel.run_single_episode\u001b[39m\u001b[34m(self, i_episode)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single_episode\u001b[39m(\u001b[38;5;28mself\u001b[39m, i_episode):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msetup_single_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_episode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     agent_names = \u001b[38;5;28mself\u001b[39m.env.parallel_agents() \n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m agent_names:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\trainers\\rl_trainer_component.py:308\u001b[39m, in \u001b[36mRLTrainerComponent.setup_single_episode\u001b[39m\u001b[34m(self, i_episode)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28mself\u001b[39m.values[\u001b[33m\"\u001b[39m\u001b[33mepisode_score\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[32m0\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent_in_training \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agents_in_training.values():\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[43magent_in_training\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:608\u001b[39m, in \u001b[36mrequires_input_proccess.<locals>.process_input_if_not_processed_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_input_if_not_processed_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m : Component, *args, **kwargs):\n\u001b[32m    607\u001b[39m     \u001b[38;5;28mself\u001b[39m.proccess_input_if_not_proccesd()\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\trainers\\agent_trainer_component.py:179\u001b[39m, in \u001b[36mAgentTrainer.setup_episode\u001b[39m\u001b[34m(self, env)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28mself\u001b[39m.values[\u001b[33m\"\u001b[39m\u001b[33mepisode_steps\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[32m0\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28mself\u001b[39m.values[\u001b[33m\"\u001b[39m\u001b[33mepisode_score\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset_agent_in_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:608\u001b[39m, in \u001b[36mrequires_input_proccess.<locals>.process_input_if_not_processed_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_input_if_not_processed_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m : Component, *args, **kwargs):\n\u001b[32m    607\u001b[39m     \u001b[38;5;28mself\u001b[39m.proccess_input_if_not_proccesd()\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\agent\\agent_component_with_memory.py:90\u001b[39m, in \u001b[36mAgentSchemaWithStateMemory.reset_agent_in_environment\u001b[39m\u001b[34m(self, initial_state)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;129m@requires_input_proccess\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset_agent_in_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_state : torch.Tensor): \u001b[38;5;66;03m#setup memory shared accross agents\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28mself\u001b[39m.state_memory = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproccess_env_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m.unsqueeze(\u001b[32m0\u001b[39m).expand(\u001b[38;5;28mself\u001b[39m.state_memory_size, *\u001b[38;5;28mself\u001b[39m.state_shape_no_memory).clone()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\fundamentals\\translator\\translator.py:40\u001b[39m, in \u001b[36mTranslatorSequence.translate_state\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     37\u001b[39m current_state = state\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m translator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.translators_sequence:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     current_state = \u001b[43mtranslator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranslate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m current_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ricardo-goncalo-thesis-project\\project\\automl\\fundamentals\\translator\\image_state_translator.py:126\u001b[39m, in \u001b[36mImageReverterToSingleChannel.translate_state\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtranslate_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[32m    124\u001b[39m     first_channel = state[:, :, \u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(first_channel, dtype=torch.float32, device=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ImageReverterToSingleChannel' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "rl_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rl_pipeline.save_configuration(save_exposed_values=True)\n",
    "from automl.basic_components.state_management import save_state\n",
    "\n",
    "\n",
    "save_state(rl_pipeline, save_definition=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATE_NUMBER = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from automl.loggers.result_logger import RESULTS_FILENAME, ResultLogger\n",
    "\n",
    "results_directory = f\"{experiment_path}\\\\RLTrainerComponent\"\n",
    "    \n",
    "results_logger = ResultLogger(input={\n",
    "                                        \"results_filename\" : RESULTS_FILENAME,\n",
    "                                        \"base_directory\" : results_directory,\n",
    "                                        \"artifact_relative_directory\" : '',\n",
    "                                        \"create_new_directory\" : False\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_logger.plot_graph(x_axis='episode', y_axis=[('total_reward', name)], to_show=False)\n",
    "results_logger.plot_confidence_interval(x_axis='episode', y_column='episode_reward',show_std=True, to_show=False, y_values_label=experiment_name, aggregate_number=AGGREGATE_NUMBER)\n",
    "results_logger.plot_linear_regression(x_axis='episode', y_axis='episode_reward', to_show=False, y_label=experiment_name + '_linear')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
