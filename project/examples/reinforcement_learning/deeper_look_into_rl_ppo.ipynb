{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook substitutes some classes in an experience by \"debug\" versions of them, which write to file almost every intermidiate step, as to help detect any incoherence in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_STORE_EXPERIMENTS = \"data\\\\rl_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment_name = \"dqn_sb3_cartpole\"\n",
    "#experiment_name = \"dqn_sb3_cartpole_ppo\"\n",
    "\n",
    "experiment_name = \"ppo_cartpole\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The base Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from automl.base_configurations.environment.cart_pole import dqn_sb3 as base_rl_configuration\n",
    "from automl.base_configurations.environment.cart_pole import ppo_sb3 as base_rl_configuration\n",
    "\n",
    "\n",
    "rl_pipeline_config = base_rl_configuration.config_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Configuration Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_pipeline_input = rl_pipeline_config[\"input\"]\n",
    "\n",
    "rl_trainer_tuple = rl_pipeline_input[\"rl_trainer\"]\n",
    "rl_trainer_input = rl_trainer_tuple[1]\n",
    "\n",
    "agents_input = rl_pipeline_input[\"agents_input\"]\n",
    "\n",
    "policy_tuple = agents_input[\"policy\"]\n",
    "policy_input = policy_tuple[1]\n",
    "\n",
    "agents_trainers_input = rl_trainer_input[\"agents_trainers_input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_tuple = agents_trainers_input[\"learner\"]\n",
    "learner_input = learner_tuple[1]\n",
    "\n",
    "optimizer_tuple = learner_input[\"optimizer\"]\n",
    "optimizer_input = optimizer_tuple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_tuple = agents_trainers_input[\"memory\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = rl_pipeline_config[\"input\"][\"environment\"]\n",
    "environment_input = environment[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change logging system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change the logging so we have full immediate  visibility of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.logger_component import LoggerSchema \n",
    "\n",
    "\n",
    "LoggerSchema.get_schema_parameter_signature(\"write_to_file_when_text_lines_over\").change_default_value(-1)\n",
    "LoggerSchema.get_schema_parameter_signature(\"necessary_logger_level\").change_default_value(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.component_with_results import ResultLogger\n",
    "\n",
    "\n",
    "ResultLogger.get_schema_parameter_signature(\"save_results_on_log\").change_default_value(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes to Help Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.ml.models.torch_model_components import TorchModelComponent\n",
    "from automl.rl.trainers.agent_trainer_ppo import AgentTrainerPPO\n",
    "from automl.rl.trainers.debug.agent_trainer_debug import AgentTrainerDebug\n",
    "from automl.ml.models.torch_model_utils import model_parameter_distance\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# we had our own debug functionality\n",
    "class AgentTrainerPPODebugClass(AgentTrainerDebug, AgentTrainerPPO):\n",
    "\n",
    "    def _proccess_input_internal(self):\n",
    "\n",
    "        super()._proccess_input_internal()\n",
    "    \n",
    "\n",
    "    def optimizeAgent(self):\n",
    "            \n",
    "            super().optimizeAgent()\n",
    "\n",
    "            self.lg.writeLine(\"\\nParam IDs of the model:\", file=\"model_optimization.txt\")\n",
    "            for p in self.model.model.parameters():\n",
    "                self.lg.writeLine(f\"{id(p)} {p.shape}\", file=\"model_optimization.txt\")\n",
    "    \n",
    "            self.lg.writeLine(\"\\nParam IDs being optimized by the actor optimizer:\", file=\"model_optimization.txt\")\n",
    "            for g in self.learner.actor_optimizer.torch_adam_opt.param_groups:\n",
    "                for p in g['params']:\n",
    "                    self.lg.writeLine(f\"optimizer param id: {id(p)}\", file=\"model_optimization.txt\")\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from automl.basic_components.artifact_management import open_or_create_folder\n",
    "from automl.component import requires_input_proccess\n",
    "from automl.loggers.logger_component import ComponentWithLogging\n",
    "from automl.rl.learners.ppo_learner import PPOLearner\n",
    "from automl.rl.learners.debug.learner_debug import LearnerDebug\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PPOLearnerDebugLearnSubstitute(PPOLearner):\n",
    "\n",
    "    def _proccess_input_internal(self):\n",
    "        super()._proccess_input_internal()\n",
    "\n",
    "        self.lg.open_or_create_relative_folder(\"learning\")\n",
    "\n",
    "    @requires_input_proccess\n",
    "    def _learn(self, trajectory, discount_factor):\n",
    "\n",
    "        path_to_write = self.lg.new_relative_path_if_exists(\"computation.txt\", dir=\"learning\")\n",
    "        \n",
    "        self.number_of_times_optimized += 1\n",
    "        \n",
    "        state_batch, action_batch, next_state_batch, reward_batch, done_batch, log_prob_batch = self._interpret_trajectory(trajectory)\n",
    "        \n",
    "        # Compute value estimates\n",
    "        values = self.critic.predict(state_batch).squeeze(-1)\n",
    "        with torch.no_grad():\n",
    "            next_values = self.critic.predict(next_state_batch).squeeze(-1)\n",
    "\n",
    "        # Mask out terminal states (no bootstrapping after done)\n",
    "        next_values = next_values * (1 - done_batch)\n",
    "\n",
    "        self.lg.writeLine(f\"\\nComputed next_values:\", file=path_to_write, use_time_stamp=False)\n",
    "        self.lg.writeLine(f\"next_value = critic(next_state) if not done else 0\\n\", file=path_to_write, use_time_stamp=False)\n",
    "\n",
    "        for i in range(len(next_values)):\n",
    "            # print A = Q - V\n",
    "            self.lg.writeLine(f\"{next_values[i]} = critic({next_state_batch[i]}) if not {done_batch[i]} else 0\", file=path_to_write, use_time_stamp=False)\n",
    "        \n",
    "        # Compute advantages using Generalized Advantage Estimation (GAE)\n",
    "        deltas = reward_batch + discount_factor * next_values - values \n",
    "        advantages = torch.zeros_like(deltas, device=self.device)\n",
    "\n",
    "        self.lg.writeLine(f\"\\nComputing {len(deltas)} deltas:\", file=path_to_write, use_time_stamp=False)\n",
    "        self.lg.writeLine(\"delta = r + discount_factor * next_values - values\\n\", file=path_to_write, use_time_stamp=False)\n",
    "\n",
    "        for i in range(len(advantages)):\n",
    "            # print A = Q - V\n",
    "            self.lg.writeLine(f\"{deltas[i]} = {reward_batch[i]} + {discount_factor} * {next_values[i]} - {values[i]}\", file=path_to_write, use_time_stamp=False)\n",
    "        \n",
    "        self.lg.writeLine(f\"\\nComputing running advantage:\", file=path_to_write, use_time_stamp=False)\n",
    "        self.lg.writeLine(f\"running_advantage[t] = deltas[t] + discount_factor * lamda_gae * running_advantage\", file=path_to_write, use_time_stamp=False)\n",
    "        self.lg.writeLine(f\"advantages[t] = running_advantage\\n\", file=path_to_write, use_time_stamp=False)\n",
    "\n",
    "\n",
    "        # GAE computation in reverse\n",
    "        running_advantage = 0\n",
    "        for t in reversed(range(len(deltas))):\n",
    "            old_running_advantage = running_advantage\n",
    "            old_advantage = advantages[t]\n",
    "\n",
    "            running_advantage = deltas[t] + discount_factor * self.lamda_gae * running_advantage\n",
    "            advantages[t] = running_advantage\n",
    "\n",
    "            self.lg.writeLine(f\"{running_advantage} = {deltas[t]} + {discount_factor} * {self.lamda_gae} * {old_running_advantage}\", file=path_to_write, use_time_stamp=False)\n",
    "            self.lg.writeLine(f\"{old_advantage} substituted by {running_advantage}\", file=path_to_write, use_time_stamp=False)\n",
    "\n",
    "        self.lg.writeLine(f\"\\nComputing new advantages:\", file=path_to_write, use_time_stamp=False)\n",
    "        self.lg.writeLine(f\"advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\\n\", file=path_to_write, use_time_stamp=False)\n",
    "\n",
    "        advantages_mean = advantages.mean()\n",
    "        advantages_std = advantages.std()\n",
    "\n",
    "        old_advantages = advantages.clone()\n",
    "\n",
    "        self.lg.writeLine(f\"Advantages mean: {advantages_mean}\", file=path_to_write, use_time_stamp=False)\n",
    "        self.lg.writeLine(f\"Advantages std: {advantages_mean}\\n\", file=path_to_write, use_time_stamp=False)\n",
    "\n",
    "        advantages = (advantages - advantages_mean) / (advantages_std + 1e-8)\n",
    "\n",
    "        for i in range(len(advantages)):\n",
    "            self.lg.writeLine(f\"{advantages[i]} = ({old_advantages[i]} - {advantages_mean}) / ({advantages_std} + 1e-8)\", file=path_to_write, use_time_stamp=False)\n",
    "\n",
    "        returns = advantages + values.detach()\n",
    "\n",
    "        # Compute new log probabilities from the policy\n",
    "        new_log_probs, entropy = self._evaluate_actions(state_batch, action_batch)\n",
    "\n",
    "        # Compute ratio (pi_theta / pi_theta_old)\n",
    "        ratio = torch.exp(new_log_probs - log_prob_batch)\n",
    "\n",
    "        # This is the true loss\n",
    "        surrogate1 = ratio * advantages\n",
    "        \n",
    "        # This is the clipped loss\n",
    "        surrogate2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "        \n",
    "        # This is the policy loss we want to minimize\n",
    "        policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "        # Compute value loss\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "\n",
    "        # Total loss\n",
    "        loss : torch.Tensor = policy_loss + self.value_loss_coef * value_loss - self.entropy_coef * entropy.mean()\n",
    "\n",
    "        self.actor_optimizer.clear_optimizer_gradients()\n",
    "        self.critic_optimizer.clear_optimizer_gradients()\n",
    "\n",
    "        loss.backward() # we do the optimization here so it goes to both optimizers\n",
    "\n",
    "        self.lg.writeLine(\"\\nDid backward step, noticing the gradients: \", file=path_to_write, use_time_stamp=False)\n",
    "\n",
    "        for name, p in self.model.model.named_parameters():\n",
    "            if p.grad is None:\n",
    "                self.lg.writeLine(f\"Grad {name} is None\", file=path_to_write, use_time_stamp=False)\n",
    "            else:\n",
    "                grad_norm = p.grad.detach().norm().item()\n",
    "                self.lg.writeLine(f\"Grad {name} norm: {grad_norm}\", file=path_to_write, use_time_stamp=False)\n",
    "\n",
    "\n",
    "\n",
    "        self.actor_optimizer.optimize_with_backward_pass_done()\n",
    "        self.critic_optimizer.optimize_with_backward_pass_done()\n",
    "\n",
    "\n",
    "class PPOLearnerDebug(LearnerDebug, PPOLearnerDebugLearnSubstitute):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from automl.ml.memory.torch_memory_component import TorchMemoryComponent\n",
    "from automl.ml.memory.debug.memory_debug import MemoryDebug\n",
    "\n",
    "\n",
    "class TorchMemoryComponentDebug(MemoryDebug, TorchMemoryComponent):\n",
    "\n",
    "\n",
    "    def _proccess_input_internal(self):\n",
    "        super()._proccess_input_internal()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.rl.rl_pipeline import RLTrainerComponent\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class RLTrainerDebug(RLTrainerComponent):\n",
    "\n",
    "    def _proccess_input_internal(self):\n",
    "        super()._proccess_input_internal()\n",
    "\n",
    "        plt.ion()  # turn on interactive mode\n",
    "\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "\n",
    "    def run_episode_step_for_agent_name(self, i_episode, agent_name):\n",
    "\n",
    "        done = super().run_episode_step_for_agent_name( i_episode, agent_name)\n",
    "\n",
    "        self.lg.writeLine(f\"Doing episode step in episode {i_episode} for agent {agent_name} was over: {done}\", file=\"observations.txt\", use_time_stamp=False)\n",
    "                        \n",
    "        return done\n",
    "    \n",
    "    def run_single_episode(self, i_episode):\n",
    "                        \n",
    "        super().run_single_episode(i_episode)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        self.ax.clear()\n",
    "\n",
    "        self.get_results_logger().plot_confidence_interval(x_axis='episode', y_column='episode_reward',show_std=False, to_show=False)\n",
    "        self.get_results_logger().plot_linear_regression(x_axis='episode', y_axis='episode_reward', to_show=False, y_label='linear')\n",
    "\n",
    "        self.ax.set_title(f\"Training progress (update {i_episode})\")\n",
    "        display(self.fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change experiment with Debug variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_trainer_input[\"default_trainer_class\"] = AgentTrainerPPODebugClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_tuple = (PPOLearnerDebug, learner_tuple[1])\n",
    "agents_trainers_input[\"learner\"] = learner_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_tuple = (TorchMemoryComponentDebug, memory_tuple[1])\n",
    "agents_trainers_input[\"memory\"] = memory_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_trainer_tuple = (RLTrainerDebug, rl_trainer_tuple[1])\n",
    "rl_pipeline_input[\"rl_trainer\"] = rl_trainer_tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if LOAD_MODEL:\n",
    "\n",
    "    #base_model_path = 'data\\\\models\\\\sb3_CartPole_dqn\\\\sb3_CartPole_dqn_perturbed_0_10'\n",
    "    #base_model_path = 'data\\\\models\\\\sb3_CartPole_dqn\\\\sb3_CartPole_dqn_perturbed_5_50'\n",
    "    base_model_path = 'data\\\\models\\\\sb3_CartPole_ppo\\\\sb3_CartPole_ppo_gaussian_0_0.8_0.9'\n",
    "\n",
    "    #base_model_path = 'data\\\\models\\\\FC_CartPole_ppo\\\\FC_CartPole_ppo'\n",
    "        \n",
    "    model_name = os.path.basename(base_model_path)\n",
    "    \n",
    "    experiment_name = f\"{experiment_name}\\\\{model_name}\"\n",
    "    \n",
    "    rl_pipeline_input = rl_pipeline_config[\"input\"]\n",
    "    \n",
    "    policy_input[\"model\"] = base_model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner_input[\"critic_model\"] = 'data\\\\models\\\\sb3_CartPole_ppo_critic\\\\sb3_CartPole_ppo_critic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rl_trainer_input[\"limit_total_steps\"] = 1000\n",
    "\n",
    "#rl_trainer_input.pop(\"limit_total_steps\", None)\n",
    "\n",
    "#rl_trainer_input[\"num_episodes\"] = 4000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agents_trainers_input[\"learning_start_step_delay\"] = 5000\n",
    "#agents_trainers_input[\"learning_start_ep_delay\"] = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agents_trainers_input[\"optimization_interval\"] = 2048\n",
    "#agents_trainers_input[\"times_to_learn\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_input[\"clip_grad_norm\"] = 0.1\n",
    "#optimizer_input[\"clip_grad_value\"] = 0.1\n",
    "#optimizer_tuple = learner_input[\"learning_rate\"] = 0.0012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen RL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from automl.rl.rl_pipeline import RLPipelineComponent\n",
    "from automl.utils.json_utils.json_component_utils import gen_component_from\n",
    "\n",
    "rl_pipeline : RLPipelineComponent = gen_component_from(rl_pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_pipeline.pass_input({\"base_directory\" : PATH_TO_STORE_EXPERIMENTS,\n",
    "                        \"artifact_relative_directory\" : experiment_name,\n",
    "                        \"create_new_directory\" : True})\n",
    "\n",
    "experiment_path = rl_pipeline.get_artifact_directory()\n",
    "\n",
    "print(f\"Experiment path: {experiment_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.global_logger import activate_global_logger\n",
    "\n",
    "activate_global_logger(rl_pipeline.get_artifact_directory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_pipeline.proccess_input_if_not_proccesd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rl_pipeline.save_configuration(save_exposed_values=True)\n",
    "from automl.basic_components.state_management import save_state\n",
    "\n",
    "\n",
    "save_state(rl_pipeline, save_definition=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATE_NUMBER = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from automl.loggers.result_logger import RESULTS_FILENAME, ResultLogger\n",
    "\n",
    "results_directory = f\"{experiment_path}\\\\RLTrainerComponent\"\n",
    "    \n",
    "results_logger = ResultLogger(input={\n",
    "                                        \"results_filename\" : RESULTS_FILENAME,\n",
    "                                        \"base_directory\" : results_directory,\n",
    "                                        \"artifact_relative_directory\" : '',\n",
    "                                        \"create_new_directory\" : False\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_logger.plot_graph(x_axis='episode', y_axis=[('total_reward', name)], to_show=False)\n",
    "results_logger.plot_confidence_interval(x_axis='episode', y_column='episode_reward',show_std=True, to_show=False, y_values_label=experiment_name, aggregate_number=AGGREGATE_NUMBER)\n",
    "results_logger.plot_linear_regression(x_axis='episode', y_axis='episode_reward', to_show=False, y_label=experiment_name + '_linear')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
