{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook substitutes some classes in an experience by \"debug\" versions of them, which write to file almost every intermidiate step, as to help detect any incoherence in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_STORE_EXPERIMENTS = \"data\\\\rl_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"dqn_multi_agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation before loading experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change logging system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.logger_component import LoggerSchema \n",
    "\n",
    "LoggerSchema.get_schema_parameter_signature(\"write_to_file_when_text_lines_over\").change_default_value(-1)\n",
    "LoggerSchema.get_schema_parameter_signature(\"necessary_logger_level\").change_default_value(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.component_with_results import ResultLogger\n",
    "\n",
    "\n",
    "ResultLogger.get_schema_parameter_signature(\"save_results_on_log\").change_default_value(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The base Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.rl.whole_configurations import rl_multi_agent_pipeline as base_rl_configuration\n",
    "\n",
    "\n",
    "rl_pipeline_config = base_rl_configuration.config_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Configuration Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_pipeline_input = rl_pipeline_config[\"input\"]\n",
    "\n",
    "rl_trainer_tuple = rl_pipeline_input[\"rl_trainer\"]\n",
    "rl_trainer_input = rl_trainer_tuple[1]\n",
    "\n",
    "agents_input = rl_pipeline_input[\"agents_input\"]\n",
    "\n",
    "policy_tuple = agents_input[\"policy\"]\n",
    "policy_input = policy_tuple[1]\n",
    "\n",
    "agents_trainers_input = rl_trainer_input[\"agents_trainers_input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_translator_tuple = agents_input[\"state_translator\"]\n",
    "state_translator_input = state_translator_tuple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_tuple = agents_trainers_input[\"learner\"]\n",
    "learner_input = learner_tuple[1]\n",
    "\n",
    "optimizer_tuple = learner_input[\"optimizer\"]\n",
    "optimizer_input = optimizer_tuple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_tuple = agents_trainers_input[\"memory\"]\n",
    "memory_input = memory_tuple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = rl_pipeline_config[\"input\"][\"environment\"]\n",
    "environment_input = environment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_strategy_tuple = agents_trainers_input[\"exploration_strategy\"]\n",
    "exploration_strategy_input = exploration_strategy_tuple[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes to the base configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to help alter experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_value_in_dict(dict_with_value : dict, key, new_value):\n",
    "    print(f\"Old value for key '{key}': {dict_with_value.get(key, None)}, new value: {new_value}\")\n",
    "    dict_with_value[key] = new_value\n",
    "\n",
    "def remove_value_in_dict(dict_with_value : dict, key, new_value):\n",
    "    print(f\"Old value for key '{key}': {dict_with_value.get(key, None)}, to be removed...\")\n",
    "    dict_with_value.pop(key, None)\n",
    "\n",
    "\n",
    "\n",
    "def substitute_tuple_value_in_dict(dict_with_tuple : dict, key, tuple_index, new_value):\n",
    "\n",
    "    tuple_value : tuple = dict_with_tuple[key]\n",
    "\n",
    "    print(f\"Old value for tuple pos {tuple_index}: {tuple_value[tuple_index]}, new value: {new_value}\")\n",
    "    new_tuple_value = tuple( new_value if tuple_index == i else tuple_value[i] for i in range(len(tuple_value)) )\n",
    "\n",
    "    dict_with_tuple[key] = new_tuple_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes to debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes to Help Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.rl.trainers.debug.agent_trainer_debug import AgentTrainerDebug\n",
    "from automl.rl.trainers.agent_trainer_component_dqn import AgentTrainerDQN\n",
    "\n",
    "# we had our own debug functionality\n",
    "class AgentTrainerDQNDebugClass(AgentTrainerDebug, AgentTrainerDQN):\n",
    "    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.component import requires_input_proccess\n",
    "from automl.rl.learners.debug.learner_debug import DQNLearnerDebug\n",
    "from automl.rl.learners.q_learner import DeepQLearnerSchema, DoubleDeepQLearnerSchema\n",
    "import torch\n",
    "\n",
    "QLearnerSchema = DoubleDeepQLearnerSchema\n",
    "\n",
    "class CustomDQNLearnerDebugStrategy(QLearnerSchema):\n",
    "\n",
    "    def _proccess_input_internal(self):\n",
    "        super()._proccess_input_internal()\n",
    "\n",
    "        self.lg.open_or_create_relative_folder(\"learning\")\n",
    "\n",
    "\n",
    "    def _apply_model_prediction_given_state_action_pairs(self, state_batch, action_batch):\n",
    "\n",
    "        '''Returns the values predicted by the current model and the values for the specific actions that were passed''' \n",
    "\n",
    "        predicted_actions_values, predicted_values_for_actions = super()._apply_model_prediction_given_state_action_pairs(state_batch, action_batch)\n",
    "\n",
    "        self.lg.writeLine(f\"\\nComputed predicted_actions_values and value for action chosen:\\n\", file=self.__path_to_write, use_time_stamp=False)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            self.lg.writeLine(f\"{i}: {predicted_actions_values[i]} [ {action_batch[i]} ] -> {predicted_values_for_actions[i]}\", file=self.__path_to_write, use_time_stamp=False)\n",
    "    \n",
    "        return predicted_actions_values, predicted_values_for_actions\n",
    "\n",
    "\n",
    "    def _apply_value_prediction_to_next_state(self, next_state_batch, done_batch, reward_batch, discount_factor):\n",
    "\n",
    "        '''\n",
    "        Returns the predicted values for the next state\n",
    "        \n",
    "        They are given by appying the Q function to them and then chosing the next \n",
    "\n",
    "        '''\n",
    "\n",
    "        next_state_q_values, next_state_v_values = super()._apply_value_prediction_to_next_state(next_state_batch, done_batch, reward_batch, discount_factor)\n",
    "\n",
    "        self.lg.writeLine(f\"\\nComputed done, next_state_values computed by target and q value of action chosen:\\n\", file=self.__path_to_write, use_time_stamp=False)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            self.lg.writeLine(f\"{i}: {done_batch[i]}, {next_state_q_values[i]} -> {next_state_v_values[i]}\", file=self.__path_to_write, use_time_stamp=False)\n",
    "\n",
    "\n",
    "        return next_state_q_values, next_state_v_values\n",
    "    \n",
    "\n",
    "    def _calculate_chosen_actions_correct_q_values(self, next_state_v_values, discount_factor, reward_batch):\n",
    "\n",
    "        old_action_values = next_state_v_values.clone()\n",
    "\n",
    "        correct_q_values_for_chosen_action = super()._calculate_chosen_actions_correct_q_values(next_state_v_values, discount_factor, reward_batch)\n",
    "\n",
    "        self.lg.writeLine(f\"\\nNext action values after multiplying by discount factor {discount_factor} and adding reward:\\n\", file=self.__path_to_write, use_time_stamp=False)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            self.lg.writeLine(f\"{i}: {correct_q_values_for_chosen_action[i]} = {old_action_values[i]} * {discount_factor} + {reward_batch[i]}\", file=self.__path_to_write, use_time_stamp=False)\n",
    "\n",
    "        return correct_q_values_for_chosen_action\n",
    "    \n",
    "    def _optimize_with_predicted_model_values_and_correct_values(self, predicted_values, correct_values):\n",
    "\n",
    "        self.lg.writeLine(f\"\\nOptimizing using error of original predicted action values and target done on future state:\\n\", file=self.__path_to_write, use_time_stamp=False)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            self.lg.writeLine(f\"{i}: {predicted_values[i]} vs {correct_values[i]}\", file=self.__path_to_write, use_time_stamp=False)\n",
    "\n",
    "        super()._optimize_with_predicted_model_values_and_correct_values(predicted_values, correct_values)\n",
    "\n",
    "\n",
    "    def learn(self, trajectory, discount_factor) -> None:\n",
    "\n",
    "        self.__path_to_write = self.lg.new_relative_path_if_exists(\"computation.txt\", dir=\"learning\")\n",
    "        \n",
    "        super().learn(trajectory, discount_factor)\n",
    "\n",
    "\n",
    "\n",
    "class CustomDQNLearnerDebug(DQNLearnerDebug, CustomDQNLearnerDebugStrategy):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from automl.ml.memory.torch_memory_component import TorchMemoryComponent\n",
    "from automl.ml.memory.debug.memory_debug import MemoryDebug\n",
    "\n",
    "\n",
    "class TorchMemoryComponentDebug(MemoryDebug, TorchMemoryComponent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from automl.rl.trainers.debug.rl_trainer_debug import RLTrainerDebug\n",
    "from automl.rl.trainers.rl_trainer.parallel_rl_trainer import RLTrainerComponentParallel\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "class RLTrainerDebug(RLTrainerComponentParallel):\n",
    "\n",
    "    is_debug_schema = True\n",
    "\n",
    "    def _proccess_input_internal(self):\n",
    "        super()._proccess_input_internal()\n",
    "\n",
    "        plt.ion()  # turn on interactive mode\n",
    "\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "\n",
    "    def run_episode_step_for_agent_name(self, i_episode, agent_name):\n",
    "\n",
    "        done = super().run_episode_step_for_agent_name( i_episode, agent_name)\n",
    "\n",
    "        self.lg.writeLine(f\"Doing episode step in episode {i_episode} for agent {agent_name} was over: {done}\", file=\"observations.txt\", use_time_stamp=False)\n",
    "                        \n",
    "        return done\n",
    "    \n",
    "    def run_single_episode(self, i_episode):\n",
    "                        \n",
    "        super().run_single_episode(i_episode)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        self.ax.clear()\n",
    "\n",
    "        self.get_results_logger().plot_confidence_interval(x_axis='episode', y_column='episode_reward',show_std=False, to_show=False, ax=self.ax)\n",
    "        self.get_results_logger().plot_linear_regression(x_axis='episode', y_axis='episode_reward', to_show=False, y_label='linear', ax=self.ax)\n",
    "\n",
    "        self.ax.set_title(f\"Training progress (update {i_episode})\")\n",
    "        display(self.fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.ml.optimizers.debug.debug_optimizers import AdamOptimizerDebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from automl.rl.trainers.debug.rl_trainer_debug import RLTrainerDebug\n",
    "from automl.fundamentals.translator.translator import Translator\n",
    "from automl.rl.policy.policy import ComponentWithLogging\n",
    "\n",
    "import torch\n",
    "torch.set_printoptions(threshold=float('inf'), linewidth=30000)\n",
    "\n",
    "\n",
    "translator_type : type[Translator] = state_translator_tuple[0]\n",
    "\n",
    "\n",
    "class TranslatorDebug(translator_type, ComponentWithLogging):\n",
    "\n",
    "    is_debug_schema = True\n",
    "\n",
    "\n",
    "    def translate_state(self, state):\n",
    "        \n",
    "        self.lg.writeLine(f\"-----------------------------------------------\\nTranslating:\\n\\n{state}\\ninto\", use_time_stamp=False, file=\"translations.txt\")\n",
    "        to_return = super().translate_state(state)\n",
    "        self.lg.writeLine(f\"\\n{to_return}\\n-----------------------------------------------\", use_time_stamp=False, file=\"translations.txt\")\n",
    "        return to_return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.ml.models.neural_model import FullyConnectedModelSchema\n",
    "\n",
    "\n",
    "class FullyConnectedModelDebug(FullyConnectedModelSchema, ComponentWithLogging):\n",
    "\n",
    "    @requires_input_proccess\n",
    "    def predict(self, state):\n",
    "        self.lg.writeLine(f\"Predicting for input with state {state.shape}, while input size is {self.input_size}\",  use_time_stamp=False, file=\"predictions.txt\")\n",
    "        to_return = self.model(state)\n",
    "        self.lg.writeLine(f\"To return shape: {to_return.shape}\\n\",  use_time_stamp=False, file=\"predictions.txt\")\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change experiment with Debug variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_value_in_dict(rl_trainer_input, \"default_trainer_class\", AgentTrainerDQNDebugClass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_tuple_value_in_dict(agents_trainers_input, \"learner\", 0, CustomDQNLearnerDebug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_tuple_value_in_dict(agents_input, \"state_translator\", 0, TranslatorDebug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_tuple_value_in_dict(agents_trainers_input, \"memory\", 0, TorchMemoryComponentDebug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_tuple_value_in_dict(rl_pipeline_input, \"rl_trainer\", 0, RLTrainerDebug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_tuple_value_in_dict(learner_input, \"optimizer\", 0, AdamOptimizerDebug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_tuple_value_in_dict(policy_input, \"model\", 0, FullyConnectedModelDebug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other value changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rl_trainer_input[\"limit_total_steps\"] = 1000\n",
    "\n",
    "#rl_trainer_input.pop(\"limit_total_steps\", None)\n",
    "\n",
    "#rl_trainer_input[\"num_episodes\"] = 4000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agents_trainers_input[\"learning_start_step_delay\"] = 5000\n",
    "#agents_trainers_input[\"learning_start_ep_delay\"] = 150\n",
    "\n",
    "#substitute_value_in_dict(agents_trainers_input, \"learning_start_ep_delay\", 2897)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_value_in_dict(agents_trainers_input, \"optimization_interval\", 10)\n",
    "substitute_value_in_dict(agents_trainers_input, \"times_to_learn\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_value_in_dict(memory_input, \"capacity\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_input[\"clip_grad_norm\"] = 0.1\n",
    "\n",
    "#substitute_value_in_dict(optimizer_input, \"clip_grad_value\", 0.2956984463839789)\n",
    "\n",
    "#substitute_value_in_dict(optimizer_input, \"learning_rate\", 0.006807860813523758)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_value_in_dict(agents_trainers_input, \"discount_factor\", 0.8790365307757482)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_value_in_dict(learner_input, \"target_update_rate\", 0.5511208693081078)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_value_in_dict(exploration_strategy_input, \"epsilon_end\", 0.009535369612528788)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen RL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from automl.rl.rl_pipeline import RLPipelineComponent\n",
    "from automl.utils.json_utils.json_component_utils import gen_component_from\n",
    "\n",
    "rl_pipeline : RLPipelineComponent = gen_component_from(rl_pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_pipeline.pass_input({\n",
    "    \"base_directory\" : PATH_TO_STORE_EXPERIMENTS,\n",
    "                        \"artifact_relative_directory\" : experiment_name,\n",
    "                        \"create_new_directory\" : True,\n",
    "                        \"do_full_setup_of_seed\" : True}\n",
    "                        )\n",
    "\n",
    "experiment_path = rl_pipeline.get_artifact_directory()\n",
    "\n",
    "print(f\"Experiment path: {experiment_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.global_logger import activate_global_logger\n",
    "\n",
    "activate_global_logger(rl_pipeline.get_artifact_directory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rl_pipeline.save_configuration(save_exposed_values=True)\n",
    "from automl.basic_components.state_management import save_state\n",
    "\n",
    "\n",
    "save_state(rl_pipeline, save_definition=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATE_NUMBER = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from automl.loggers.result_logger import RESULTS_FILENAME, ResultLogger\n",
    "\n",
    "results_directory = f\"{experiment_path}\\\\RLTrainerComponent\"\n",
    "    \n",
    "results_logger = ResultLogger(input={\n",
    "                                        \"results_filename\" : RESULTS_FILENAME,\n",
    "                                        \"base_directory\" : results_directory,\n",
    "                                        \"artifact_relative_directory\" : '',\n",
    "                                        \"create_new_directory\" : False\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_logger.plot_graph(x_axis='episode', y_axis=[('total_reward', name)], to_show=False)\n",
    "results_logger.plot_confidence_interval(x_axis='episode', y_column='episode_reward',show_std=True, to_show=False, y_values_label=experiment_name, aggregate_number=AGGREGATE_NUMBER)\n",
    "results_logger.plot_linear_regression(x_axis='episode', y_axis='episode_reward', to_show=False, y_label=experiment_name + '_linear')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
