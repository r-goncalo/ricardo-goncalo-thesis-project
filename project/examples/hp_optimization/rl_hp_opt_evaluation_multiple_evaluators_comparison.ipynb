{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\")) #make the folder \"automl\" part of this\n",
    "\n",
    "RESULTS_PATH = 'results.csv'\n",
    "OPTUNA_DATABASE = 'study_results.db'\n",
    "BASE_CONFIGURATION_NAME = 'configuration'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.result_logger import ResultLogger\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "from automl.utils.optuna_utils import load_study_from_database\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_experiment_path = \"C:\\\\Experiments\\\\rl-zoo-CartPole-dqn-2\\\\HPOptimizationExperiments\\\\3\\\\experiments\"\n",
    "#experiment_relative_path = 'original'\n",
    "\n",
    "base_experiment_path = \"C:\\\\ricardo-goncalo-thesis-project\\\\project\\\\examples\\\\hp_optimization\\\\data\\\\hp_lodaded_exps\"\n",
    "experiment_relative_path = \"exp_115\"\n",
    "\n",
    "#base_experiment_path = \"C:\\\\Experiments\\\\rl-zoo-CartPole-ppo-multi_thread\\\\HpExperiments\\\\1\"\n",
    "#experiment_relative_path = \"experiment\"\n",
    "\n",
    "experiment_path = f'{base_experiment_path}\\\\{experiment_relative_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(experiment_path):\n",
    "    raise Exception(\"DOES NOT EXIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.core.advanced_input_management import gen_component_from\n",
    "from automl.hp_opt.hp_optimization_pipeline import HyperparameterOptimizationPipeline\n",
    "\n",
    "\n",
    "hyperparameter_optimization_pipeline : HyperparameterOptimizationPipeline = gen_component_from(experiment_path)\n",
    "hyperparameter_optimization_pipeline.change_logger_level(\"NONE\") # we don't want any logging done while we're looking into it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of HyperparameterOptimizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import get_hp_opt_results_logger\n",
    "\n",
    "hyperparameter_optimization_results = hyperparameter_optimization_pipeline.get_decoupled_results_logger()\n",
    "\n",
    "print(f\"Hyperparameter_optimization_results in path: {hyperparameter_optimization_results.get_artifact_directory()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_in_results = set(hyperparameter_optimization_results.get_dataframe()[\"experiment\"].values)\n",
    "component_indexes_in_results = set(hyperparameter_optimization_results.get_dataframe()[\"component_index\"].values)\n",
    "\n",
    "print(f\"Experiments in results:\\n{experiments_in_results}\")\n",
    "print(f\"Component indexes per experiment:\\n{component_indexes_in_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_available_for_component_indexes = [\"red\", \"blue\", \"green\"]\n",
    "colors_for_component_indexes = {}\n",
    "\n",
    "color_iter = iter(colors_available_for_component_indexes)\n",
    "for component_index in component_indexes_in_results:\n",
    "    colors_for_component_indexes[component_index] = next(color_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import get_hp_opt_optuna_study\n",
    "\n",
    "\n",
    "optuna_study = hyperparameter_optimization_pipeline.get_study()\n",
    "trials_in_optuna = optuna_study.get_trials()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"optuna_study done with with best value {optuna_study.best_value} in trial {optuna_study.best_trial.number} with best parameters:\\n{optuna_study.best_params}\")\n",
    "\n",
    "except:\n",
    "    print(\"No best trial yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_plot = []\n",
    "# parameters_to_plot = [\"hidden_size\", \"hidden_layers\"]\n",
    "\n",
    "if len(parameters_to_plot) > 1:\n",
    "\n",
    "    fig = vis.plot_contour(optuna_study, params=parameters_to_plot)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global evaluation of configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATE_NUMBER = 10 #the number of neighbor points to sum to plot the needed graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import study_of_configuration      \n",
    "from automl.hp_opt.hp_eval_results.hp_eval_results import study_of_components_for_configuration      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import get_results_of_configurations_components                          \n",
    "\n",
    "results_of_configurations : dict[str, ResultLogger] = get_results_of_configurations_components(experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Configurations:  {results_of_configurations.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global view of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_optimization_pipeline.proccess_input_if_not_proccesd()\n",
    "config_dict_to_optimize = hyperparameter_optimization_pipeline.config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps_to_do = config_dict_to_optimize[\"input\"][\"rl_trainer\"][1][\"limit_total_steps\"]\n",
    "steps_done_in_step = max_steps_to_do / hyperparameter_optimization_pipeline.n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.rl.evaluators.rl_learning_evaluator import RLLearningEvaluatorSlope\n",
    "\n",
    "\n",
    "evaluator_to_compare_to = RLLearningEvaluatorSlope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import get_results_of_configuration_in_path\n",
    "from automl.hp_opt.hp_opt_strategies.hp_optimization_loader_detached import COMPONENT_INDEX_TO_USE_OPTUNA_KEY, CONFIGURATION_PATH_OPTUNA_KEY\n",
    "\n",
    "\n",
    "for trial in trials_in_optuna:\n",
    "    \n",
    "    trial : optuna.trial.FrozenTrial = trial\n",
    "    best_index = int(trial.user_attrs.get(COMPONENT_INDEX_TO_USE_OPTUNA_KEY))\n",
    "    \n",
    "    if best_index is not None:\n",
    "        results_logger_of_trial = get_results_of_configuration_in_path(\n",
    "            os.path.join(trial.user_attrs.get(CONFIGURATION_PATH_OPTUNA_KEY), str(best_index)),\n",
    "            \"RLTrainerComponent\"\n",
    "        )\n",
    "\n",
    "        print(\"-\" * 10 + f\"Trial: {trial.number}\" + \"-\" * 10)\n",
    "        \n",
    "        for step, value in trial.intermediate_values.items():\n",
    "            evaluator_to_compare_to.pass_input({\n",
    "                \"init\" : (step * steps_done_in_step, \"step\"),\n",
    "                \"final\" : (steps_done_in_step + step * steps_done_in_step, \"step\")\n",
    "            })\n",
    "\n",
    "            new_value = evaluator_to_compare_to.evaluate(results_logger_of_trial)\n",
    "\n",
    "            results.append((trial.number, step, value, new_value))\n",
    "\n",
    "            print(f\"    Step: {step} had original value {value}, {new_value} with new evaluator\")\n",
    "\n",
    "        print()        \n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trials, step, value, new_value, ordered by value\")\n",
    "\n",
    "results_sorted = sorted(results, key=lambda x: x[2])\n",
    "\n",
    "for trial_number, step, value, new_value in results_sorted:\n",
    "    print(\n",
    "        f\"Trial {trial_number}, Step {step}, \"\n",
    "        f\"value={value}, new_value={new_value}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trials, step, value, new_value, ordered by new_value\")\n",
    "\n",
    "results_sorted = sorted(results, key=lambda x: x[3])\n",
    "\n",
    "for trial_number, step, value, new_value in results_sorted:\n",
    "    print(\n",
    "        f\"Trial {trial_number}, Step {step}, \"\n",
    "        f\"value={value}, new_value={new_value}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract values\n",
    "values = np.array([r[2] for r in results], dtype=float)\n",
    "new_values = np.array([r[3] for r in results], dtype=float)\n",
    "\n",
    "def min_max_normalize(x):\n",
    "    min_x = np.min(x)\n",
    "    max_x = np.max(x)\n",
    "    if max_x - min_x == 0:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - min_x) / (max_x - min_x)\n",
    "\n",
    "# Normalize both\n",
    "normalized_values = min_max_normalize(values)\n",
    "normalized_new_values = min_max_normalize(new_values)\n",
    "\n",
    "normalized_results = list(\n",
    "    zip(\n",
    "        [r[0] for r in results],  # trial number\n",
    "        [r[1] for r in results],  # step\n",
    "        normalized_values,\n",
    "        normalized_new_values,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot\n",
    "x_axis = np.arange(len(results))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x_axis, normalized_values)\n",
    "plt.scatter(x_axis, normalized_new_values)\n",
    "\n",
    "plt.xlabel(\"Result Index\")\n",
    "plt.ylabel(\"Normalized Value\")\n",
    "plt.title(\"Normalized Original vs New Evaluator Values\")\n",
    "plt.legend([\"Original (normalized)\", \"New (normalized)\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
