{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\")) #make the folder \"automl\" part of this\n",
    "\n",
    "RESULTS_PATH = 'results.csv'\n",
    "OPTUNA_DATABASE = 'study_results.db'\n",
    "BASE_CONFIGURATION_NAME = 'configuration'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.result_logger import ResultLogger\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "from automl.utils.optuna_utils import load_study_from_database\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_experiment_path = \"C:\\\\Experiments\\\\rl-zoo-CartPole-dqn-2\\\\HPOptimizationExperiments\\\\2\\\\experiments\"\n",
    "experiment_relative_path = 'original'\n",
    "experiment_path = f'{base_experiment_path}\\\\{experiment_relative_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(experiment_path):\n",
    "    raise Exception(\"DOES NOT EXIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of HyperparameterOptimizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import get_hp_opt_results_logger\n",
    "\n",
    "hyperparameter_optimization_results = get_hp_opt_results_logger(experiment_path)\n",
    "\n",
    "print(f\"Hyperparameter_optimization_results in path: {hyperparameter_optimization_results.get_artifact_directory()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import get_hp_opt_optuna_study\n",
    "\n",
    "\n",
    "optuna_study = get_hp_opt_optuna_study(hyperparameter_optimization_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"optuna_study done with with best value {optuna_study.best_value} in trial {optuna_study.best_trial.number} with best parameters:\\n{optuna_study.best_params}\")\n",
    "\n",
    "except:\n",
    "    print(\"No best trial yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import print_optuna_trials_info\n",
    "\n",
    "print_optuna_trials_info(optuna_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_slice\n",
    "\n",
    "fig = plot_slice(optuna_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import print_optuna_param_importances\n",
    "\n",
    "print_optuna_param_importances(optuna_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vis.plot_param_importances(optuna_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = vis.plot_parallel_coordinate(optuna_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vis.plot_intermediate_values(optuna_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vis.plot_optimization_history(optuna_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import plot_scattered_values_for_param\n",
    "\n",
    "if False: # this is not really necessary here, it is best to use it studying single configurations\n",
    "\n",
    "    try:\n",
    "        plot_scattered_values_for_param(optuna_study)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot scattered values because of error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_plot = []\n",
    "# parameters_to_plot = [\"hidden_size\", \"hidden_layers\"]\n",
    "\n",
    "if len(parameters_to_plot) > 1:\n",
    "\n",
    "    fig = vis.plot_contour(optuna_study, params=parameters_to_plot)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global evaluation of configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATE_NUMBER = 10 #the number of neighbor points to sum to plot the needed graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import study_of_configuration                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import get_results_of_configurations                          \n",
    "\n",
    "\n",
    "\n",
    "results_of_configurations : dict[str, ResultLogger] = get_results_of_configurations(experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Configurations:  {results_of_configurations.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global view of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = hyperparameter_optimization_results.list_of_unique_values(\"step\")\n",
    "print(steps_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in steps_done:\n",
    "\n",
    "    hyperparameter_optimization_results.plot_bar_graph(x_axis='experiment', y_axis='result', to_show=False, fixed_value_tuple=(\"step\", step))\n",
    "    hyperparameter_optimization_results.plot_linear_regression(x_axis='experiment', y_axis='result', to_show=False, fixed_value_tuple=(\"step\", step))\n",
    "    hyperparameter_optimization_results.plot_current_graph(title=f'experiments_at_step_{step}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruned Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import get_pruned_trials                          \n",
    "\n",
    "\n",
    "pruned_optuna_trials, pruned_optuna_trials_per_steps, pruned_trials = get_pruned_trials(optuna_study)\n",
    "\n",
    "print(f\"Pruned trials: {pruned_trials}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst Pruned Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORST_PRUNED = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_completed_steps in pruned_optuna_trials_per_steps.keys():\n",
    "    \n",
    "    pruned_optuna_trials : list[optuna.Trial] = pruned_optuna_trials_per_steps[n_completed_steps][0:WORST_PRUNED] \n",
    "    \n",
    "    pruned_trials = [f'configuration_{trial.number}' for trial in pruned_optuna_trials ]\n",
    "\n",
    "    for configuration_name in pruned_trials:\n",
    "        \n",
    "        results_logger = results_of_configurations[configuration_name]\n",
    "    \n",
    "        study_of_configuration(configuration_name, results_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Pruned Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PRUNED = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_completed_steps in pruned_optuna_trials_per_steps.keys():\n",
    "    \n",
    "    #correct this\n",
    "    pruned_optuna_trials : list[optuna.Trial] = pruned_optuna_trials_per_steps[n_completed_steps][-BEST_PRUNED:] \n",
    "    \n",
    "    pruned_trials = [f'configuration_{trial.number}' for trial in pruned_optuna_trials ]\n",
    "\n",
    "    for configuration_name in pruned_trials:\n",
    "        \n",
    "        results_logger = results_of_configurations[configuration_name]\n",
    "    \n",
    "        study_of_configuration(configuration_name, results_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed Trials Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_optuna_trials = [trial for trial in optuna_study.trials if trial.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "completed_optuna_trials.sort(key=lambda trial: trial.value) # sort given the trial value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst Completed Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORST = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "worst_optuna_trials = completed_optuna_trials[0:N_WORST]\n",
    "\n",
    "worst_configurations_to_study = [f\"configuration_{trial.number}\" for trial in worst_optuna_trials]\n",
    "\n",
    "print(f\"Worst configurations to study: {worst_configurations_to_study}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for configuration_name in worst_configurations_to_study:\n",
    "    \n",
    "    results_logger = results_of_configurations[configuration_name]\n",
    "\n",
    "    study_of_configuration(configuration_name, results_logger)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Completed Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_completed_optuna_trials = [trial for trial in optuna_study.trials if trial.state == optuna.trial.TrialState.RUNNING or trial.state == optuna.trial.TrialState.WAITING]\n",
    "\n",
    "non_completed_optuna_trials.sort(key=lambda trial: trial.value) # sort given the trial value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All non completed trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_completed_trials = [f\"configuration_{trial.number}\" for trial in non_completed_optuna_trials]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for configuration_name in non_completed_trials:\n",
    "\n",
    "    try:\n",
    "    \n",
    "        results_logger = results_of_configurations[configuration_name]\n",
    "\n",
    "        study_of_configuration(configuration_name, results_logger)\n",
    "\n",
    "    except:\n",
    "        print(f\"Could not look into non completed configuration: {configuration_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BEST = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optuna_trials = completed_optuna_trials[(len(completed_optuna_trials) - N_BEST):]\n",
    "\n",
    "best_configurations_to_study = [f\"configuration_{trial.number}\" for trial in best_optuna_trials]\n",
    "\n",
    "print(f\"Best configurations to study: {best_configurations_to_study}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for configuration_name in best_configurations_to_study:\n",
    "    \n",
    "    results_logger = results_of_configurations[configuration_name]\n",
    "\n",
    "    study_of_configuration(configuration_name, results_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smaller study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration study in optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trials_to_study = [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import get_trials_with_decreasing_intermediates, print_intermidiate_values\n",
    "trials_to_study = get_trials_with_decreasing_intermediates(optuna_study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_intermidiate_values(trials_to_study, optuna_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.hp_opt.hp_eval_results.hp_eval_results import plot_scattered_values_for_param\n",
    "\n",
    "try:\n",
    "    plot_scattered_values_for_param(optuna_study, trials_to_study)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot scattered values because of error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations_to_study = [f'configuration_{trial_n}' for trial_n in trials_to_study]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "\n",
    "    for configuration_name in configurations_to_study:\n",
    "    \n",
    "        results_logger = results_of_configurations[configuration_name]\n",
    "\n",
    "        study_of_configuration(configuration_name, results_logger)\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\\nAvailable keys are {results_of_configurations.keys()}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study for each agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_in_study = []\n",
    "# agents_in_study = [\"agent_1\", \"agent_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_to_study : dict[str, ResultLogger]= {}\n",
    "\n",
    "for configuration_name in configurations_to_study:\n",
    "    \n",
    "    results_logger = results_of_configurations[configuration_name]\n",
    "    \n",
    "    for agent_name in agents_in_study:\n",
    "      \n",
    "        agent_results_logger = ResultLogger(input={\n",
    "                                            \"logger_directory\" : f\"{results_logger.lg.logDir}\\\\{agent_name}\",\n",
    "                                            \"filename\" : RESULTS_PATH,\n",
    "                                            \"create_new_directory\" : False\n",
    "                                          })\n",
    "\n",
    "        agents_to_study[f\"{configuration_name}_{agent_name}\"] = agent_results_logger\n",
    "        \n",
    "        agent_results_logger.proccess_input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent_name, agent_results_logger in agents_to_study.items():\n",
    "    \n",
    "    study_of_configuration(agent_name, agent_results_logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
