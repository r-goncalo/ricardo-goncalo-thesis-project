{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook substitutes some classes in an experience by \"debug\" versions of them, which write to file almost every intermidiate step, as to help detect any incoherence in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_STORE_EXPERIMENTS = \"data\\\\rl_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"dqn_multi_agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation before loading experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change logging system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.logger_component import LoggerSchema \n",
    "\n",
    "LoggerSchema.get_schema_parameter_signature(\"write_to_file_when_text_lines_over\").change_default_value(-1)\n",
    "LoggerSchema.get_schema_parameter_signature(\"necessary_logger_level\").change_default_value(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.component_with_results import ResultLogger\n",
    "\n",
    "ResultLogger.get_schema_parameter_signature(\"save_results_on_log\").change_default_value(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The base Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.rl.whole_configurations import rl_multi_agent_pipeline as base_rl_configuration\n",
    "rl_pipeline_config = base_rl_configuration.config_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from automl.rl.trainers.debug.rl_trainer_debug import RLTrainerDebug\n",
    "from automl.rl.trainers.rl_trainer.parallel_rl_trainer import RLTrainerComponentParallel\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "class RLTrainerDebug(RLTrainerComponentParallel):\n",
    "\n",
    "    is_debug_schema = True\n",
    "\n",
    "    def _proccess_input_internal(self):\n",
    "        super()._proccess_input_internal()\n",
    "\n",
    "        plt.ion()  # turn on interactive mode\n",
    "\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "\n",
    "    def run_episode_step_for_agent_name(self, i_episode, agent_name):\n",
    "\n",
    "        done = super().run_episode_step_for_agent_name( i_episode, agent_name)\n",
    "\n",
    "        self.lg.writeLine(f\"Doing episode step in episode {i_episode} for agent {agent_name} was over: {done}\", file=\"observations.txt\", use_time_stamp=False)\n",
    "                        \n",
    "        return done\n",
    "    \n",
    "    def run_single_episode(self, i_episode):\n",
    "                        \n",
    "        super().run_single_episode(i_episode)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        self.ax.clear()\n",
    "\n",
    "        self.get_results_logger().plot_confidence_interval(x_axis='episode', y_column='episode_reward',show_std=False, to_show=False, ax=self.ax)\n",
    "        self.get_results_logger().plot_linear_regression(x_axis='episode', y_axis='episode_reward', to_show=False, y_label='linear', ax=self.ax)\n",
    "\n",
    "        self.ax.set_title(f\"Training progress (update {i_episode})\")\n",
    "        display(self.fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.core.debug.debug_utils import substitute_classes_by_debug_classes\n",
    "from automl.ml.models.debug.torch_model_debug import TorchModelComponentDebug\n",
    "from automl.rl.learners.debug.learner_debug import DQNLearnerDebug\n",
    "from automl.rl.trainers.debug.agent_trainer_debug import AgentTrainerDebug\n",
    "\n",
    "rl_pipeline_config = substitute_classes_by_debug_classes(rl_pipeline_config, [\n",
    "    AgentTrainerDebug, \n",
    "    DQNLearnerDebug, \n",
    "    TorchModelComponentDebug,\n",
    "    #TranslatorDebug,\n",
    "    #MemoryDebug,\n",
    "    #RLTrainerDebug,\n",
    "    #AdamOptimizerDebug,\n",
    "\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Configuration Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.rl.policy.policy import ModelComponent\n",
    "\n",
    "\n",
    "rl_pipeline_input : dict[str, any] = rl_pipeline_config[\"input\"]\n",
    "rl_pipeline_children : list = rl_pipeline_config.get(\"child_components\", None)\n",
    "\n",
    "rl_trainer_tuple = rl_pipeline_input[\"rl_trainer\"]\n",
    "rl_trainer_input : dict[str, any] = rl_trainer_tuple[1]\n",
    "\n",
    "agents_input : dict[str, any] = rl_pipeline_input[\"agents_input\"]\n",
    "\n",
    "policy_tuple = agents_input[\"policy\"]\n",
    "policy_input : dict[str, any] = policy_tuple[1]\n",
    "\n",
    "model_tuple = policy_input[\"model\"]\n",
    "model_class : type[ModelComponent] = model_tuple[0] \n",
    "model_input : dict[str, any] = model_tuple[1]\n",
    "\n",
    "agents_trainers_input : dict[str, any] = rl_trainer_input[\"agents_trainers_input\"]\n",
    "\n",
    "state_translator_tuple = agents_input[\"state_translator\"]\n",
    "state_translator_input : dict[str, any] = state_translator_tuple[1]\n",
    "\n",
    "learner_tuple = agents_trainers_input[\"learner\"]\n",
    "learner_input : dict[str, any] = learner_tuple[1]\n",
    "\n",
    "optimizer_tuple = learner_input[\"optimizer\"]\n",
    "optimizer_input : dict[str, any] = optimizer_tuple[1]\n",
    "\n",
    "memory_tuple = agents_trainers_input[\"memory\"]\n",
    "memory_input : dict[str, any] = memory_tuple[1]\n",
    "\n",
    "environment = rl_pipeline_config[\"input\"][\"environment\"]\n",
    "environment_input : dict[str, any] = environment[1]\n",
    "\n",
    "exploration_strategy_tuple = agents_trainers_input[\"exploration_strategy\"]\n",
    "exploration_strategy_input : dict[str, any] = exploration_strategy_tuple[1]\n",
    "\n",
    "agent_models_in_sequence = model_input[\"models\"]\n",
    "\n",
    "agent_fcn_model_tuple = agent_models_in_sequence[1]\n",
    "\n",
    "agent_fct_model_class : type[ModelComponent] = agent_fcn_model_tuple[0]\n",
    "agent_fct_model_input : dict[str, any] = agent_fcn_model_tuple[1]\n",
    "\n",
    "shared_model_definition = rl_pipeline_children[0]\n",
    "\n",
    "shared_model_type : type[ModelComponent] = shared_model_definition[\"__type__\"]\n",
    "shared_model_input : dict[str, any] = shared_model_definition[\"input\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change experiment with Debug variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.utils.collection_utils import substitute_value_in_dict\n",
    "\n",
    "\n",
    "AgentTrainerDebug.get_schema_parameter_signature(\"verify_model_difference_after_optimize\").change_default_value(False)\n",
    "\n",
    "substitute_value_in_dict(agents_trainers_input, \"verify_model_difference_after_optimize\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQNLearnerDebug.get_schema_parameter_signature(\"compare_old_and_new_target_predictions\").change_default_value(False)\n",
    "DQNLearnerDebug.get_schema_parameter_signature(\"compare_old_and_new_target_model_params\").change_default_value(False)\n",
    "\n",
    "substitute_value_in_dict(learner_input, \"compare_old_and_new_target_predictions\", False)\n",
    "substitute_value_in_dict(learner_input, \"compare_old_and_new_target_model_params\", False)\n",
    "substitute_value_in_dict(learner_input, \"compare_old_and_new_model_predictions\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_value_in_dict(rl_trainer_input, \"num_episodes\", 1000)\n",
    "\n",
    "rl_trainer_input.pop(\"limit_total_steps\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other value changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agents_trainers_input[\"learning_start_step_delay\"] = 5000\n",
    "#agents_trainers_input[\"learning_start_ep_delay\"] = 150\n",
    "\n",
    "#substitute_value_in_dict(agents_trainers_input, \"learning_start_ep_delay\", 2897)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_value_in_dict(agents_trainers_input, \"batch_size\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_value_in_dict(agents_trainers_input, \"optimization_interval\", 20)\n",
    "substitute_value_in_dict(agents_trainers_input, \"times_to_learn\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_value_in_dict(memory_input, \"capacity\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_input[\"clip_grad_norm\"] = 0.1\n",
    "\n",
    "#substitute_value_in_dict(optimizer_input, \"clip_grad_value\", 10)\n",
    "\n",
    "#substitute_value_in_dict(optimizer_input, \"learning_rate\", 0.006807860813523758)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_value_in_dict(model_input, \"layers\", [32, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_value_in_dict(agents_trainers_input, \"discount_factor\", 0.8790365307757482)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_value_in_dict(learner_input, \"target_update_rate\", 0.5511208693081078)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute_value_in_dict(exploration_strategy_input, \"epsilon_end\", 0.009535369612528788)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen RL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from automl.rl.rl_pipeline import RLPipelineComponent\n",
    "from automl.utils.json_utils.json_component_utils import gen_component_from\n",
    "\n",
    "rl_pipeline : RLPipelineComponent = gen_component_from(rl_pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_pipeline.pass_input({\n",
    "    \"base_directory\" : PATH_TO_STORE_EXPERIMENTS,\n",
    "                        \"artifact_relative_directory\" : experiment_name,\n",
    "                        \"create_new_directory\" : True,\n",
    "                        \"do_full_setup_of_seed\" : True}\n",
    "                        )\n",
    "\n",
    "experiment_path = rl_pipeline.get_artifact_directory()\n",
    "\n",
    "print(f\"Experiment path: {experiment_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.global_logger import activate_global_logger\n",
    "\n",
    "activate_global_logger(rl_pipeline.get_artifact_directory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.basic_components.exec_component import save_state\n",
    "\n",
    "\n",
    "save_state(rl_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rl_pipeline.save_configuration(save_exposed_values=True)\n",
    "from automl.basic_components.state_management import save_state\n",
    "\n",
    "\n",
    "save_state(rl_pipeline, save_definition=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATE_NUMBER = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "results_logger = rl_pipeline.get_results_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_logger.plot_graph(x_axis='episode', y_axis=[('total_reward', name)], to_show=False)\n",
    "\n",
    "try:\n",
    "    results_logger.plot_confidence_interval(x_axis='episode', y_column='episode_reward',show_std=True, to_show=False, y_values_label=experiment_name, aggregate_number=AGGREGATE_NUMBER)\n",
    "    results_logger.plot_linear_regression(x_axis='episode', y_axis='episode_reward', to_show=False, y_label=experiment_name + '_linear')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.rl.evaluators.rl_component_evaluator import RLPipelineEvaluator\n",
    "from automl.rl.evaluators.rl_evaluator_player import EvaluatorWithPlayer\n",
    "from automl.rl.evaluators.rl_std_avg_evaluator import LastValuesAvgStdEvaluator\n",
    "from automl.rl.rl_player.rl_parallel_player import RLParallelPlayer\n",
    "\n",
    "\n",
    "evaluator_definition = {\n",
    "                        \"__type__\": EvaluatorWithPlayer,\n",
    "                        \"name\": \"EvaluatorWithPlayer\",\n",
    "                        \"input\" : {\n",
    "                            \"rl_player_definition\" : (RLParallelPlayer, {}),\n",
    "                            \"base_evaluator\" : (LastValuesAvgStdEvaluator, {\"value_to_use\" : \"episode_reward\"})\n",
    "                            }       \n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_component : RLPipelineEvaluator = gen_component_from(evaluator_definition)\n",
    "rl_pipeline.define_component_as_child(evaluator_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = evaluator_component.evaluate(rl_pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
