{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\")) #make the folder \"automl\" part of this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2\n",
    "state_memory_size = 1\n",
    "limit_steps = 60\n",
    "optimization_interval = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Trainer Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from automl.rl_components.rl_pipeline import RLPipelineComponent\n",
    "\n",
    "\n",
    "rl_pipeline_input = {\n",
    "    \"device\" : \"gpu\",\n",
    "    \"num_episodes\" : num_episodes,\n",
    "    \"state_memory_size\" : state_memory_size,\n",
    "    \"limit_steps\" : limit_steps ,\n",
    "    \"optimization_interval\" : optimization_interval\n",
    "}\n",
    "\n",
    "\n",
    "rl_pipeline = RLPipelineComponent(input=rl_pipeline_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening a log... Log Dir: data\\logs Log Name:\n",
      "Logs folder did not exist, creating one at data\\logs\n",
      "Log directory did not exist, creating it at: data\\logs\\log_0\n",
      "Type of object with name: <class 'automl.rl_components.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: There was an error trying to setup the device in 'gpu': Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu\n",
      "RLPipelineComponent: The model will trained and evaluated on: cpu\n",
      "RLPipelineComponent: Creating agents\n",
      "Opening a log... Log Dir: data\\logs\\log_0 Log Name:agent_1\n",
      "Log directory did not exist, creating it at: data\\logs\\log_0\\agent_1\n",
      "RLPipelineComponent: State for agent agent_1 has shape: Z: 280 Y: 480 X: 3\n",
      "Action space of agent paddle_0: Discrete(3)\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "Opening a log... Log Dir: data\\logs\\log_0 Log Name:agent_2\n",
      "Log directory did not exist, creating it at: data\\logs\\log_0\\agent_2\n",
      "RLPipelineComponent: State for agent agent_2 has shape: Z: 280 Y: 480 X: 3\n",
      "Action space of agent paddle_1: Discrete(3)\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized {'paddle_0': <automl.rl_components.agent_components.AgentSchema object at 0x0000021A212BE7C0>, 'paddle_1': <automl.rl_components.agent_components.AgentSchema object at 0x0000021A212BE880>} agents\n",
      "RLPipelineComponent: Initializing trainer\n",
      "Type of object with name: <class 'automl.rl_components.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episodes of training\n",
      "RLPipelineComponent: RLTrainerComponent: Initializing wandb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 0\n",
      "<class 'int'>\n",
      "<class 'numpy.int64'>\n",
      "Creating policy model using default values and passed shape... Model input: {'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3}\n",
      "Type of object with name: <class 'automl.rl_components.agent_components.AgentSchema'> and name passed: \n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing policy model...\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': device(type='cpu')}\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 0, optimizing at step 0 that is the total step 0\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent AgentSchema\n",
      "<class 'int'>\n",
      "<class 'numpy.int64'>\n",
      "Creating policy model using default values and passed shape... Model input: {'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3}\n",
      "Type of object with name: <class 'automl.rl_components.agent_components.AgentSchema'> and name passed: \n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing policy model...\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0015687942504882812 seconds\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': device(type='cpu')}\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 0, optimizing at step 50 that is the total step 50\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 0, reached step 60 that is beyond the current limit, 60\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 0 with duration: 60, total reward: 6.555555555555547 and real time duration of 1.202765703201294 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 1\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 1, optimizing at step 40 that is the total step 100\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 1, reached step 60 that is beyond the current limit, 60\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 1 with duration: 60, total reward: 6.555555555555547 and real time duration of 0.7373406887054443 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: \n",
      "Training took 1.942084550857544 seconds, 0.016184037923812865 per step (120)\n",
      "RLPipelineComponent: RLTrainerComponent: Closing wandb...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode_time_per_step_durations</td><td>█▁</td></tr><tr><td>total_reward</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode_time_per_step_durations</td><td>0.01229</td></tr><tr><td>total_reward</td><td>6.55556</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br><code>wandb sync data\\logs\\log_0\\wandb\\offline-run-20250212_111409-pss7n5em<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>data\\logs\\log_0\\wandb\\offline-run-20250212_111409-pss7n5em\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rl_pipeline.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"__type__\": \"<class 'automl.rl_components.rl_pipeline.RLPipelineComponent'>\",\n",
      "    \"name\": \"RLPipelineComponent\",\n",
      "    \"input\": {\n",
      "        \"num_episodes\": 2,\n",
      "        \"state_memory_size\": 1,\n",
      "        \"limit_steps\": 60,\n",
      "        \"optimization_interval\": 50,\n",
      "        \"environment\": {\n",
      "            \"__type__\": \"PettingZooEnvironmentLoader\",\n",
      "            \"name\": \"PettingZooEnvironmentLoader\",\n",
      "            \"localization\": [\n",
      "                0\n",
      "            ]\n",
      "        },\n",
      "        \"agents\": {\n",
      "            \"paddle_0\": {\n",
      "                \"__type__\": \"AgentSchema\",\n",
      "                \"name\": \"agent_1\",\n",
      "                \"localization\": [\n",
      "                    1\n",
      "                ]\n",
      "            },\n",
      "            \"paddle_1\": {\n",
      "                \"__type__\": \"AgentSchema\",\n",
      "                \"name\": \"agent_2\",\n",
      "                \"localization\": [\n",
      "                    2\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"save_interval\": 100,\n",
      "        \"rl_trainer\": \"\",\n",
      "        \"created_agents_input\": {}\n",
      "    },\n",
      "    \"child_components\": [\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.environment.environment_components.PettingZooEnvironmentLoader'>\",\n",
      "            \"name\": \"PettingZooEnvironmentLoader\",\n",
      "            \"input\": {\n",
      "                \"petting_zoo_environment\": \"cooperative_pong\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.agent_components.AgentSchema'>\",\n",
      "            \"name\": \"agent_1\",\n",
      "            \"input\": {\n",
      "                \"name\": \"agent_1\",\n",
      "                \"model_input_shape\": [\n",
      "                    3,\n",
      "                    480,\n",
      "                    280\n",
      "                ],\n",
      "                \"training_context\": {\n",
      "                    \"total_steps\": 120\n",
      "                },\n",
      "                \"batch_size\": 64,\n",
      "                \"discount_factor\": 0.95,\n",
      "                \"exploration_strategy\": {\n",
      "                    \"__type__\": \"EpsilonGreedyStrategy\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"memory\": {\n",
      "                    \"__type__\": \"MemoryComponent\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        1\n",
      "                    ]\n",
      "                },\n",
      "                \"learner\": {\n",
      "                    \"__type__\": \"DeepQLearnerSchema\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        2\n",
      "                    ]\n",
      "                },\n",
      "                \"policy_model\": {\n",
      "                    \"__type__\": \"ConvModelSchema\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        3\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.exploration_strategy_components.EpsilonGreedyStrategy'>\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"input\": {\n",
      "                        \"training_context\": {\n",
      "                            \"total_steps\": 120\n",
      "                        },\n",
      "                        \"epsilon_end\": 0.025,\n",
      "                        \"epsilon_start\": 1.0,\n",
      "                        \"epsilon_decay\": 0.01\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.memory_components.MemoryComponent'>\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"input\": {\n",
      "                        \"capacity\": 200\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.learner_component.DeepQLearnerSchema'>\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentSchema\",\n",
      "                            \"name\": \"agent_1\",\n",
      "                            \"localization\": [\n",
      "                                1\n",
      "                            ]\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.model_components.ConvModelSchema'>\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"input\": {\n",
      "                        \"board_x\": 3,\n",
      "                        \"board_y\": 480,\n",
      "                        \"board_z\": 280,\n",
      "                        \"output_size\": 3\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.agent_components.AgentSchema'>\",\n",
      "            \"name\": \"agent_2\",\n",
      "            \"input\": {\n",
      "                \"name\": \"agent_2\",\n",
      "                \"model_input_shape\": [\n",
      "                    3,\n",
      "                    480,\n",
      "                    280\n",
      "                ],\n",
      "                \"training_context\": {\n",
      "                    \"total_steps\": 120\n",
      "                },\n",
      "                \"batch_size\": 64,\n",
      "                \"discount_factor\": 0.95,\n",
      "                \"exploration_strategy\": {\n",
      "                    \"__type__\": \"EpsilonGreedyStrategy\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"memory\": {\n",
      "                    \"__type__\": \"MemoryComponent\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        1\n",
      "                    ]\n",
      "                },\n",
      "                \"learner\": {\n",
      "                    \"__type__\": \"DeepQLearnerSchema\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        2\n",
      "                    ]\n",
      "                },\n",
      "                \"policy_model\": {\n",
      "                    \"__type__\": \"ConvModelSchema\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        3\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.exploration_strategy_components.EpsilonGreedyStrategy'>\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"input\": {\n",
      "                        \"training_context\": {\n",
      "                            \"total_steps\": 120\n",
      "                        },\n",
      "                        \"epsilon_end\": 0.025,\n",
      "                        \"epsilon_start\": 1.0,\n",
      "                        \"epsilon_decay\": 0.01\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.memory_components.MemoryComponent'>\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"input\": {\n",
      "                        \"capacity\": 200\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.learner_component.DeepQLearnerSchema'>\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentSchema\",\n",
      "                            \"name\": \"agent_2\",\n",
      "                            \"localization\": [\n",
      "                                2\n",
      "                            ]\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.model_components.ConvModelSchema'>\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"input\": {\n",
      "                        \"board_x\": 3,\n",
      "                        \"board_y\": 480,\n",
      "                        \"board_z\": 280,\n",
      "                        \"output_size\": 3\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.rl_trainer_component.RLTrainerComponent'>\",\n",
      "            \"name\": \"RLTrainerComponent\",\n",
      "            \"input\": {\n",
      "                \"num_episodes\": 2,\n",
      "                \"state_memory_size\": 1,\n",
      "                \"environment\": {\n",
      "                    \"__type__\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"name\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"localization\": [\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"limit_steps\": 60,\n",
      "                \"optimization_interval\": 50,\n",
      "                \"agents\": {\n",
      "                    \"paddle_0\": {\n",
      "                        \"__type__\": \"AgentSchema\",\n",
      "                        \"name\": \"agent_1\",\n",
      "                        \"localization\": [\n",
      "                            1\n",
      "                        ]\n",
      "                    },\n",
      "                    \"paddle_1\": {\n",
      "                        \"__type__\": \"AgentSchema\",\n",
      "                        \"name\": \"agent_2\",\n",
      "                        \"localization\": [\n",
      "                            2\n",
      "                        ]\n",
      "                    }\n",
      "                },\n",
      "                \"save_interval\": 100\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rl_pipeline.save_configuration(toPrint=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
