{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\")) #make the folder \"automl\" part of this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2\n",
    "state_memory_size = 1\n",
    "limit_steps = 60\n",
    "optimization_interval = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Trainer Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from automl.rl_components.rl_pipeline import RLPipelineComponent\n",
    "\n",
    "\n",
    "rl_pipeline_input = {\n",
    "    \"device\" : \"gpu\",\n",
    "    \"num_episodes\" : num_episodes,\n",
    "    \"state_memory_size\" : state_memory_size,\n",
    "    \"limit_steps\" : limit_steps ,\n",
    "    \"optimization_interval\" : optimization_interval\n",
    "}\n",
    "\n",
    "\n",
    "rl_pipeline = RLPipelineComponent(input=rl_pipeline_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening a log... Log Dir: data\\logs Log Name:\n",
      "Log directory did not exist, creating it at: data\\logs\\log_27\n",
      "Type of object with name: <class 'automl.rl_components.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: There was an error trying to setup the device in 'gpu': Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu\n",
      "RLPipelineComponent: The model will trained and evaluated on: cpu\n",
      "RLPipelineComponent: Creating agents\n",
      "Opening a log... Log Dir: data\\logs\\log_27 Log Name:agent_1\n",
      "Log directory did not exist, creating it at: data\\logs\\log_27\\agent_1\n",
      "RLPipelineComponent: State for agent agent_1 has shape: Z: 280 Y: 480 X: 3\n",
      "Action space of agent paddle_0: Discrete(3)\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "Opening a log... Log Dir: data\\logs\\log_27 Log Name:agent_2\n",
      "Log directory did not exist, creating it at: data\\logs\\log_27\\agent_2\n",
      "RLPipelineComponent: State for agent agent_2 has shape: Z: 280 Y: 480 X: 3\n",
      "Action space of agent paddle_1: Discrete(3)\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized {'paddle_0': <automl.rl_components.agent_components.AgentComponent object at 0x000001ACE0A4CBE0>, 'paddle_1': <automl.rl_components.agent_components.AgentComponent object at 0x000001ACE0A4CC70>} agents\n",
      "RLPipelineComponent: Initializing trainer\n",
      "Type of object with name: <class 'automl.rl_components.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episodes of training\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 0\n",
      "<class 'int'>\n",
      "<class 'numpy.int64'>\n",
      "Creating policy model using default values and passed shape... Model input: {'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3}\n",
      "Type of object with name: <class 'automl.rl_components.agent_components.AgentComponent'> and name passed: \n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing policy model...\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': device(type='cpu')}\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 0, optimizing at step 0 that is the total step 0\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent AgentComponent\n",
      "<class 'int'>\n",
      "<class 'numpy.int64'>\n",
      "Creating policy model using default values and passed shape... Model input: {'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3}\n",
      "Type of object with name: <class 'automl.rl_components.agent_components.AgentComponent'> and name passed: \n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing policy model...\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.001001596450805664 seconds\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': device(type='cpu')}\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 0, optimizing at step 50 that is the total step 50\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 0 with duration: 60, total reward: 0 and real time duration of 1.1855177879333496 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 1\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 1, optimizing at step 40 that is the total step 100\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 1 with duration: 60, total reward: 0 and real time duration of 0.7324106693267822 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: \n",
      "Training took 1.920421838760376 seconds, 0.016003515323003134 per step (120)\n"
     ]
    }
   ],
   "source": [
    "rl_pipeline.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"__type__\": \"<class 'automl.rl_components.rl_pipeline.RLPipelineComponent'>\",\n",
      "    \"name\": \"RLPipelineComponent\",\n",
      "    \"input\": {\n",
      "        \"num_episodes\": 2,\n",
      "        \"state_memory_size\": 1,\n",
      "        \"limit_steps\": 60,\n",
      "        \"optimization_interval\": 50,\n",
      "        \"environment\": {\n",
      "            \"__type__\": \"PettingZooEnvironmentLoader\",\n",
      "            \"name\": \"PettingZooEnvironmentLoader\",\n",
      "            \"localization\": [\n",
      "                0\n",
      "            ]\n",
      "        },\n",
      "        \"agents\": {\n",
      "            \"paddle_0\": {\n",
      "                \"__type__\": \"AgentComponent\",\n",
      "                \"name\": \"agent_1\",\n",
      "                \"localization\": [\n",
      "                    1\n",
      "                ]\n",
      "            },\n",
      "            \"paddle_1\": {\n",
      "                \"__type__\": \"AgentComponent\",\n",
      "                \"name\": \"agent_2\",\n",
      "                \"localization\": [\n",
      "                    2\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"save_interval\": 100,\n",
      "        \"rl_trainer\": \"\",\n",
      "        \"created_agents_input\": {}\n",
      "    },\n",
      "    \"child_components\": [\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.environment.environment_components.PettingZooEnvironmentLoader'>\",\n",
      "            \"name\": \"PettingZooEnvironmentLoader\",\n",
      "            \"input\": {\n",
      "                \"petting_zoo_environment\": \"cooperative_pong\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.agent_components.AgentComponent'>\",\n",
      "            \"name\": \"agent_1\",\n",
      "            \"input\": {\n",
      "                \"name\": \"agent_1\",\n",
      "                \"model_input_shape\": [\n",
      "                    3,\n",
      "                    480,\n",
      "                    280\n",
      "                ],\n",
      "                \"training_context\": {\n",
      "                    \"total_steps\": 120\n",
      "                },\n",
      "                \"batch_size\": 64,\n",
      "                \"discount_factor\": 0.95,\n",
      "                \"exploration_strategy\": {\n",
      "                    \"__type__\": \"EpsilonGreedyStrategy\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"memory\": {\n",
      "                    \"__type__\": \"MemoryComponent\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        1\n",
      "                    ]\n",
      "                },\n",
      "                \"learner\": {\n",
      "                    \"__type__\": \"DeepQLearnerComponent\",\n",
      "                    \"name\": \"DeepQLearnerComponent\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        2\n",
      "                    ]\n",
      "                },\n",
      "                \"policy_model\": {\n",
      "                    \"__type__\": \"ConvModelComponent\",\n",
      "                    \"name\": \"ConvModelComponent\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        3\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.exploration_strategy_components.EpsilonGreedyStrategy'>\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"input\": {\n",
      "                        \"training_context\": {\n",
      "                            \"total_steps\": 120\n",
      "                        },\n",
      "                        \"epsilon_end\": 0.025,\n",
      "                        \"epsilon_start\": 1.0,\n",
      "                        \"epsilon_decay\": 0.01\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.memory_components.MemoryComponent'>\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"input\": {\n",
      "                        \"capacity\": 200\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.learner_component.DeepQLearnerComponent'>\",\n",
      "                    \"name\": \"DeepQLearnerComponent\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentComponent\",\n",
      "                            \"name\": \"agent_1\",\n",
      "                            \"localization\": [\n",
      "                                1\n",
      "                            ]\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.model_components.ConvModelComponent'>\",\n",
      "                    \"name\": \"ConvModelComponent\",\n",
      "                    \"input\": {\n",
      "                        \"board_x\": 3,\n",
      "                        \"board_y\": 480,\n",
      "                        \"board_z\": 280,\n",
      "                        \"output_size\": 3\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.agent_components.AgentComponent'>\",\n",
      "            \"name\": \"agent_2\",\n",
      "            \"input\": {\n",
      "                \"name\": \"agent_2\",\n",
      "                \"model_input_shape\": [\n",
      "                    3,\n",
      "                    480,\n",
      "                    280\n",
      "                ],\n",
      "                \"training_context\": {\n",
      "                    \"total_steps\": 120\n",
      "                },\n",
      "                \"batch_size\": 64,\n",
      "                \"discount_factor\": 0.95,\n",
      "                \"exploration_strategy\": {\n",
      "                    \"__type__\": \"EpsilonGreedyStrategy\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"memory\": {\n",
      "                    \"__type__\": \"MemoryComponent\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        1\n",
      "                    ]\n",
      "                },\n",
      "                \"learner\": {\n",
      "                    \"__type__\": \"DeepQLearnerComponent\",\n",
      "                    \"name\": \"DeepQLearnerComponent\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        2\n",
      "                    ]\n",
      "                },\n",
      "                \"policy_model\": {\n",
      "                    \"__type__\": \"ConvModelComponent\",\n",
      "                    \"name\": \"ConvModelComponent\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        3\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.exploration_strategy_components.EpsilonGreedyStrategy'>\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"input\": {\n",
      "                        \"training_context\": {\n",
      "                            \"total_steps\": 120\n",
      "                        },\n",
      "                        \"epsilon_end\": 0.025,\n",
      "                        \"epsilon_start\": 1.0,\n",
      "                        \"epsilon_decay\": 0.01\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.memory_components.MemoryComponent'>\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"input\": {\n",
      "                        \"capacity\": 200\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.learner_component.DeepQLearnerComponent'>\",\n",
      "                    \"name\": \"DeepQLearnerComponent\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentComponent\",\n",
      "                            \"name\": \"agent_2\",\n",
      "                            \"localization\": [\n",
      "                                2\n",
      "                            ]\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.model_components.ConvModelComponent'>\",\n",
      "                    \"name\": \"ConvModelComponent\",\n",
      "                    \"input\": {\n",
      "                        \"board_x\": 3,\n",
      "                        \"board_y\": 480,\n",
      "                        \"board_z\": 280,\n",
      "                        \"output_size\": 3\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.rl_trainer_component.RLTrainerComponent'>\",\n",
      "            \"name\": \"RLTrainerComponent\",\n",
      "            \"input\": {\n",
      "                \"num_episodes\": 2,\n",
      "                \"state_memory_size\": 1,\n",
      "                \"environment\": {\n",
      "                    \"__type__\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"name\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"localization\": [\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"limit_steps\": 60,\n",
      "                \"optimization_interval\": 50,\n",
      "                \"agents\": {\n",
      "                    \"paddle_0\": {\n",
      "                        \"__type__\": \"AgentComponent\",\n",
      "                        \"name\": \"agent_1\",\n",
      "                        \"localization\": [\n",
      "                            1\n",
      "                        ]\n",
      "                    },\n",
      "                    \"paddle_1\": {\n",
      "                        \"__type__\": \"AgentComponent\",\n",
      "                        \"name\": \"agent_2\",\n",
      "                        \"localization\": [\n",
      "                            2\n",
      "                        ]\n",
      "                    }\n",
      "                },\n",
      "                \"save_interval\": 100\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rl_pipeline.save_configuration(toPrint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_configuration = rl_pipeline.lg.logDir + '\\\\configuration.json' #gets the configuration of the latest component\n",
    "\n",
    "from automl.logger_component import LoggerSchema\n",
    "\n",
    "loaded_component = LoggerSchema.load_configuration(path_of_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening a log... Log Dir: data\\logs Log Name:\n",
      "Log directory did not exist, creating it at: data\\logs\\log_28\n",
      "Type of object with name: <class 'automl.rl_components.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: The model will trained and evaluated on: cpu\n",
      "RLPipelineComponent: Initializing trainer\n",
      "Type of object with name: <class 'automl.rl_components.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episodes of training\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 0\n",
      "Opening a log... Log Dir: data\\logs Log Name:\n",
      "Log directory did not exist, creating it at: data\\logs\\log_29\n",
      "Type of object with name: <class 'automl.rl_components.agent_components.AgentComponent'> and name passed: \n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing policy model...\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': None}\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 0, optimizing at step 0 that is the total step 0\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "Opening a log... Log Dir: data\\logs Log Name:\n",
      "Log directory did not exist, creating it at: data\\logs\\log_30\n",
      "Type of object with name: <class 'automl.rl_components.agent_components.AgentComponent'> and name passed: \n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing policy model...\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0009992122650146484 seconds\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': None}\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 0, optimizing at step 50 that is the total step 50\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 0 with duration: 60, total reward: 0 and real time duration of 1.2405436038970947 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 1\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 1, optimizing at step 40 that is the total step 100\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 1, reached step 60 that is beyond the current limit, 60\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 1 with duration: 60, total reward: 0 and real time duration of 0.8210208415985107 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: \n",
      "Training took 2.0625627040863037 seconds, 0.017188022534052532 per step (120)\n"
     ]
    }
   ],
   "source": [
    "loaded_component.train() #tries to train it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"__type__\": \"<class 'automl.rl_components.rl_pipeline.RLPipelineComponent'>\",\n",
      "    \"name\": \"RLPipelineComponent\",\n",
      "    \"input\": {\n",
      "        \"num_episodes\": 2,\n",
      "        \"state_memory_size\": 1,\n",
      "        \"limit_steps\": 60,\n",
      "        \"optimization_interval\": 50,\n",
      "        \"environment\": {\n",
      "            \"__type__\": \"PettingZooEnvironmentLoader\",\n",
      "            \"name\": \"PettingZooEnvironmentLoader\",\n",
      "            \"localization\": []\n",
      "        },\n",
      "        \"agents\": {\n",
      "            \"paddle_0\": {\n",
      "                \"__type__\": \"AgentComponent\",\n",
      "                \"name\": \"agent_1\",\n",
      "                \"localization\": []\n",
      "            },\n",
      "            \"paddle_1\": {\n",
      "                \"__type__\": \"AgentComponent\",\n",
      "                \"name\": \"agent_2\",\n",
      "                \"localization\": []\n",
      "            }\n",
      "        },\n",
      "        \"save_interval\": 100,\n",
      "        \"rl_trainer\": \"\",\n",
      "        \"created_agents_input\": {}\n",
      "    },\n",
      "    \"child_components\": [\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.environment.environment_components.PettingZooEnvironmentLoader'>\",\n",
      "            \"name\": \"PettingZooEnvironmentLoader\",\n",
      "            \"input\": {\n",
      "                \"petting_zoo_environment\": \"cooperative_pong\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.agent_components.AgentComponent'>\",\n",
      "            \"name\": \"agent_1\",\n",
      "            \"input\": {\n",
      "                \"name\": \"agent_1\",\n",
      "                \"model_input_shape\": [\n",
      "                    3,\n",
      "                    480,\n",
      "                    280\n",
      "                ],\n",
      "                \"training_context\": {\n",
      "                    \"total_steps\": 120\n",
      "                },\n",
      "                \"batch_size\": 64,\n",
      "                \"discount_factor\": 0.95,\n",
      "                \"exploration_strategy\": {\n",
      "                    \"__type__\": \"EpsilonGreedyStrategy\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"localization\": []\n",
      "                },\n",
      "                \"memory\": {\n",
      "                    \"__type__\": \"MemoryComponent\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"localization\": []\n",
      "                },\n",
      "                \"learner\": {\n",
      "                    \"__type__\": \"DeepQLearnerComponent\",\n",
      "                    \"name\": \"DeepQLearnerComponent\",\n",
      "                    \"localization\": []\n",
      "                },\n",
      "                \"policy_model\": {\n",
      "                    \"__type__\": \"ConvModelComponent\",\n",
      "                    \"name\": \"ConvModelComponent\",\n",
      "                    \"localization\": []\n",
      "                },\n",
      "                \"model_output_shape\": \"\"\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.exploration_strategy_components.EpsilonGreedyStrategy'>\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"input\": {\n",
      "                        \"training_context\": {\n",
      "                            \"total_steps\": 120\n",
      "                        },\n",
      "                        \"epsilon_end\": 0.025,\n",
      "                        \"epsilon_start\": 1.0,\n",
      "                        \"epsilon_decay\": 0.01\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.memory_components.MemoryComponent'>\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"input\": {\n",
      "                        \"capacity\": 200\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.learner_component.DeepQLearnerComponent'>\",\n",
      "                    \"name\": \"DeepQLearnerComponent\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentComponent\",\n",
      "                            \"name\": \"agent_1\",\n",
      "                            \"localization\": []\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.model_components.ConvModelComponent'>\",\n",
      "                    \"name\": \"ConvModelComponent\",\n",
      "                    \"input\": {\n",
      "                        \"board_x\": 3,\n",
      "                        \"board_y\": 480,\n",
      "                        \"board_z\": 280,\n",
      "                        \"output_size\": 3\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.agent_components.AgentComponent'>\",\n",
      "            \"name\": \"agent_2\",\n",
      "            \"input\": {\n",
      "                \"name\": \"agent_2\",\n",
      "                \"model_input_shape\": [\n",
      "                    3,\n",
      "                    480,\n",
      "                    280\n",
      "                ],\n",
      "                \"training_context\": {\n",
      "                    \"total_steps\": 120\n",
      "                },\n",
      "                \"batch_size\": 64,\n",
      "                \"discount_factor\": 0.95,\n",
      "                \"exploration_strategy\": {\n",
      "                    \"__type__\": \"EpsilonGreedyStrategy\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"localization\": []\n",
      "                },\n",
      "                \"memory\": {\n",
      "                    \"__type__\": \"MemoryComponent\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"localization\": []\n",
      "                },\n",
      "                \"learner\": {\n",
      "                    \"__type__\": \"DeepQLearnerComponent\",\n",
      "                    \"name\": \"DeepQLearnerComponent\",\n",
      "                    \"localization\": []\n",
      "                },\n",
      "                \"policy_model\": {\n",
      "                    \"__type__\": \"ConvModelComponent\",\n",
      "                    \"name\": \"ConvModelComponent\",\n",
      "                    \"localization\": []\n",
      "                },\n",
      "                \"model_output_shape\": \"\"\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.exploration_strategy_components.EpsilonGreedyStrategy'>\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"input\": {\n",
      "                        \"training_context\": {\n",
      "                            \"total_steps\": 120\n",
      "                        },\n",
      "                        \"epsilon_end\": 0.025,\n",
      "                        \"epsilon_start\": 1.0,\n",
      "                        \"epsilon_decay\": 0.01\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.memory_components.MemoryComponent'>\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"input\": {\n",
      "                        \"capacity\": 200\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.learner_component.DeepQLearnerComponent'>\",\n",
      "                    \"name\": \"DeepQLearnerComponent\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentComponent\",\n",
      "                            \"name\": \"agent_2\",\n",
      "                            \"localization\": []\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.model_components.ConvModelComponent'>\",\n",
      "                    \"name\": \"ConvModelComponent\",\n",
      "                    \"input\": {\n",
      "                        \"board_x\": 3,\n",
      "                        \"board_y\": 480,\n",
      "                        \"board_z\": 280,\n",
      "                        \"output_size\": 3\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.rl_trainer_component.RLTrainerComponent'>\",\n",
      "            \"name\": \"RLTrainerComponent\",\n",
      "            \"input\": {\n",
      "                \"num_episodes\": 2,\n",
      "                \"state_memory_size\": 1,\n",
      "                \"environment\": {\n",
      "                    \"__type__\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"name\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"localization\": []\n",
      "                },\n",
      "                \"limit_steps\": 60,\n",
      "                \"optimization_interval\": 50,\n",
      "                \"agents\": {\n",
      "                    \"paddle_0\": {\n",
      "                        \"__type__\": \"AgentComponent\",\n",
      "                        \"name\": \"agent_1\",\n",
      "                        \"localization\": []\n",
      "                    },\n",
      "                    \"paddle_1\": {\n",
      "                        \"__type__\": \"AgentComponent\",\n",
      "                        \"name\": \"agent_2\",\n",
      "                        \"localization\": []\n",
      "                    }\n",
      "                },\n",
      "                \"save_interval\": 100\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.rl_trainer_component.RLTrainerComponent'>\",\n",
      "            \"name\": \"RLTrainerComponent\",\n",
      "            \"input\": {\n",
      "                \"num_episodes\": 2,\n",
      "                \"state_memory_size\": 1,\n",
      "                \"environment\": {\n",
      "                    \"__type__\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"name\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"localization\": []\n",
      "                },\n",
      "                \"limit_steps\": 60,\n",
      "                \"optimization_interval\": 50,\n",
      "                \"agents\": {\n",
      "                    \"paddle_0\": {\n",
      "                        \"__type__\": \"AgentComponent\",\n",
      "                        \"name\": \"agent_1\",\n",
      "                        \"localization\": []\n",
      "                    },\n",
      "                    \"paddle_1\": {\n",
      "                        \"__type__\": \"AgentComponent\",\n",
      "                        \"name\": \"agent_2\",\n",
      "                        \"localization\": []\n",
      "                    }\n",
      "                },\n",
      "                \"save_interval\": 100\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "loaded_component.save_configuration(toPrint=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
