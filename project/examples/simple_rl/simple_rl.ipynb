{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\")) #make the folder \"automl\" part of this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 3\n",
    "state_memory_size = 1\n",
    "limit_steps = 200\n",
    "optimization_interval = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Trainer Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from automl.base_configurations.base_configurations import load_configuration\n",
    "\n",
    "#rl_pipeline_input = {\n",
    "#    \"device\" : \"cpu\",\n",
    "#    \"num_episodes\" : num_episodes,\n",
    "#    \"state_memory_size\" : 2,\n",
    "#    \"limit_steps\" : limit_steps ,\n",
    "#    \"optimization_interval\" : optimization_interval,\n",
    "#    \"agent_input\" : {\n",
    "#        \"exploration_strategy_class\" : \"<class 'automl.rl.exploration.epsilon_greedy.EpsilonGreedyStrategy'>\",\n",
    "#        \"model_class\" : \"<class 'automl.ml.models.neural_model.FullyConnectedModelSchema'>\"\n",
    "#    }\n",
    "#}\n",
    "#\n",
    "#\n",
    "#rl_pipeline = RLPipelineComponent(input=rl_pipeline_input)\n",
    "\n",
    "rl_pipeline = load_configuration(configuration_name='basic_rl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_log_directory for object RLPipelineComponent\n",
      "Opening a log in directory: data\\experiments\\RLPipelineComponent_21, with name:\n",
      "WARNING: Log directory already existed, was this inteded behaviour?\n",
      "Type of object with name: <class 'automl.rl.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Created logger in directory: data\\experiments\\RLPipelineComponent_21\n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: The model will trained and evaluated on: cuda\n",
      "RLPipelineComponent: No agents defined, creating them...\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized agents\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x000002D1BF6DCC70> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\RLPipelineComponent_21, with name:agent_1\n",
      "Log directory did not exist, creating it at: data\\experiments\\RLPipelineComponent_21\\agent_1\n",
      "RLPipelineComponent: State for agent AgentSchema has shape: torch.Size([3, 280, 480])\n",
      "RLPipelineComponent: Action space of agent <automl.rl.agent.agent_components.AgentSchema object at 0x000002D1BF6DCCA0> has shape: Discrete(3)\n",
      "Opening a log in directory: data\\experiments\\RLPipelineComponent_21, with name:agent_2\n",
      "Log directory did not exist, creating it at: data\\experiments\\RLPipelineComponent_21\\agent_2\n",
      "WARNING: input with key state_memory_size passed to component RLTrainerComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLTrainerComponent: Created logger in directory: data\\experiments\\RLPipelineComponent_21\n",
      "RLTrainerComponent: Agent paddle_0 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Agent paddle_1 came without a trainer, creating one...\n",
      "WARNING: input with key logger_object passed to component AgentTrainer but not in its input signature, will be ignored\n",
      "RLTrainerComponent: Starting to run episodes of training\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\RLPipelineComponent_21\\agent_1\n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing agent with more than one state memory size (2)\n",
      "agent_1: State length is 480\n",
      "agent_1: Initializing policy model...\n",
      "agent_1: Creating policy model...\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "AgentSchema: Created logger in directory: data\\experiments\\RLPipelineComponent_21\\agent_2\n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing agent with more than one state memory size (2)\n",
      "agent_2: State length is 480\n",
      "agent_2: Initializing policy model...\n",
      "agent_2: Creating policy model...\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "AgentTrainer: Setting up episode 1\n",
      "AgentTrainer: Setting up episode 1\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "Created fully connected model with input size = 806400, and output size = 3\n",
      "Initializing model with input{'hidden_layers': 3, 'hidden_size': 64, 'input_shape': (2, torch.Size([3, 280, 480])), 'output_shape': Discrete(3), 'device': device(type='cuda')}\n",
      "Size of model input: torch.Size([1, 806400])\n",
      "Max values: tensor([ 0.0307,  0.1053, -0.0530], device='cuda:0'), Max index: tensor([0, 0, 0], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "action is not in action space",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrl_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:367\u001b[0m, in \u001b[0;36mrequires_input_proccess.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_was_proccessed:\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproccess_input()\n\u001b[1;32m--> 367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\rl_pipeline.py:172\u001b[0m, in \u001b[0;36mRLPipelineComponent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect() \u001b[38;5;66;03m#this forces the garbage collector to collect any abandoned objects\u001b[39;00m\n\u001b[0;32m    170\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache() \u001b[38;5;66;03m#this clears cache of cuda\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrl_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:367\u001b[0m, in \u001b[0;36mrequires_input_proccess.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_was_proccessed:\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproccess_input()\n\u001b[1;32m--> 367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\trainers\\rl_trainer_component.py:98\u001b[0m, in \u001b[0;36mRLTrainerComponent.run_episodes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m#each episode is an instance of playing the game\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_episodes):\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__run_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_episode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_in_training \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents_in_training\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    101\u001b[0m         agent_in_training\u001b[38;5;241m.\u001b[39mend_episode() \n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\trainers\\rl_trainer_component.py:136\u001b[0m, in \u001b[0;36mRLTrainerComponent.__run_episode\u001b[1;34m(self, i_episode)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magent_iter(): \u001b[38;5;66;03m#iterates infinitely over the agents that should be acting in the environment\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     agent_in_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents_in_training[agent_name] \u001b[38;5;66;03m#gets the agent trainer\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m     reward, done \u001b[38;5;241m=\u001b[39m \u001b[43magent_in_training\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other_agent_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents_in_training\u001b[38;5;241m.\u001b[39mkeys(): \u001b[38;5;66;03m#make the other agents observe the transiction without remembering it\u001b[39;00m\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m other_agent_name \u001b[38;5;241m!=\u001b[39m agent_name:\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:367\u001b[0m, in \u001b[0;36mrequires_input_proccess.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_was_proccessed:\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproccess_input()\n\u001b[1;32m--> 367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\trainers\\agent_trainer_component.py:106\u001b[0m, in \u001b[0;36mAgentTrainer.do_training_step\u001b[1;34m(self, i_episode, env)\u001b[0m\n\u001b[0;32m    102\u001b[0m observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobserve(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    104\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mselect_action(observation) \u001b[38;5;66;03m# decides the next action to take (can be random)\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#makes the game proccess the action that was taken\u001b[39;00m\n\u001b[0;32m    108\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mlast()\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\environment\\environment_components.py:123\u001b[0m, in \u001b[0;36mPettingZooEnvironmentLoader.step\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\order_enforcing.py:96\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:47\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\.conda\\envs\\aa\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\assert_out_of_bounds.py:17\u001b[0m, in \u001b[0;36mAssertOutOfBoundsWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m         action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminations[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection]\n\u001b[0;32m     21\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncations[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection]\n\u001b[0;32m     22\u001b[0m         )\n\u001b[0;32m     23\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection)\u001b[38;5;241m.\u001b[39mcontains(\n\u001b[0;32m     24\u001b[0m         action\n\u001b[0;32m     25\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction is not in action space\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mAssertionError\u001b[0m: action is not in action space"
     ]
    }
   ],
   "source": [
    "rl_pipeline.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_pipeline.plot_graphs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_pipeline.save_configuration(toPrint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl.loggers.logger_component import LoggerSchema\n",
    "from automl.utils.json_component_utils import json_string_of_component\n",
    "\n",
    "loaded_component = LoggerSchema.load_configuration(rl_pipeline.lg.logDir + '\\\\configuration.json')\n",
    "\n",
    "json_string =  json_string_of_component(loaded_component)\n",
    "\n",
    "print(json_string)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
