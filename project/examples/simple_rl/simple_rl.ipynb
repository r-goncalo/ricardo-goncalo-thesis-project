{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\")) #make the folder \"automl\" part of this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "state_memory_size = 1\n",
    "limit_steps = 100\n",
    "optimization_interval = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Trainer Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'project'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrl_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RLPipelineComponent\n\u001b[0;32m      7\u001b[0m rl_pipeline_input \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_episodes\u001b[39m\u001b[38;5;124m\"\u001b[39m : num_episodes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimization_interval\u001b[39m\u001b[38;5;124m\"\u001b[39m : optimization_interval\n\u001b[0;32m     13\u001b[0m }\n\u001b[0;32m     16\u001b[0m rl_pipeline \u001b[38;5;241m=\u001b[39m RLPipelineComponent(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mrl_pipeline_input)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\rl_pipeline.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexploration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mepsilong_greedy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EpsilonGreedyStrategy\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_components\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvModelSchema\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrl_trainer_component\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RLTrainerComponent\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironment_components\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PettingZooEnvironmentLoader\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger_component\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggerSchema\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\rl_trainer_component.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputSignature, Schema, requires_input_proccess\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger_component\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggerSchema\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mproject\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautoml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresult_logger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResultLogger\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'project'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from automl.rl.rl_pipeline import RLPipelineComponent\n",
    "\n",
    "\n",
    "rl_pipeline_input = {\n",
    "    \"device\" : \"gpu\",\n",
    "    \"num_episodes\" : num_episodes,\n",
    "    \"state_memory_size\" : state_memory_size,\n",
    "    \"limit_steps\" : limit_steps ,\n",
    "    \"optimization_interval\" : optimization_interval\n",
    "}\n",
    "\n",
    "\n",
    "rl_pipeline = RLPipelineComponent(input=rl_pipeline_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening a log... Log Dir: data\\logs Log Name:\n",
      "Log directory did not exist, creating it at: data\\logs\\log_1\n",
      "Type of object with name: <class 'automl.rl_components.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: There was an error trying to setup the device in 'gpu': Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu\n",
      "RLPipelineComponent: The model will trained and evaluated on: cpu\n",
      "RLPipelineComponent: Creating agents\n",
      "Opening a log... Log Dir: data\\logs\\log_1 Log Name:agent_1\n",
      "Log directory did not exist, creating it at: data\\logs\\log_1\\agent_1\n",
      "RLPipelineComponent: State for agent agent_1 has shape: Z: 280 Y: 480 X: 3\n",
      "Action space of agent paddle_0: Discrete(3)\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "Opening a log... Log Dir: data\\logs\\log_1 Log Name:agent_2\n",
      "Log directory did not exist, creating it at: data\\logs\\log_1\\agent_2\n",
      "RLPipelineComponent: State for agent agent_2 has shape: Z: 280 Y: 480 X: 3\n",
      "Action space of agent paddle_1: Discrete(3)\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized {'paddle_0': <automl.rl_components.agent_components.AgentSchema object at 0x0000023FDF43DB80>, 'paddle_1': <automl.rl_components.agent_components.AgentSchema object at 0x0000023FDF43DC40>} agents\n",
      "RLPipelineComponent: Initializing trainer\n",
      "Type of object with name: <class 'automl.rl_components.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episodes of training\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 0\n",
      "<class 'int'>\n",
      "<class 'numpy.int64'>\n",
      "Creating policy model using default values and passed shape... Model input: {'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3}\n",
      "Type of object with name: <class 'automl.rl_components.agent_components.AgentSchema'> and name passed: \n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing policy model...\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': device(type='cpu')}\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 0, optimizing at step 0 that is the total step 0\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent AgentSchema\n",
      "<class 'int'>\n",
      "<class 'numpy.int64'>\n",
      "Creating policy model using default values and passed shape... Model input: {'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3}\n",
      "Type of object with name: <class 'automl.rl_components.agent_components.AgentSchema'> and name passed: \n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing policy model...\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0010051727294921875 seconds\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': device(type='cpu')}\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 0, optimizing at step 50 that is the total step 50\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 0 with duration: 60, total reward: -3.5555555555555634 and real time duration of 1.4598760604858398 seconds\n",
      "Type of object with name: <class 'automl.result_yielder.result_yielder_component.ResultLogger'> and name passed: \n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\result_yielder\\result_yielder_component.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLPipelineComponent: RLTrainerComponent: In episode 1, optimizing at step 40 that is the total step 100\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 1, optimizing at step 90 that is the total step 150\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "Cloning model\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': device(type='cpu')}\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 6.846477031707764 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "Cloning model\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': device(type='cpu')}\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.9119198322296143 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 1 with duration: 96, total reward: 0.4444444444444233 and real time duration of 10.247753620147705 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 2\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 2, optimizing at step 44 that is the total step 200\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.2264528274536133 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1397578716278076 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 2 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.3485960960388184 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 3\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 3, optimizing at step 34 that is the total step 250\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1948113441467285 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.207820177078247 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 3, optimizing at step 84 that is the total step 300\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1703262329101562 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1716301441192627 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 3, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 3 with duration: 100, total reward: 10.999999999999977 and real time duration of 6.1722412109375 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 4\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 4, optimizing at step 34 that is the total step 350\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.106790542602539 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1154077053070068 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 4, optimizing at step 84 that is the total step 400\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0966832637786865 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1453819274902344 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 4, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 4 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.9357054233551025 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 5\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 5, optimizing at step 34 that is the total step 450\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0728983879089355 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.095484972000122 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 5 with duration: 60, total reward: -3.5555555555555634 and real time duration of 2.9725797176361084 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 6\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 6, optimizing at step 24 that is the total step 500\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0943379402160645 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.083113193511963 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 6, optimizing at step 74 that is the total step 550\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.08821439743042 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.076347827911377 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 6, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 6 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.743207931518555 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 7\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 7, optimizing at step 24 that is the total step 600\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0643224716186523 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0675652027130127 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 7, optimizing at step 74 that is the total step 650\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 0.9963102340698242 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0424435138702393 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 7 with duration: 80, total reward: -1.3333333333333481 and real time duration of 5.406911134719849 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 8\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 8, optimizing at step 44 that is the total step 700\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0748450756072998 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1006591320037842 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 8 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.110868215560913 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 9\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 9, optimizing at step 34 that is the total step 750\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.039769172668457 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0965585708618164 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 9 with duration: 68, total reward: -2.6666666666666776 and real time duration of 3.0962650775909424 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 10\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 10, optimizing at step 16 that is the total step 800\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0409715175628662 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.056103229522705 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 10, optimizing at step 66 that is the total step 850\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0309638977050781 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.057988166809082 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 10 with duration: 96, total reward: 0.4444444444444233 and real time duration of 5.680783987045288 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 11\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 11, optimizing at step 20 that is the total step 900\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0508403778076172 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.073465347290039 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 11, optimizing at step 70 that is the total step 950\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.047349452972412 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.09541916847229 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 11, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 11 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.791629314422607 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 12\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 12, optimizing at step 20 that is the total step 1000\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0402042865753174 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0516142845153809 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 12, optimizing at step 70 that is the total step 1050\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.062145471572876 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0615184307098389 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 12, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 12 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.764633893966675 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 13\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 13, optimizing at step 20 that is the total step 1100\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0468113422393799 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0868785381317139 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 13 with duration: 60, total reward: -3.5555555555555634 and real time duration of 2.963813066482544 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 14\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 14, optimizing at step 10 that is the total step 1150\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0769507884979248 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0843515396118164 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 14, optimizing at step 60 that is the total step 1200\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0727460384368896 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.063019037246704 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 14 with duration: 96, total reward: 0.4444444444444233 and real time duration of 5.6204283237457275 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 15\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 15, optimizing at step 14 that is the total step 1250\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0787122249603271 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.109192132949829 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 15 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.031989574432373 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 16\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 16, optimizing at step 4 that is the total step 1300\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0400149822235107 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.077186107635498 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 16, optimizing at step 54 that is the total step 1350\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0699183940887451 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0747191905975342 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 16 with duration: 60, total reward: -3.5555555555555634 and real time duration of 5.132050275802612 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 17\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 17, optimizing at step 44 that is the total step 1400\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.023613691329956 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0880305767059326 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 17 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.098789691925049 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 18\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 18, optimizing at step 34 that is the total step 1450\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0263285636901855 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.071422815322876 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 18 with duration: 68, total reward: -2.6666666666666776 and real time duration of 3.1146042346954346 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 19\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 19, optimizing at step 16 that is the total step 1500\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0507326126098633 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0882039070129395 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 19, optimizing at step 66 that is the total step 1550\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0465569496154785 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0544977188110352 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 19, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 19 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.651525259017944 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 20\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 20, optimizing at step 16 that is the total step 1600\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0896198749542236 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0798132419586182 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 20 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.151716947555542 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 21\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 21, optimizing at step 6 that is the total step 1650\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.053853988647461 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0704917907714844 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 21, optimizing at step 56 that is the total step 1700\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0612847805023193 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0737731456756592 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 21 with duration: 96, total reward: 0.4444444444444233 and real time duration of 5.738919258117676 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 22\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 22, optimizing at step 10 that is the total step 1750\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0911481380462646 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0620529651641846 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 22, optimizing at step 60 that is the total step 1800\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0428516864776611 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1066930294036865 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 22 with duration: 80, total reward: -1.3333333333333481 and real time duration of 5.593528509140015 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 23\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 23, optimizing at step 30 that is the total step 1850\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0487644672393799 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1118454933166504 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 23, optimizing at step 80 that is the total step 1900\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.052217721939087 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1107909679412842 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 23, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 23 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.912493944168091 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 24\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 24, optimizing at step 30 that is the total step 1950\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0323350429534912 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.099046230316162 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 24, optimizing at step 80 that is the total step 2000\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0485413074493408 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0837547779083252 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 24, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 24 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.827820777893066 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 25\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 25, optimizing at step 30 that is the total step 2050\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0761439800262451 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0490949153900146 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 25, optimizing at step 80 that is the total step 2100\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1297705173492432 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.095743179321289 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 25, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 25 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.731945514678955 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 26\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 26, optimizing at step 30 that is the total step 2150\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0660626888275146 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0840275287628174 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 26, optimizing at step 80 that is the total step 2200\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0155677795410156 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0846476554870605 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 26 with duration: 96, total reward: 0.4444444444444233 and real time duration of 5.805469989776611 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 27\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 27, optimizing at step 34 that is the total step 2250\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0417301654815674 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0887205600738525 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 27, optimizing at step 84 that is the total step 2300\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0298807621002197 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1698315143585205 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 27, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 27 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.865488290786743 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 28\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 28, optimizing at step 34 that is the total step 2350\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.032289981842041 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0424063205718994 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 28, optimizing at step 84 that is the total step 2400\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0649282932281494 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0801334381103516 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 28, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 28 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.728606700897217 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 29\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 29, optimizing at step 34 that is the total step 2450\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.048877477645874 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0947198867797852 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 29, optimizing at step 84 that is the total step 2500\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.116281270980835 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1522986888885498 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 29, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 29 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.959185361862183 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 30\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 30, optimizing at step 34 that is the total step 2550\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0499942302703857 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0948021411895752 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 30 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.093862533569336 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 31\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 31, optimizing at step 24 that is the total step 2600\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.038499355316162 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1081421375274658 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 31, optimizing at step 74 that is the total step 2650\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.051844835281372 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1050398349761963 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 31, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 31 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.903622627258301 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 32\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 32, optimizing at step 24 that is the total step 2700\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0351276397705078 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0496113300323486 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 32, optimizing at step 74 that is the total step 2750\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.019284725189209 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.120032787322998 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 32 with duration: 80, total reward: -1.3333333333333481 and real time duration of 5.390717506408691 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 33\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 33, optimizing at step 44 that is the total step 2800\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0715372562408447 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.073915719985962 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 33, optimizing at step 94 that is the total step 2850\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1279017925262451 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1217875480651855 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 33, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 33 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.9288365840911865 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 34\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 34, optimizing at step 44 that is the total step 2900\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0306782722473145 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0814363956451416 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 34, optimizing at step 94 that is the total step 2950\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0467231273651123 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0791397094726562 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 34, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 34 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.791597127914429 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 35\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 35, optimizing at step 44 that is the total step 3000\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0376896858215332 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0902526378631592 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 35 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.1059038639068604 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 36\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 36, optimizing at step 34 that is the total step 3050\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.079207181930542 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1067497730255127 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 36 with duration: 80, total reward: -1.3333333333333481 and real time duration of 3.4300997257232666 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 37\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 37, optimizing at step 4 that is the total step 3100\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0465779304504395 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0929539203643799 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 37, optimizing at step 54 that is the total step 3150\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0452394485473633 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0904242992401123 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 37 with duration: 80, total reward: -1.3333333333333481 and real time duration of 5.404310941696167 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 38\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 38, optimizing at step 24 that is the total step 3200\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0821194648742676 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.120072364807129 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 38 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.037337303161621 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 39\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 39, optimizing at step 14 that is the total step 3250\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0736503601074219 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0654523372650146 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 39, optimizing at step 64 that is the total step 3300\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0790002346038818 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0816800594329834 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 39 with duration: 96, total reward: 0.4444444444444233 and real time duration of 5.81199312210083 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 40\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 40, optimizing at step 18 that is the total step 3350\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0744948387145996 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0618014335632324 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 40, optimizing at step 68 that is the total step 3400\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1342706680297852 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1991510391235352 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 40 with duration: 80, total reward: -1.3333333333333481 and real time duration of 5.602912664413452 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 41\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 41, optimizing at step 38 that is the total step 3450\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1046411991119385 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1455540657043457 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 41 with duration: 70, total reward: -2.444444444444456 and real time duration of 3.26106858253479 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 42\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 42, optimizing at step 18 that is the total step 3500\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0932555198669434 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0906834602355957 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 42 with duration: 68, total reward: -2.6666666666666776 and real time duration of 3.1298985481262207 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 43\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 43, optimizing at step 0 that is the total step 3550\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0587232112884521 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0919568538665771 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 43, optimizing at step 50 that is the total step 3600\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.059065818786621 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0752308368682861 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 43 with duration: 70, total reward: -2.444444444444456 and real time duration of 5.25236439704895 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 44\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 44, optimizing at step 30 that is the total step 3650\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1141362190246582 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.203810691833496 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 44 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.0987985134124756 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 45\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 45, optimizing at step 20 that is the total step 3700\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0627174377441406 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.077437400817871 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 45 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.0249664783477783 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 46\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 46, optimizing at step 10 that is the total step 3750\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0996387004852295 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0683326721191406 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 46, optimizing at step 60 that is the total step 3800\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0992448329925537 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1202678680419922 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 46 with duration: 70, total reward: -2.444444444444456 and real time duration of 5.364190578460693 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 47\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 47, optimizing at step 40 that is the total step 3850\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.120945930480957 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1636950969696045 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 47 with duration: 70, total reward: -2.444444444444456 and real time duration of 3.279414415359497 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 48\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 48, optimizing at step 20 that is the total step 3900\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.13480806350708 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.224616527557373 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 48, optimizing at step 70 that is the total step 3950\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.080106258392334 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.08648681640625 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 48, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 48 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.922221422195435 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 49\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 49, optimizing at step 20 that is the total step 4000\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0489153861999512 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0887119770050049 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 49, optimizing at step 70 that is the total step 4050\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0315499305725098 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0790166854858398 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 49, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 49 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.85568642616272 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 50\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 50, optimizing at step 20 that is the total step 4100\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0898292064666748 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0889859199523926 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 50 with duration: 60, total reward: -3.5555555555555634 and real time duration of 2.9973912239074707 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 51\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 51, optimizing at step 10 that is the total step 4150\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.2067508697509766 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.2555263042449951 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 51, optimizing at step 60 that is the total step 4200\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1507060527801514 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1265637874603271 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 51 with duration: 68, total reward: -2.6666666666666776 and real time duration of 5.848071336746216 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 52\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 52, optimizing at step 42 that is the total step 4250\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1492912769317627 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.22607421875 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 52 with duration: 68, total reward: -2.6666666666666776 and real time duration of 3.3712961673736572 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 53\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 53, optimizing at step 24 that is the total step 4300\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1836445331573486 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1967594623565674 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 53, optimizing at step 74 that is the total step 4350\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.2225258350372314 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1687188148498535 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 53, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 53 with duration: 100, total reward: 10.999999999999977 and real time duration of 6.346704959869385 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 54\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 54, optimizing at step 24 that is the total step 4400\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.153113842010498 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1341307163238525 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 54, optimizing at step 74 that is the total step 4450\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1407184600830078 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.097794532775879 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 54 with duration: 80, total reward: -1.3333333333333481 and real time duration of 5.778301239013672 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 55\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 55, optimizing at step 44 that is the total step 4500\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0632667541503906 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1003146171569824 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 55 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.1646058559417725 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 56\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 56, optimizing at step 34 that is the total step 4550\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0740742683410645 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1088507175445557 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 56 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.1274070739746094 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 57\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 57, optimizing at step 24 that is the total step 4600\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.108550786972046 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1681773662567139 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 57, optimizing at step 74 that is the total step 4650\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.2632272243499756 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1185870170593262 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 57 with duration: 80, total reward: -1.3333333333333481 and real time duration of 5.846586465835571 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 58\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 58, optimizing at step 44 that is the total step 4700\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0306286811828613 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.079071044921875 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 58, optimizing at step 94 that is the total step 4750\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0755751132965088 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1254315376281738 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 58, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 58 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.979387521743774 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 59\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 59, optimizing at step 44 that is the total step 4800\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1519556045532227 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.242682933807373 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 59, optimizing at step 94 that is the total step 4850\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0775043964385986 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.2436838150024414 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 59 with duration: 96, total reward: 0.4444444444444233 and real time duration of 6.2574074268341064 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 60\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 60, optimizing at step 48 that is the total step 4900\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.050199031829834 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1243343353271484 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 60 with duration: 68, total reward: -2.6666666666666776 and real time duration of 3.2784340381622314 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 61\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 61, optimizing at step 30 that is the total step 4950\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1644346714019775 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.2261841297149658 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 61, optimizing at step 80 that is the total step 5000\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0777604579925537 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0867083072662354 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 61, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 61 with duration: 100, total reward: 10.999999999999977 and real time duration of 6.083379030227661 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 62\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 62, optimizing at step 30 that is the total step 5050\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0651562213897705 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0628340244293213 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 62, optimizing at step 80 that is the total step 5100\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0042574405670166 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0633890628814697 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 62 with duration: 96, total reward: 0.4444444444444233 and real time duration of 5.5601959228515625 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 63\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 63, optimizing at step 34 that is the total step 5150\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1122276782989502 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.075040340423584 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 63, optimizing at step 84 that is the total step 5200\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0075421333312988 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0499389171600342 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 63, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 63 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.83550500869751 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 64\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 64, optimizing at step 34 that is the total step 5250\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0441687107086182 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0776357650756836 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 64, optimizing at step 84 that is the total step 5300\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0855016708374023 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1096365451812744 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 64, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 64 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.720025539398193 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 65\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 65, optimizing at step 34 that is the total step 5350\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0541229248046875 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.158231496810913 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 65 with duration: 68, total reward: -2.6666666666666776 and real time duration of 3.300332546234131 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 66\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 66, optimizing at step 16 that is the total step 5400\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0376341342926025 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0417540073394775 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 66, optimizing at step 66 that is the total step 5450\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0418570041656494 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.115551471710205 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 66, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 66 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.745853662490845 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 67\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 67, optimizing at step 16 that is the total step 5500\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.2516906261444092 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1549549102783203 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 67, optimizing at step 66 that is the total step 5550\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1036109924316406 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.184678077697754 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 67 with duration: 80, total reward: -1.3333333333333481 and real time duration of 6.099100589752197 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 68\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 68, optimizing at step 36 that is the total step 5600\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1303620338439941 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.124807596206665 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 68, optimizing at step 86 that is the total step 5650\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0833735466003418 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1219215393066406 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 68, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 68 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.967778444290161 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 69\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 69, optimizing at step 36 that is the total step 5700\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0814225673675537 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0949668884277344 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 69 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.1620585918426514 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 70\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 70, optimizing at step 26 that is the total step 5750\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1123096942901611 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1148841381072998 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 70 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.103618860244751 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 71\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 71, optimizing at step 16 that is the total step 5800\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1048457622528076 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1185243129730225 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 71 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.08815336227417 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 72\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 72, optimizing at step 6 that is the total step 5850\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.160583734512329 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1829419136047363 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 72, optimizing at step 56 that is the total step 5900\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.060490369796753 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.2207162380218506 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 72 with duration: 60, total reward: -3.5555555555555634 and real time duration of 5.554561614990234 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 73\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 73, optimizing at step 46 that is the total step 5950\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.164952278137207 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1863946914672852 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 73 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.3354811668395996 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 74\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 74, optimizing at step 36 that is the total step 6000\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0949199199676514 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1289355754852295 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 74, optimizing at step 86 that is the total step 6050\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0783281326293945 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0977199077606201 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 74, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 74 with duration: 100, total reward: 10.999999999999977 and real time duration of 6.078830718994141 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 75\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 75, optimizing at step 36 that is the total step 6100\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1051356792449951 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0738413333892822 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 75 with duration: 80, total reward: -1.3333333333333481 and real time duration of 3.4000446796417236 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 76\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 76, optimizing at step 6 that is the total step 6150\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0743799209594727 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0895812511444092 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 76, optimizing at step 56 that is the total step 6200\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.3902561664581299 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.3643248081207275 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 76 with duration: 68, total reward: -2.6666666666666776 and real time duration of 6.103825807571411 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 77\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 77, optimizing at step 38 that is the total step 6250\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.138092041015625 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.2933425903320312 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 77, optimizing at step 88 that is the total step 6300\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1393566131591797 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1156816482543945 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 77, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 77 with duration: 100, total reward: 10.999999999999977 and real time duration of 6.311761379241943 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 78\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 78, optimizing at step 38 that is the total step 6350\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1007931232452393 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.104219675064087 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 78 with duration: 80, total reward: -1.3333333333333481 and real time duration of 3.3307511806488037 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 79\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 79, optimizing at step 8 that is the total step 6400\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.10506010055542 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1012251377105713 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 79, optimizing at step 58 that is the total step 6450\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0733366012573242 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0816099643707275 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 79 with duration: 80, total reward: -1.3333333333333481 and real time duration of 5.566241264343262 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 80\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 80, optimizing at step 28 that is the total step 6500\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0441277027130127 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0944585800170898 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 80, optimizing at step 78 that is the total step 6550\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0299687385559082 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.020836591720581 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 80, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 80 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.754902124404907 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 81\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 81, optimizing at step 28 that is the total step 6600\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0658743381500244 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0953173637390137 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 81, optimizing at step 78 that is the total step 6650\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0474557876586914 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0313336849212646 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 81, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 81 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.616793870925903 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 82\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 82, optimizing at step 28 that is the total step 6700\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1038141250610352 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.069765329360962 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 82, optimizing at step 78 that is the total step 6750\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0342521667480469 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.041074275970459 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 82, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 82 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.793984651565552 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 83\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 83, optimizing at step 28 that is the total step 6800\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1114654541015625 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.153200387954712 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 83, optimizing at step 78 that is the total step 6850\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0303499698638916 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0831108093261719 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 83, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 83 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.992900371551514 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 84\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 84, optimizing at step 28 that is the total step 6900\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0410220623016357 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1052050590515137 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 84, optimizing at step 78 that is the total step 6950\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0495262145996094 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0841026306152344 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 84, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 84 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.727478504180908 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 85\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 85, optimizing at step 28 that is the total step 7000\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0570926666259766 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0789263248443604 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 85, optimizing at step 78 that is the total step 7050\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0633797645568848 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1153843402862549 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 85 with duration: 80, total reward: -1.3333333333333481 and real time duration of 5.420754671096802 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 86\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 86, optimizing at step 48 that is the total step 7100\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0008955001831055 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0459566116333008 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 86 with duration: 60, total reward: -3.5555555555555634 and real time duration of 2.9656786918640137 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 87\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 87, optimizing at step 38 that is the total step 7150\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.058788537979126 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0930020809173584 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 87 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.088693141937256 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 88\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 88, optimizing at step 28 that is the total step 7200\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0556232929229736 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0791113376617432 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 88, optimizing at step 78 that is the total step 7250\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1019444465637207 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0715968608856201 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 88, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 88 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.692723751068115 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 89\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 89, optimizing at step 28 that is the total step 7300\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.042895793914795 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0664746761322021 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 89 with duration: 68, total reward: -2.6666666666666776 and real time duration of 3.0326855182647705 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 90\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 90, optimizing at step 10 that is the total step 7350\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1037449836730957 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.078221321105957 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 90, optimizing at step 60 that is the total step 7400\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.078040361404419 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0697827339172363 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 90 with duration: 96, total reward: 0.4444444444444233 and real time duration of 5.944164037704468 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 91\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 91, optimizing at step 14 that is the total step 7450\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0544517040252686 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0562899112701416 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 91 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.063305139541626 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 92\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 92, optimizing at step 4 that is the total step 7500\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.043820858001709 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1147358417510986 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 92, optimizing at step 54 that is the total step 7550\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0417232513427734 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.11362624168396 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 92 with duration: 80, total reward: -1.3333333333333481 and real time duration of 5.575764179229736 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 93\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 93, optimizing at step 24 that is the total step 7600\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0926566123962402 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.11690354347229 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 93 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.14023494720459 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 94\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 94, optimizing at step 14 that is the total step 7650\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0620150566101074 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0811333656311035 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 94, optimizing at step 64 that is the total step 7700\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0987293720245361 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.076899766921997 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 94 with duration: 70, total reward: -2.444444444444456 and real time duration of 5.346174240112305 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 95\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 95, optimizing at step 44 that is the total step 7750\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.059870719909668 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0540399551391602 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 95, optimizing at step 94 that is the total step 7800\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.1230497360229492 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0873603820800781 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 95, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 95 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.778712034225464 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 96\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 96, optimizing at step 44 that is the total step 7850\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0481688976287842 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0881924629211426 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 96 with duration: 60, total reward: -3.5555555555555634 and real time duration of 3.1714115142822266 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 97\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 97, optimizing at step 34 that is the total step 7900\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0385475158691406 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0824074745178223 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 97 with duration: 70, total reward: -2.444444444444456 and real time duration of 3.1629576683044434 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 98\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 98, optimizing at step 14 that is the total step 7950\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0650944709777832 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.104316234588623 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 98, optimizing at step 64 that is the total step 8000\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0492072105407715 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0768544673919678 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 98, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 98 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.825202465057373 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episode 99\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 99, optimizing at step 14 that is the total step 8050\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.085791826248169 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0755107402801514 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 99, optimizing at step 64 that is the total step 8100\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_1\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0633437633514404 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: Optimizing agent agent_2\n",
      "RLPipelineComponent: RLTrainerComponent: Optimization took 1.0673558712005615 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 99, reached step 100 that is beyond the current limit, 100\n",
      "RLPipelineComponent: RLTrainerComponent: Ended episode: 99 with duration: 100, total reward: 10.999999999999977 and real time duration of 5.720584392547607 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: \n",
      "Training took 480.8482663631439 seconds, 0.059101311008252694 per step (8136)\n"
     ]
    }
   ],
   "source": [
    "rl_pipeline.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"__type__\": \"<class 'automl.rl_components.rl_pipeline.RLPipelineComponent'>\",\n",
      "    \"name\": \"RLPipelineComponent\",\n",
      "    \"input\": {\n",
      "        \"num_episodes\": 100,\n",
      "        \"state_memory_size\": 1,\n",
      "        \"limit_steps\": 100,\n",
      "        \"optimization_interval\": 50,\n",
      "        \"environment\": {\n",
      "            \"__type__\": \"PettingZooEnvironmentLoader\",\n",
      "            \"name\": \"PettingZooEnvironmentLoader\",\n",
      "            \"localization\": [\n",
      "                0\n",
      "            ]\n",
      "        },\n",
      "        \"agents\": {\n",
      "            \"paddle_0\": {\n",
      "                \"__type__\": \"AgentSchema\",\n",
      "                \"name\": \"agent_1\",\n",
      "                \"localization\": [\n",
      "                    1\n",
      "                ]\n",
      "            },\n",
      "            \"paddle_1\": {\n",
      "                \"__type__\": \"AgentSchema\",\n",
      "                \"name\": \"agent_2\",\n",
      "                \"localization\": [\n",
      "                    2\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"save_interval\": 100,\n",
      "        \"rl_trainer\": \"\",\n",
      "        \"created_agents_input\": {}\n",
      "    },\n",
      "    \"child_components\": [\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.environment.environment_components.PettingZooEnvironmentLoader'>\",\n",
      "            \"name\": \"PettingZooEnvironmentLoader\",\n",
      "            \"input\": {\n",
      "                \"petting_zoo_environment\": \"cooperative_pong\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.agent_components.AgentSchema'>\",\n",
      "            \"name\": \"agent_1\",\n",
      "            \"input\": {\n",
      "                \"name\": \"agent_1\",\n",
      "                \"model_input_shape\": [\n",
      "                    3,\n",
      "                    480,\n",
      "                    280\n",
      "                ],\n",
      "                \"training_context\": {\n",
      "                    \"total_steps\": 8136\n",
      "                },\n",
      "                \"batch_size\": 64,\n",
      "                \"discount_factor\": 0.95,\n",
      "                \"exploration_strategy\": {\n",
      "                    \"__type__\": \"EpsilonGreedyStrategy\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"memory\": {\n",
      "                    \"__type__\": \"MemoryComponent\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        1\n",
      "                    ]\n",
      "                },\n",
      "                \"learner\": {\n",
      "                    \"__type__\": \"DeepQLearnerSchema\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        2\n",
      "                    ]\n",
      "                },\n",
      "                \"policy_model\": {\n",
      "                    \"__type__\": \"ConvModelSchema\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        3\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.exploration_strategy_components.EpsilonGreedyStrategy'>\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"input\": {\n",
      "                        \"training_context\": {\n",
      "                            \"total_steps\": 8136\n",
      "                        },\n",
      "                        \"epsilon_end\": 0.025,\n",
      "                        \"epsilon_start\": 1.0,\n",
      "                        \"epsilon_decay\": 0.01\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.memory_components.MemoryComponent'>\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"input\": {\n",
      "                        \"capacity\": 200\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.learner_component.DeepQLearnerSchema'>\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentSchema\",\n",
      "                            \"name\": \"agent_1\",\n",
      "                            \"localization\": [\n",
      "                                1\n",
      "                            ]\n",
      "                        },\n",
      "                        \"target_update_rate\": 0.05,\n",
      "                        \"update_target_at_optimization\": true,\n",
      "                        \"optimizer\": {\n",
      "                            \"__type__\": \"AdamOptimizer\",\n",
      "                            \"name\": \"AdamOptimizer\",\n",
      "                            \"localization\": []\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.model_components.ConvModelSchema'>\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"input\": {\n",
      "                        \"board_x\": 3,\n",
      "                        \"board_y\": 480,\n",
      "                        \"board_z\": 280,\n",
      "                        \"output_size\": 3\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.agent_components.AgentSchema'>\",\n",
      "            \"name\": \"agent_2\",\n",
      "            \"input\": {\n",
      "                \"name\": \"agent_2\",\n",
      "                \"model_input_shape\": [\n",
      "                    3,\n",
      "                    480,\n",
      "                    280\n",
      "                ],\n",
      "                \"training_context\": {\n",
      "                    \"total_steps\": 8136\n",
      "                },\n",
      "                \"batch_size\": 64,\n",
      "                \"discount_factor\": 0.95,\n",
      "                \"exploration_strategy\": {\n",
      "                    \"__type__\": \"EpsilonGreedyStrategy\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"memory\": {\n",
      "                    \"__type__\": \"MemoryComponent\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        1\n",
      "                    ]\n",
      "                },\n",
      "                \"learner\": {\n",
      "                    \"__type__\": \"DeepQLearnerSchema\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        2\n",
      "                    ]\n",
      "                },\n",
      "                \"policy_model\": {\n",
      "                    \"__type__\": \"ConvModelSchema\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        3\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.exploration_strategy_components.EpsilonGreedyStrategy'>\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"input\": {\n",
      "                        \"training_context\": {\n",
      "                            \"total_steps\": 8136\n",
      "                        },\n",
      "                        \"epsilon_end\": 0.025,\n",
      "                        \"epsilon_start\": 1.0,\n",
      "                        \"epsilon_decay\": 0.01\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.memory_components.MemoryComponent'>\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"input\": {\n",
      "                        \"capacity\": 200\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.learner_component.DeepQLearnerSchema'>\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentSchema\",\n",
      "                            \"name\": \"agent_2\",\n",
      "                            \"localization\": [\n",
      "                                2\n",
      "                            ]\n",
      "                        },\n",
      "                        \"target_update_rate\": 0.05,\n",
      "                        \"update_target_at_optimization\": true,\n",
      "                        \"optimizer\": {\n",
      "                            \"__type__\": \"AdamOptimizer\",\n",
      "                            \"name\": \"AdamOptimizer\",\n",
      "                            \"localization\": []\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl_components.model_components.ConvModelSchema'>\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"input\": {\n",
      "                        \"board_x\": 3,\n",
      "                        \"board_y\": 480,\n",
      "                        \"board_z\": 280,\n",
      "                        \"output_size\": 3\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl_components.rl_trainer_component.RLTrainerComponent'>\",\n",
      "            \"name\": \"RLTrainerComponent\",\n",
      "            \"input\": {\n",
      "                \"num_episodes\": 100,\n",
      "                \"state_memory_size\": 1,\n",
      "                \"environment\": {\n",
      "                    \"__type__\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"name\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"localization\": [\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"limit_steps\": 100,\n",
      "                \"optimization_interval\": 50,\n",
      "                \"agents\": {\n",
      "                    \"paddle_0\": {\n",
      "                        \"__type__\": \"AgentSchema\",\n",
      "                        \"name\": \"agent_1\",\n",
      "                        \"localization\": [\n",
      "                            1\n",
      "                        ]\n",
      "                    },\n",
      "                    \"paddle_1\": {\n",
      "                        \"__type__\": \"AgentSchema\",\n",
      "                        \"name\": \"agent_2\",\n",
      "                        \"localization\": [\n",
      "                            2\n",
      "                        ]\n",
      "                    }\n",
      "                },\n",
      "                \"save_interval\": 100\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rl_pipeline.save_configuration(toPrint=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
