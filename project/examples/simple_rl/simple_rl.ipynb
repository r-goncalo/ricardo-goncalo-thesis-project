{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\")) #make the folder \"automl\" part of this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5\n",
    "state_memory_size = 2\n",
    "limit_steps = 100\n",
    "optimization_interval = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Trainer Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from automl.rl.rl_pipeline import RLPipelineComponent\n",
    "\n",
    "\n",
    "rl_pipeline_input = {\n",
    "    \"device\" : \"gpu\",\n",
    "    \"num_episodes\" : num_episodes,\n",
    "    \"state_memory_size\" : state_memory_size,\n",
    "    \"limit_steps\" : limit_steps ,\n",
    "    \"optimization_interval\" : optimization_interval\n",
    "}\n",
    "\n",
    "\n",
    "rl_pipeline = RLPipelineComponent(input=rl_pipeline_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening a log... Log Dir: data\\logs Log Name:\n",
      "Log directory did not exist, creating it at: data\\logs\\log_9\n",
      "Type of object with name: <class 'automl.rl.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: There was an error trying to setup the device in 'gpu': Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu\n",
      "RLPipelineComponent: The model will trained and evaluated on: cpu\n",
      "RLPipelineComponent: Creating agents\n",
      "Opening a log... Log Dir: data\\logs\\log_9 Log Name:agent_1\n",
      "Log directory did not exist, creating it at: data\\logs\\log_9\\agent_1\n",
      "RLPipelineComponent: State for agent agent_1 has shape: Z: 280 Y: 480 X: 3\n",
      "Action space of agent paddle_0: Discrete(3)\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "Opening a log... Log Dir: data\\logs\\log_9 Log Name:agent_2\n",
      "Log directory did not exist, creating it at: data\\logs\\log_9\\agent_2\n",
      "RLPipelineComponent: State for agent agent_2 has shape: Z: 280 Y: 480 X: 3\n",
      "Action space of agent paddle_1: Discrete(3)\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized {'paddle_0': <automl.rl.agent.agent_components.AgentSchema object at 0x000002A50BB36EB0>, 'paddle_1': <automl.rl.agent.agent_components.AgentSchema object at 0x000002A50BB36F70>} agents\n",
      "RLPipelineComponent: Initializing trainer\n",
      "WARNING: input with key state_memory_size passed to component RLTrainerComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLPipelineComponent: RLTrainerComponent: Agent paddle_0 came without a trainer, creating one...\n",
      "RLPipelineComponent: RLTrainerComponent: Agent paddle_1 came without a trainer, creating one...\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episodes of training\n",
      "Type of object with name: <class 'automl.rl.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "Type of object with name: <class 'automl.rl.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "AgentTrainer: Setting up episode1\n",
      "Initializing agent with more than one state memory size (2)\n",
      "State length is 280\n",
      "Creating policy model using default values and passed shape... Model input: {'board_x': 3, 'board_y': 480, 'board_z': 560, 'output_size': 3}\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing policy model...\n",
      "state memory reseted 560\n",
      "AgentTrainer: Setting up episode1\n",
      "Initializing agent with more than one state memory size (2)\n",
      "State length is 280\n",
      "Creating policy model using default values and passed shape... Model input: {'board_x': 3, 'board_y': 480, 'board_z': 560, 'output_size': 3}\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing policy model...\n",
      "state memory reseted 560\n",
      "280\n",
      "3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "On Component RLPipelineComponent of type RLPipelineComponent:\n On Component agent_1 of type AgentSchema:\n stack expects each tensor to be equal size, but got [280, 3] at entry 0 and [480, 3] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrl_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:315\u001b[0m, in \u001b[0;36muses_component_exception.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m original_args \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;66;03m#arguments of an error\u001b[39;00m\n\u001b[0;32m    314\u001b[0m e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOn Component \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39moriginal_args[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:310\u001b[0m, in \u001b[0;36muses_component_exception.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m : Schema, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    313\u001b[0m         original_args \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;66;03m#arguments of an error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:301\u001b[0m, in \u001b[0;36mrequires_input_proccess.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_was_proccessed:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproccess_input()\n\u001b[1;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\rl_pipeline.py:170\u001b[0m, in \u001b[0;36mRLPipelineComponent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;129m@uses_component_exception\u001b[39m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;129m@requires_input_proccess\u001b[39m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):        \n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrl_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:301\u001b[0m, in \u001b[0;36mrequires_input_proccess.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_was_proccessed:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproccess_input()\n\u001b[1;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\rl_trainer_component.py:85\u001b[0m, in \u001b[0;36mRLTrainerComponent.run_episodes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m#each episode is an instance of playing the game\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_episodes):\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__run_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_episode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\rl_trainer_component.py:102\u001b[0m, in \u001b[0;36mRLTrainerComponent.__run_episode\u001b[1;34m(self, i_episode)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magent_iter(): \u001b[38;5;66;03m#iterates infinitely over the agents that should be acting in the environment\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     agent_in_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents_in_training[agent_name] \u001b[38;5;66;03m#gets the agent trainer\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[43magent_in_training\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other_agent_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents_in_training\u001b[38;5;241m.\u001b[39mkeys(): \u001b[38;5;66;03m#make the other agents observe the transiction\u001b[39;00m\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m other_agent_name \u001b[38;5;241m!=\u001b[39m agent_name:\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:301\u001b[0m, in \u001b[0;36mrequires_input_proccess.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_was_proccessed:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproccess_input()\n\u001b[1;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\agent_trainer_component.py:100\u001b[0m, in \u001b[0;36mAgentTrainer.do_training_step\u001b[1;34m(self, i_episode, env)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;129m@requires_input_proccess\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_training_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, i_episode, env : EnvironmentComponent):\n\u001b[0;32m     98\u001b[0m         observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobserve(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m--> 100\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# decides the next action to take (can be random)\u001b[39;00m\n\u001b[0;32m    102\u001b[0m         env\u001b[38;5;241m.\u001b[39mstep(action) \u001b[38;5;66;03m#makes the game proccess the action that was taken\u001b[39;00m\n\u001b[0;32m    104\u001b[0m         observation, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mlast()\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:301\u001b[0m, in \u001b[0;36mrequires_input_proccess.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_was_proccessed:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproccess_input()\n\u001b[1;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:315\u001b[0m, in \u001b[0;36muses_component_exception.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m original_args \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;66;03m#arguments of an error\u001b[39;00m\n\u001b[0;32m    314\u001b[0m e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOn Component \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39moriginal_args[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:310\u001b[0m, in \u001b[0;36muses_component_exception.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m : Schema, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    313\u001b[0m         original_args \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;66;03m#arguments of an error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\agent\\agent_components.py:171\u001b[0m, in \u001b[0;36mAgentSchema.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;129m@requires_input_proccess\u001b[39m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;129m@uses_component_exception\u001b[39m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m#selects action using policy prediction\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_strategy\u001b[38;5;241m.\u001b[39mselect_action(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_memory )\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\component.py:301\u001b[0m, in \u001b[0;36mrequires_input_proccess.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_was_proccessed:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproccess_input()\n\u001b[1;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\rl\\agent\\agent_components.py:237\u001b[0m, in \u001b[0;36mAgentSchema.update_state_memory\u001b[1;34m(self, new_state)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(prev_state_memory))\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(prev_state_memory[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m--> 237\u001b[0m     new_state_memory \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_state_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_memory \u001b[38;5;241m=\u001b[39m new_state_memory\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: On Component RLPipelineComponent of type RLPipelineComponent:\n On Component agent_1 of type AgentSchema:\n stack expects each tensor to be equal size, but got [280, 3] at entry 0 and [480, 3] at entry 1"
     ]
    }
   ],
   "source": [
    "rl_pipeline.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"__type__\": \"<class 'automl.rl.rl_pipeline.RLPipelineComponent'>\",\n",
      "    \"name\": \"RLPipelineComponent\",\n",
      "    \"input\": {\n",
      "        \"num_episodes\": 5,\n",
      "        \"state_memory_size\": 1,\n",
      "        \"limit_steps\": 100,\n",
      "        \"optimization_interval\": 50,\n",
      "        \"environment\": {\n",
      "            \"__type__\": \"PettingZooEnvironmentLoader\",\n",
      "            \"name\": \"PettingZooEnvironmentLoader\",\n",
      "            \"localization\": [\n",
      "                0\n",
      "            ]\n",
      "        },\n",
      "        \"agents\": {\n",
      "            \"paddle_0\": {\n",
      "                \"__type__\": \"AgentTrainer\",\n",
      "                \"name\": \"AgentTrainer\",\n",
      "                \"localization\": [\n",
      "                    3,\n",
      "                    0\n",
      "                ]\n",
      "            },\n",
      "            \"paddle_1\": {\n",
      "                \"__type__\": \"AgentTrainer\",\n",
      "                \"name\": \"AgentTrainer\",\n",
      "                \"localization\": [\n",
      "                    3,\n",
      "                    1\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"save_interval\": 100,\n",
      "        \"rl_trainer\": \"\",\n",
      "        \"created_agents_input\": {}\n",
      "    },\n",
      "    \"child_components\": [\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl.environment.environment_components.PettingZooEnvironmentLoader'>\",\n",
      "            \"name\": \"PettingZooEnvironmentLoader\",\n",
      "            \"input\": {\n",
      "                \"petting_zoo_environment\": \"cooperative_pong\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl.agent.agent_components.AgentSchema'>\",\n",
      "            \"name\": \"agent_1\",\n",
      "            \"input\": {\n",
      "                \"name\": \"agent_1\",\n",
      "                \"model_input_shape\": [\n",
      "                    3,\n",
      "                    480,\n",
      "                    280\n",
      "                ],\n",
      "                \"training_context\": {\n",
      "                    \"total_steps\": 0\n",
      "                },\n",
      "                \"batch_size\": 64,\n",
      "                \"discount_factor\": 0.95,\n",
      "                \"exploration_strategy\": {\n",
      "                    \"__type__\": \"EpsilonGreedyStrategy\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"memory\": {\n",
      "                    \"__type__\": \"MemoryComponent\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        1\n",
      "                    ]\n",
      "                },\n",
      "                \"learner\": {\n",
      "                    \"__type__\": \"DeepQLearnerSchema\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        2\n",
      "                    ]\n",
      "                },\n",
      "                \"state_memory_size\": 1,\n",
      "                \"policy_model\": {\n",
      "                    \"__type__\": \"ConvModelSchema\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        3\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.exploration.epsilong_greedy.EpsilonGreedyStrategy'>\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"input\": {\n",
      "                        \"training_context\": {\n",
      "                            \"total_steps\": 0\n",
      "                        },\n",
      "                        \"epsilon_end\": 0.025,\n",
      "                        \"epsilon_start\": 1.0,\n",
      "                        \"epsilon_decay\": 0.01\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.memory_components.MemoryComponent'>\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"input\": {\n",
      "                        \"capacity\": 200\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.learners.learner_component.DeepQLearnerSchema'>\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentSchema\",\n",
      "                            \"name\": \"agent_1\",\n",
      "                            \"localization\": [\n",
      "                                1\n",
      "                            ]\n",
      "                        },\n",
      "                        \"target_update_rate\": 0.05,\n",
      "                        \"update_target_at_optimization\": true,\n",
      "                        \"optimizer\": {\n",
      "                            \"__type__\": \"AdamOptimizer\",\n",
      "                            \"name\": \"AdamOptimizer\",\n",
      "                            \"localization\": []\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.ml.models.model_components.ConvModelSchema'>\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"input\": {\n",
      "                        \"board_x\": 3,\n",
      "                        \"board_y\": 480,\n",
      "                        \"board_z\": 280,\n",
      "                        \"output_size\": 3\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl.agent.agent_components.AgentSchema'>\",\n",
      "            \"name\": \"agent_2\",\n",
      "            \"input\": {\n",
      "                \"name\": \"agent_2\",\n",
      "                \"model_input_shape\": [\n",
      "                    3,\n",
      "                    480,\n",
      "                    280\n",
      "                ],\n",
      "                \"training_context\": {\n",
      "                    \"total_steps\": 0\n",
      "                },\n",
      "                \"batch_size\": 64,\n",
      "                \"discount_factor\": 0.95,\n",
      "                \"exploration_strategy\": {\n",
      "                    \"__type__\": \"EpsilonGreedyStrategy\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"memory\": {\n",
      "                    \"__type__\": \"MemoryComponent\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        1\n",
      "                    ]\n",
      "                },\n",
      "                \"learner\": {\n",
      "                    \"__type__\": \"DeepQLearnerSchema\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        2\n",
      "                    ]\n",
      "                },\n",
      "                \"state_memory_size\": 1,\n",
      "                \"policy_model\": {\n",
      "                    \"__type__\": \"ConvModelSchema\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        3\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.exploration.epsilong_greedy.EpsilonGreedyStrategy'>\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"input\": {\n",
      "                        \"training_context\": {\n",
      "                            \"total_steps\": 0\n",
      "                        },\n",
      "                        \"epsilon_end\": 0.025,\n",
      "                        \"epsilon_start\": 1.0,\n",
      "                        \"epsilon_decay\": 0.01\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.memory_components.MemoryComponent'>\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"input\": {\n",
      "                        \"capacity\": 200\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.learners.learner_component.DeepQLearnerSchema'>\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentSchema\",\n",
      "                            \"name\": \"agent_2\",\n",
      "                            \"localization\": [\n",
      "                                2\n",
      "                            ]\n",
      "                        },\n",
      "                        \"target_update_rate\": 0.05,\n",
      "                        \"update_target_at_optimization\": true,\n",
      "                        \"optimizer\": {\n",
      "                            \"__type__\": \"AdamOptimizer\",\n",
      "                            \"name\": \"AdamOptimizer\",\n",
      "                            \"localization\": []\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.ml.models.model_components.ConvModelSchema'>\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"input\": {\n",
      "                        \"board_x\": 3,\n",
      "                        \"board_y\": 480,\n",
      "                        \"board_z\": 280,\n",
      "                        \"output_size\": 3\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl.rl_trainer_component.RLTrainerComponent'>\",\n",
      "            \"name\": \"RLTrainerComponent\",\n",
      "            \"input\": {\n",
      "                \"num_episodes\": 5,\n",
      "                \"environment\": {\n",
      "                    \"__type__\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"name\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"localization\": [\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"limit_steps\": 100,\n",
      "                \"optimization_interval\": 50,\n",
      "                \"agents\": {\n",
      "                    \"paddle_0\": {\n",
      "                        \"__type__\": \"AgentTrainer\",\n",
      "                        \"name\": \"AgentTrainer\",\n",
      "                        \"localization\": [\n",
      "                            3,\n",
      "                            0\n",
      "                        ]\n",
      "                    },\n",
      "                    \"paddle_1\": {\n",
      "                        \"__type__\": \"AgentTrainer\",\n",
      "                        \"name\": \"AgentTrainer\",\n",
      "                        \"localization\": [\n",
      "                            3,\n",
      "                            1\n",
      "                        ]\n",
      "                    }\n",
      "                },\n",
      "                \"save_interval\": 100\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.agent_trainer_component.AgentTrainer'>\",\n",
      "                    \"name\": \"AgentTrainer\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentSchema\",\n",
      "                            \"name\": \"agent_1\",\n",
      "                            \"localization\": [\n",
      "                                1\n",
      "                            ]\n",
      "                        },\n",
      "                        \"optimization_interval\": 50,\n",
      "                        \"save_interval\": 100\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.agent_trainer_component.AgentTrainer'>\",\n",
      "                    \"name\": \"AgentTrainer\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentSchema\",\n",
      "                            \"name\": \"agent_2\",\n",
      "                            \"localization\": [\n",
      "                                2\n",
      "                            ]\n",
      "                        },\n",
      "                        \"optimization_interval\": 50,\n",
      "                        \"save_interval\": 100\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rl_pipeline.save_configuration(toPrint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 6]]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [1, 2],\n",
      "        [3, 4],\n",
      "        [1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 6],\n",
      "        [5, 6],\n",
      "        [7, 6],\n",
      "        [5, 6],\n",
      "        [7, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "matrix = torch.tensor([ [[1, 2], [3, 4]], [[5, 6], [7, 6]]])\n",
    "\n",
    "def print_matrix(m):\n",
    "    \n",
    "    for z in range(0, len(m)):\n",
    "        for y in range(0, len(m[z])):\n",
    "            print(f\"{m[y]} \", end='')\n",
    "            \n",
    "        print()\n",
    "\n",
    "print(matrix)\n",
    "\n",
    "new_matrix = torch.cat([  matrix for _ in range(0, 3)])\n",
    "\n",
    "print(new_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
