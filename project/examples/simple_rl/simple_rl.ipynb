{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\")) #make the folder \"automl\" part of this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from automl.rl_components.rl_trainer_component import RLTrainerComponent\n",
    "from automl.rl_components.agent_components import AgentComponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Opening a log... Log Dir: data\\logs Log Name:\n",
      "Log directory did not exist, creating it at: data\\logs\\log_2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../../..\")) #make the folder \"project\" part of this\n",
    "\n",
    "from project.logger import Log\n",
    "\n",
    "lg = Log.openLog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import cooperative_pong_v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_translator(state, device):\n",
    "    return torch.from_numpy(state).to(torch.float32).to(device)\n",
    "\n",
    "class Env(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.env = cooperative_pong_v5.env(render_mode=\"none\")\n",
    "        self.env.reset()\n",
    "        \n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "        \n",
    "    def __str__(self):\n",
    "        \n",
    "        return \"Petting zoo cooperative pong v5\"\n",
    "        \n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "        \n",
    "    def observe(self, *args):\n",
    "        return state_translator(self.env.observe(*args), self.device)\n",
    "        \n",
    "    def agents(self):\n",
    "        return self.env.agents\n",
    "    \n",
    "    def action_space(self, *args):\n",
    "        return self.env.action_space(*args)\n",
    "    \n",
    "    def last(self):\n",
    "        \n",
    "        observation, reward, termination, truncation, info = self.env.last()\n",
    "        \n",
    "        #returns state, reward, done, info\n",
    "        return state_translator(observation, self.device), reward, termination, info\n",
    "    \n",
    "    def agent_iter(self):\n",
    "        \n",
    "        return self.env.agent_iter()\n",
    "    \n",
    "    def step(self, *args):\n",
    "        \n",
    "        return self.env.step(*args)\n",
    "    \n",
    "    def rewards(self):\n",
    "        return self.env.rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "state_memory_size = 1\n",
    "limit_steps = 100\n",
    "optimization_interval = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Trainer Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from automl.rl_components.rl_pipeline import RLPipelineComponent\n",
    "\n",
    "env = Env()\n",
    "\n",
    "rl_pipeline_input = {\n",
    "    \"device\" : \"gpu\",\n",
    "    \"logger\" : lg,\n",
    "    \"num_episodes\" : num_episodes,\n",
    "    \"state_memory_size\" : state_memory_size,\n",
    "    \"environment\" : env,\n",
    "    \"limit_steps\" : limit_steps ,\n",
    "    \"optimization_interval\" : optimization_interval\n",
    "}\n",
    "\n",
    "\n",
    "rl_pipeline = RLPipelineComponent(input=rl_pipeline_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: There was an error trying to setup the device in 'gpu': Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu\n",
      "RLPipelineComponent: The model will trained and evaluated on: cpu\n",
      "RLPipelineComponent: Creating agents\n",
      "Opening a log... Log Dir: data\\logs\\log_2 Log Name:agent_1\n",
      "Log directory did not exist, creating it at: data\\logs\\log_2\\agent_1\n",
      "RLPipelineComponent: State for agent agent_1 has shape: Z: 280 Y: 480 X: 3\n",
      "Action space of agent paddle_0: Discrete(3)\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "Opening a log... Log Dir: data\\logs\\log_2 Log Name:agent_2\n",
      "Log directory did not exist, creating it at: data\\logs\\log_2\\agent_2\n",
      "RLPipelineComponent: State for agent agent_2 has shape: Z: 280 Y: 480 X: 3\n",
      "Action space of agent paddle_1: Discrete(3)\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized {'paddle_0': <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>, 'paddle_1': <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>} agents\n",
      "RLPipelineComponent: Initializing trainer\n",
      "Starting to run episodes of training\n",
      "Starting to run episode 0\n",
      "agent_1: Batch size: 64 Gamma: 0.95 Tau: 0.05\n",
      "agent_1: Initializing policy model...\n",
      "agent_1: Creating policy model using default values and passed shape...\n",
      "agent_1: Initializing target model...\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': device(type='cpu')}\n",
      "Cloning model\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': device(type='cpu')}\n",
      "agent_1: Instantiating an empty memory with size 200\n",
      "In episode 0, optimizing at step 0 that is the total step 0\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Optimization took 0.19292807579040527 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "agent_2: Batch size: 64 Gamma: 0.95 Tau: 0.05\n",
      "agent_2: Initializing policy model...\n",
      "agent_2: Creating policy model using default values and passed shape...\n",
      "agent_2: Initializing target model...\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': device(type='cpu')}\n",
      "Cloning model\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 280, 'output_size': 3, 'device': device(type='cpu')}\n",
      "agent_2: Instantiating an empty memory with size 200\n",
      "Optimization took 0.5629405975341797 seconds\n",
      "In episode 0, optimizing at step 50 that is the total step 50\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Optimization took 0.15811920166015625 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Optimization took 0.1680908203125 seconds\n",
      "Ended episode: 0 with duration: 80, total reward: 0 and real time duration of 2.84182071685791 seconds\n",
      "Starting to run episode 1\n",
      "In episode 1, optimizing at step 20 that is the total step 100\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Optimization took 0.12569618225097656 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Optimization took 0.1683495044708252 seconds\n",
      "Ended episode: 1 with duration: 68, total reward: 0 and real time duration of 1.254422903060913 seconds\n",
      "Starting to run episode 2\n",
      "In episode 2, optimizing at step 2 that is the total step 150\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D163728B50>\n",
      "Optimization took 3.903014659881592 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D165043D60>\n",
      "Optimization took 1.1931214332580566 seconds\n",
      "In episode 2, optimizing at step 52 that is the total step 200\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D16372F190>\n",
      "Optimization took 1.0000715255737305 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D16372F790>\n",
      "Optimization took 0.9104225635528564 seconds\n",
      "In episode 2, reached step 100 that is beyond the current limit, 100\n",
      "Ended episode: 2 with duration: 100, total reward: 0 and real time duration of 8.31331753730774 seconds\n",
      "Starting to run episode 3\n",
      "In episode 3, optimizing at step 2 that is the total step 250\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D163727F70>\n",
      "Optimization took 0.9533514976501465 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D168A27490>\n",
      "Optimization took 0.9631814956665039 seconds\n",
      "In episode 3, optimizing at step 52 that is the total step 300\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D165043D60>\n",
      "Optimization took 0.9996871948242188 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D165043D60>\n",
      "Optimization took 0.9586365222930908 seconds\n",
      "Ended episode: 3 with duration: 60, total reward: 0 and real time duration of 4.61432957649231 seconds\n",
      "Starting to run episode 4\n",
      "In episode 4, optimizing at step 42 that is the total step 350\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D163728AF0>\n",
      "Optimization took 0.9496979713439941 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D163728AF0>\n",
      "Optimization took 0.9467360973358154 seconds\n",
      "Ended episode: 4 with duration: 80, total reward: 0 and real time duration of 2.917224168777466 seconds\n",
      "Starting to run episode 5\n",
      "In episode 5, optimizing at step 12 that is the total step 400\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D168A27340>\n",
      "Optimization took 0.9505414962768555 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D168A271F0>\n",
      "Optimization took 0.9287388324737549 seconds\n",
      "In episode 5, optimizing at step 62 that is the total step 450\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D168A271F0>\n",
      "Optimization took 0.949413537979126 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D168A271F0>\n",
      "Optimization took 0.9277420043945312 seconds\n",
      "In episode 5, reached step 100 that is beyond the current limit, 100\n",
      "Ended episode: 5 with duration: 100, total reward: 0 and real time duration of 5.070969343185425 seconds\n",
      "Starting to run episode 6\n",
      "In episode 6, optimizing at step 12 that is the total step 500\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D168A27940>\n",
      "Optimization took 0.9629666805267334 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D168A27940>\n",
      "Optimization took 0.9305284023284912 seconds\n",
      "In episode 6, optimizing at step 62 that is the total step 550\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D16372F790>\n",
      "Optimization took 1.002415657043457 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D163727EE0>\n",
      "Optimization took 0.938636302947998 seconds\n",
      "In episode 6, reached step 100 that is beyond the current limit, 100\n",
      "Ended episode: 6 with duration: 100, total reward: 0 and real time duration of 5.037240505218506 seconds\n",
      "Starting to run episode 7\n",
      "In episode 7, optimizing at step 12 that is the total step 600\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D16372FEE0>\n",
      "Optimization took 1.0562763214111328 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D16372FEE0>\n",
      "Optimization took 0.9708116054534912 seconds\n",
      "In episode 7, optimizing at step 62 that is the total step 650\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D16372F790>\n",
      "Optimization took 0.9735965728759766 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D163728AF0>\n",
      "Optimization took 0.985222339630127 seconds\n",
      "In episode 7, reached step 100 that is beyond the current limit, 100\n",
      "Ended episode: 7 with duration: 100, total reward: 0 and real time duration of 5.337242126464844 seconds\n",
      "Starting to run episode 8\n",
      "In episode 8, optimizing at step 12 that is the total step 700\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D165043D60>\n",
      "Optimization took 0.9606490135192871 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D16372FF10>\n",
      "Optimization took 0.9687855243682861 seconds\n",
      "In episode 8, optimizing at step 62 that is the total step 750\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D16372F760>\n",
      "Optimization took 0.9408829212188721 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D16372F790>\n",
      "Optimization took 1.047044277191162 seconds\n",
      "In episode 8, reached step 100 that is beyond the current limit, 100\n",
      "Ended episode: 8 with duration: 100, total reward: 0 and real time duration of 5.341794013977051 seconds\n",
      "Starting to run episode 9\n",
      "In episode 9, optimizing at step 12 that is the total step 800\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F640>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D16372F760>\n",
      "Optimization took 1.0062546730041504 seconds\n",
      "Optimizing agent <automl.rl_components.agent_components.AgentComponent object at 0x000001D16372F6A0>\n",
      "Gradients of predicted: <MaxBackward0 object at 0x000001D168A27220>\n",
      "Optimization took 1.0116767883300781 seconds\n",
      "Ended episode: 9 with duration: 60, total reward: 0 and real time duration of 2.7769713401794434 seconds\n",
      "\n",
      "Training took 43.520753383636475 seconds, 0.05132164314108075 per step (848)\n"
     ]
    }
   ],
   "source": [
    "rl_pipeline.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
