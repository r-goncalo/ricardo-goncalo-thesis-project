{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\")) #make the folder \"automl\" part of this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5\n",
    "state_memory_size = 1\n",
    "limit_steps = 100\n",
    "optimization_interval = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Trainer Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from automl.rl.rl_pipeline import RLPipelineComponent\n",
    "\n",
    "\n",
    "rl_pipeline_input = {\n",
    "    \"device\" : \"gpu\",\n",
    "    \"num_episodes\" : num_episodes,\n",
    "    \"state_memory_size\" : 2,\n",
    "    \"limit_steps\" : limit_steps ,\n",
    "    \"optimization_interval\" : optimization_interval\n",
    "}\n",
    "\n",
    "\n",
    "rl_pipeline = RLPipelineComponent(input=rl_pipeline_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening a log... Log Dir: data\\logs Log Name:\n",
      "Log directory did not exist, creating it at: data\\logs\\log_4\n",
      "Type of object with name: <class 'automl.rl.rl_pipeline.RLPipelineComponent'> and name passed: \n",
      "RLPipelineComponent: Trying to use cuda...\n",
      "RLPipelineComponent: There was an error trying to setup the device in 'gpu': Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu\n",
      "RLPipelineComponent: The model will trained and evaluated on: cpu\n",
      "RLPipelineComponent: Creating agents\n",
      "Opening a log... Log Dir: data\\logs\\log_4 Log Name:agent_1\n",
      "Log directory did not exist, creating it at: data\\logs\\log_4\\agent_1\n",
      "RLPipelineComponent: State for agent agent_1 has shape: Z: 280 Y: 480 X: 3\n",
      "Action space of agent paddle_0: Discrete(3)\n",
      "RLPipelineComponent: Created agent in training agent_1\n",
      "Opening a log... Log Dir: data\\logs\\log_4 Log Name:agent_2\n",
      "Log directory did not exist, creating it at: data\\logs\\log_4\\agent_2\n",
      "RLPipelineComponent: State for agent agent_2 has shape: Z: 280 Y: 480 X: 3\n",
      "Action space of agent paddle_1: Discrete(3)\n",
      "RLPipelineComponent: Created agent in training agent_2\n",
      "RLPipelineComponent: Initialized {'paddle_0': <automl.rl.agent.agent_components.AgentSchema object at 0x00000272211EB400>, 'paddle_1': <automl.rl.agent.agent_components.AgentSchema object at 0x00000272211EB4C0>} agents\n",
      "RLPipelineComponent: Initializing trainer\n",
      "WARNING: input with key state_memory_size passed to component RLTrainerComponent but not in its input signature, will be ignored\n",
      "Type of object with name: <class 'automl.rl.trainers.rl_trainer_component.RLTrainerComponent'> and name passed: \n",
      "RLPipelineComponent: RLTrainerComponent: Agent paddle_0 came without a trainer, creating one...\n",
      "RLPipelineComponent: RLTrainerComponent: Agent paddle_1 came without a trainer, creating one...\n",
      "RLPipelineComponent: RLTrainerComponent: Starting to run episodes of training\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "Type of object with name: <class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'> and name passed: \n",
      "AgentTrainer: Setting up training session\n",
      "AgentTrainer: Setting up episode1\n",
      "Initializing agent with more than one state memory size (2)\n",
      "State length is 280\n",
      "Creating policy model using default values and passed shape... Model input: {'board_x': 3, 'board_y': 480, 'board_z': 560, 'output_size': 3}\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "agent_1: Batch size: 64 Gamma: 0.95\n",
      "agent_1: Initializing policy model...\n",
      "AgentTrainer: Setting up episode1\n",
      "Initializing agent with more than one state memory size (2)\n",
      "State length is 280\n",
      "Creating policy model using default values and passed shape... Model input: {'board_x': 3, 'board_y': 480, 'board_z': 560, 'output_size': 3}\n",
      "Type of object with name: <class 'automl.rl.agent.agent_components.AgentSchema'> and name passed: \n",
      "agent_2: Batch size: 64 Gamma: 0.95\n",
      "agent_2: Initializing policy model...\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 560, 'output_size': 3, 'device': device(type='cpu')}\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 560, 'output_size': 3, 'device': device(type='cpu')}\n",
      "AgentTrainer: In episode 0, optimizing at step 50 that is the total step 50\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 0.0 seconds\n",
      "AgentTrainer: In episode 0, optimizing at step 50 that is the total step 50\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 0.0 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 0, reached step 100 that is beyond the current limit, 100\n",
      "AgentTrainer: Ended episode: 1 with duration: 50, total reward: 5.44444444444444\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "AgentTrainer: Ended episode: 1 with duration: 50, total reward: 5.555555555555551\n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "Type of object with name: <class 'automl.loggers.result_logger.ResultLogger'> and name passed: \n",
      "AgentTrainer: Setting up episode2\n",
      "AgentTrainer: Setting up episode2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df))\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df))\n",
      "c:\\Users\\ricar\\Universidade\\Tese\\Tese\\rep\\ricardo-goncalo-thesis-project\\project\\automl\\loggers\\result_logger.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.dataframe = pandas.concat((self.dataframe, results_df))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentTrainer: In episode 1, optimizing at step 50 that is the total step 150\n",
      "AgentTrainer: Optimizing agent...\n",
      "Cloning model\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 560, 'output_size': 3, 'device': device(type='cpu')}\n",
      "AgentTrainer: Optimization took 3.251708984375 seconds\n",
      "AgentTrainer: In episode 1, optimizing at step 50 that is the total step 150\n",
      "AgentTrainer: Optimizing agent...\n",
      "Cloning model\n",
      "Initializing model with input{'board_x': 3, 'board_y': 480, 'board_z': 560, 'output_size': 3, 'device': device(type='cpu')}\n",
      "AgentTrainer: Optimization took 1.5878336429595947 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 1, reached step 100 that is beyond the current limit, 100\n",
      "AgentTrainer: Ended episode: 2 with duration: 50, total reward: 5.44444444444444\n",
      "AgentTrainer: Ended episode: 2 with duration: 50, total reward: 5.555555555555551\n",
      "AgentTrainer: Setting up episode3\n",
      "AgentTrainer: Setting up episode3\n",
      "AgentTrainer: In episode 2, optimizing at step 50 that is the total step 250\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 1.2670707702636719 seconds\n",
      "AgentTrainer: In episode 2, optimizing at step 50 that is the total step 250\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 1.2966845035552979 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 2, reached step 100 that is beyond the current limit, 100\n",
      "AgentTrainer: Ended episode: 3 with duration: 50, total reward: 5.44444444444444\n",
      "AgentTrainer: Ended episode: 3 with duration: 50, total reward: 5.555555555555551\n",
      "AgentTrainer: Setting up episode4\n",
      "AgentTrainer: Setting up episode4\n",
      "AgentTrainer: Ended episode: 4 with duration: 30, total reward: 3.222222222222223\n",
      "AgentTrainer: Ended episode: 4 with duration: 30, total reward: -6.777777777777777\n",
      "AgentTrainer: Setting up episode5\n",
      "AgentTrainer: Setting up episode5\n",
      "AgentTrainer: In episode 4, optimizing at step 40 that is the total step 400\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 1.216963529586792 seconds\n",
      "AgentTrainer: In episode 4, optimizing at step 40 that is the total step 400\n",
      "AgentTrainer: Optimizing agent...\n",
      "AgentTrainer: Optimization took 1.1566674709320068 seconds\n",
      "RLPipelineComponent: RLTrainerComponent: In episode 4, reached step 100 that is beyond the current limit, 100\n",
      "AgentTrainer: Ended episode: 5 with duration: 50, total reward: 5.44444444444444\n",
      "AgentTrainer: Ended episode: 5 with duration: 50, total reward: 5.555555555555551\n"
     ]
    }
   ],
   "source": [
    "rl_pipeline.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_pipeline.plot_graphs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"__type__\": \"<class 'automl.rl.rl_pipeline.RLPipelineComponent'>\",\n",
      "    \"name\": \"RLPipelineComponent\",\n",
      "    \"input\": {\n",
      "        \"num_episodes\": 5,\n",
      "        \"state_memory_size\": 2,\n",
      "        \"limit_steps\": 100,\n",
      "        \"optimization_interval\": 50,\n",
      "        \"environment\": {\n",
      "            \"__type__\": \"PettingZooEnvironmentLoader\",\n",
      "            \"name\": \"PettingZooEnvironmentLoader\",\n",
      "            \"localization\": [\n",
      "                0\n",
      "            ]\n",
      "        },\n",
      "        \"agents\": {\n",
      "            \"paddle_0\": {\n",
      "                \"__type__\": \"AgentTrainer\",\n",
      "                \"name\": \"AgentTrainer\",\n",
      "                \"localization\": [\n",
      "                    3,\n",
      "                    0\n",
      "                ]\n",
      "            },\n",
      "            \"paddle_1\": {\n",
      "                \"__type__\": \"AgentTrainer\",\n",
      "                \"name\": \"AgentTrainer\",\n",
      "                \"localization\": [\n",
      "                    3,\n",
      "                    1\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"save_interval\": 100,\n",
      "        \"rl_trainer\": \"\",\n",
      "        \"created_agents_input\": {}\n",
      "    },\n",
      "    \"child_components\": [\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl.environment.environment_components.PettingZooEnvironmentLoader'>\",\n",
      "            \"name\": \"PettingZooEnvironmentLoader\",\n",
      "            \"input\": {\n",
      "                \"petting_zoo_environment\": \"cooperative_pong\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl.agent.agent_components.AgentSchema'>\",\n",
      "            \"name\": \"agent_1\",\n",
      "            \"input\": {\n",
      "                \"name\": \"agent_1\",\n",
      "                \"state_memory_size\": 2,\n",
      "                \"state_shape\": [\n",
      "                    3,\n",
      "                    480,\n",
      "                    280\n",
      "                ],\n",
      "                \"training_context\": {\n",
      "                    \"total_steps\": 0,\n",
      "                    \"episode_steps\": 100,\n",
      "                    \"episodes_done\": 5,\n",
      "                    \"total_score\": 0,\n",
      "                    \"episode_score\": 100\n",
      "                },\n",
      "                \"batch_size\": 64,\n",
      "                \"discount_factor\": 0.95,\n",
      "                \"exploration_strategy\": {\n",
      "                    \"__type__\": \"EpsilonGreedyStrategy\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"memory\": {\n",
      "                    \"__type__\": \"MemoryComponent\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        1\n",
      "                    ]\n",
      "                },\n",
      "                \"learner\": {\n",
      "                    \"__type__\": \"DeepQLearnerSchema\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        2\n",
      "                    ]\n",
      "                },\n",
      "                \"policy_model\": {\n",
      "                    \"__type__\": \"ConvModelSchema\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"localization\": [\n",
      "                        1,\n",
      "                        3\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.exploration.epsilong_greedy.EpsilonGreedyStrategy'>\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"input\": {\n",
      "                        \"training_context\": {\n",
      "                            \"total_steps\": 0,\n",
      "                            \"episode_steps\": 100,\n",
      "                            \"episodes_done\": 5,\n",
      "                            \"total_score\": 0,\n",
      "                            \"episode_score\": 100\n",
      "                        },\n",
      "                        \"epsilon_end\": 0.025,\n",
      "                        \"epsilon_start\": 1.0,\n",
      "                        \"epsilon_decay\": 0.01\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.memory_components.MemoryComponent'>\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"input\": {\n",
      "                        \"capacity\": 200\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.learners.learner_component.DeepQLearnerSchema'>\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentSchema\",\n",
      "                            \"name\": \"agent_1\",\n",
      "                            \"localization\": [\n",
      "                                1\n",
      "                            ]\n",
      "                        },\n",
      "                        \"target_update_rate\": 0.05,\n",
      "                        \"update_target_at_optimization\": true,\n",
      "                        \"optimizer\": {\n",
      "                            \"__type__\": \"AdamOptimizer\",\n",
      "                            \"name\": \"AdamOptimizer\",\n",
      "                            \"localization\": []\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.ml.models.model_components.ConvModelSchema'>\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"input\": {\n",
      "                        \"board_x\": 3,\n",
      "                        \"board_y\": 480,\n",
      "                        \"board_z\": 560,\n",
      "                        \"output_size\": 3\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl.agent.agent_components.AgentSchema'>\",\n",
      "            \"name\": \"agent_2\",\n",
      "            \"input\": {\n",
      "                \"name\": \"agent_2\",\n",
      "                \"state_memory_size\": 2,\n",
      "                \"state_shape\": [\n",
      "                    3,\n",
      "                    480,\n",
      "                    280\n",
      "                ],\n",
      "                \"training_context\": {\n",
      "                    \"total_steps\": 0,\n",
      "                    \"episode_steps\": 100,\n",
      "                    \"episodes_done\": 5,\n",
      "                    \"total_score\": 0,\n",
      "                    \"episode_score\": 100\n",
      "                },\n",
      "                \"batch_size\": 64,\n",
      "                \"discount_factor\": 0.95,\n",
      "                \"exploration_strategy\": {\n",
      "                    \"__type__\": \"EpsilonGreedyStrategy\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"memory\": {\n",
      "                    \"__type__\": \"MemoryComponent\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        1\n",
      "                    ]\n",
      "                },\n",
      "                \"learner\": {\n",
      "                    \"__type__\": \"DeepQLearnerSchema\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        2\n",
      "                    ]\n",
      "                },\n",
      "                \"policy_model\": {\n",
      "                    \"__type__\": \"ConvModelSchema\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"localization\": [\n",
      "                        2,\n",
      "                        3\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.exploration.epsilong_greedy.EpsilonGreedyStrategy'>\",\n",
      "                    \"name\": \"EpsilonGreedyStrategy\",\n",
      "                    \"input\": {\n",
      "                        \"training_context\": {\n",
      "                            \"total_steps\": 0,\n",
      "                            \"episode_steps\": 100,\n",
      "                            \"episodes_done\": 5,\n",
      "                            \"total_score\": 0,\n",
      "                            \"episode_score\": 100\n",
      "                        },\n",
      "                        \"epsilon_end\": 0.025,\n",
      "                        \"epsilon_start\": 1.0,\n",
      "                        \"epsilon_decay\": 0.01\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.memory_components.MemoryComponent'>\",\n",
      "                    \"name\": \"MemoryComponent\",\n",
      "                    \"input\": {\n",
      "                        \"capacity\": 200\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.learners.learner_component.DeepQLearnerSchema'>\",\n",
      "                    \"name\": \"DeepQLearnerSchema\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentSchema\",\n",
      "                            \"name\": \"agent_2\",\n",
      "                            \"localization\": [\n",
      "                                2\n",
      "                            ]\n",
      "                        },\n",
      "                        \"target_update_rate\": 0.05,\n",
      "                        \"update_target_at_optimization\": true,\n",
      "                        \"optimizer\": {\n",
      "                            \"__type__\": \"AdamOptimizer\",\n",
      "                            \"name\": \"AdamOptimizer\",\n",
      "                            \"localization\": []\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.ml.models.model_components.ConvModelSchema'>\",\n",
      "                    \"name\": \"ConvModelSchema\",\n",
      "                    \"input\": {\n",
      "                        \"board_x\": 3,\n",
      "                        \"board_y\": 480,\n",
      "                        \"board_z\": 560,\n",
      "                        \"output_size\": 3\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"__type__\": \"<class 'automl.rl.trainers.rl_trainer_component.RLTrainerComponent'>\",\n",
      "            \"name\": \"RLTrainerComponent\",\n",
      "            \"input\": {\n",
      "                \"num_episodes\": 5,\n",
      "                \"environment\": {\n",
      "                    \"__type__\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"name\": \"PettingZooEnvironmentLoader\",\n",
      "                    \"localization\": [\n",
      "                        0\n",
      "                    ]\n",
      "                },\n",
      "                \"limit_steps\": 100,\n",
      "                \"optimization_interval\": 50,\n",
      "                \"agents\": {\n",
      "                    \"paddle_0\": {\n",
      "                        \"__type__\": \"AgentTrainer\",\n",
      "                        \"name\": \"AgentTrainer\",\n",
      "                        \"localization\": [\n",
      "                            3,\n",
      "                            0\n",
      "                        ]\n",
      "                    },\n",
      "                    \"paddle_1\": {\n",
      "                        \"__type__\": \"AgentTrainer\",\n",
      "                        \"name\": \"AgentTrainer\",\n",
      "                        \"localization\": [\n",
      "                            3,\n",
      "                            1\n",
      "                        ]\n",
      "                    }\n",
      "                },\n",
      "                \"save_interval\": 100\n",
      "            },\n",
      "            \"child_components\": [\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'>\",\n",
      "                    \"name\": \"AgentTrainer\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentSchema\",\n",
      "                            \"name\": \"agent_1\",\n",
      "                            \"localization\": [\n",
      "                                1\n",
      "                            ]\n",
      "                        },\n",
      "                        \"optimization_interval\": 50,\n",
      "                        \"save_interval\": 100\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"__type__\": \"<class 'automl.rl.trainers.agent_trainer_component.AgentTrainer'>\",\n",
      "                    \"name\": \"AgentTrainer\",\n",
      "                    \"input\": {\n",
      "                        \"agent\": {\n",
      "                            \"__type__\": \"AgentSchema\",\n",
      "                            \"name\": \"agent_2\",\n",
      "                            \"localization\": [\n",
      "                                2\n",
      "                            ]\n",
      "                        },\n",
      "                        \"optimization_interval\": 50,\n",
      "                        \"save_interval\": 100\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rl_pipeline.save_configuration(toPrint=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
